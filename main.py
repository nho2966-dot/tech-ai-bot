import os
import asyncio
import httpx
import tweepy
import sqlite3
import hashlib
import random
import re
import subprocess
import yt_dlp
import time
from datetime import datetime
from loguru import logger

# =========================================================
# ğŸ” KEYS & AUTH
# =========================================================
OPENAI_KEY = os.getenv("OPENAI_API_KEY")
XAI_KEY = os.getenv("XAI_API_KEY")        
QWEN_KEY = os.getenv("QWEN_API_KEY")
X_KEY = os.getenv("X_API_KEY")
X_SECRET = os.getenv("X_API_SECRET")
X_TOKEN = os.getenv("X_ACCESS_TOKEN")
X_ACCESS_S = os.getenv("X_ACCESS_SECRET")
BEARER_TOKEN = os.getenv("BEARER_TOKEN")

auth = tweepy.OAuth1UserHandler(X_KEY, X_SECRET, X_TOKEN, X_ACCESS_S)
api_v1 = tweepy.API(auth)
client_v2 = tweepy.Client(
    bearer_token=BEARER_TOKEN,
    consumer_key=X_KEY, consumer_secret=X_SECRET,
    access_token=X_TOKEN, access_token_secret=X_ACCESS_S,
    wait_on_rate_limit=True 
)

# =========================================================
# ğŸ—„ DATABASE
# =========================================================
conn = sqlite3.connect("nasser_sovereign_flexible.db")
cursor = conn.cursor()
cursor.execute("CREATE TABLE IF NOT EXISTS published (hash TEXT PRIMARY KEY, time TEXT)")
conn.commit()

# =========================================================
# âš™ï¸ CONFIGURABLE PARAMETERS
# =========================================================
daily_videos_count = 1           
video_length_seconds = 45        
tweets_per_thread = 3            

# =========================================================
# ğŸ›¡ IMPROVED FILTER
# =========================================================
def nasser_filter(text):
    if not text: return ""
    text = text.replace("Ø§Ù„Ø«ÙˆØ±Ø© Ø§Ù„ØµÙ†Ø§Ø¹ÙŠØ© Ø§Ù„Ø±Ø§Ø¨Ø¹Ø©", "Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ÙˆØ£Ø­Ø¯Ø« Ø£Ø¯ÙˆØ§ØªÙ‡")
    
    banned = [
        "stock","market","investment","funding","revenue","profit","Ø³Ù‡Ù…","ØªØ¯Ø§ÙˆÙ„","Ø¹Ù…Ù„Ø©","cryptocurrency","Ø¨ÙŠØªÙƒÙˆÙŠÙ†"
    ]
    for word in banned: 
        text = re.sub(rf"\b{word}\b", "", text, flags=re.IGNORECASE)
        
    return text.strip()

# =========================================================
# ğŸ§  SOVEREIGN BRAIN
# =========================================================
class SovereignBrain:
    async def generate(self, prompt, system_msg):
        brains = [
            ("GROK", "https://api.x.ai/v1/chat/completions", {"Authorization": f"Bearer {XAI_KEY}"}, "grok-beta"),
            ("OPENAI", "https://api.openai.com/v1/chat/completions", {"Authorization": f"Bearer {OPENAI_KEY}"}, "gpt-4o-mini"),
            ("QWEN", "https://api.labs.qwen.ai/v1/chat/completions", {"Authorization": f"Bearer {QWEN_KEY}"}, "qwen-7b")
        ]
        for name, url, headers, model in brains:
            try:
                async with httpx.AsyncClient(timeout=60) as client:
                    r = await client.post(url, headers=headers, json={
                        "model": model,
                        "messages": [{"role": "system", "content": system_msg}, {"role": "user", "content": prompt}]
                    })
                    r.raise_for_status()
                    return r.json()['choices'][0]['message']['content']
            except Exception as e:
                logger.warning(f"âš ï¸ Brain {name} failed: {e}")
                continue
        return "Ø³Ø± ØªÙ‚Ù†ÙŠ Ø¬Ø¯ÙŠØ¯ ÙÙŠ Ø§Ù„Ø·Ø±ÙŠÙ‚ Ø¥Ù„ÙŠÙƒÙ….."

brain = SovereignBrain()

# =========================================================
# ğŸ¥ MULTI-SOURCE RADAR
# =========================================================
TRUSTED_CHANNELS = [
    "https://www.youtube.com/@mkbhd",
    "https://www.youtube.com/@Mrwhosetheboss",
    "https://www.youtube.com/@ProperHonestTech",
    "https://www.youtube.com/@HowToMen",
    "https://www.youtube.com/@MattWolfe",
    "https://www.youtube.com/@TheAIAdvantage"
]

def fetch_tech_video():
    logger.info("ğŸ” Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø®Ø¨Ø§ÙŠØ§ ØªÙ‚Ù†ÙŠØ© Ø¬Ø¯ÙŠØ¯Ø© Ù„Ù… ØªÙÙ†Ø´Ø± Ù…Ù† Ù‚Ø¨Ù„...")
    ydl_opts = {'quiet': True, 'extract_flat': True, 'daterange': yt_dlp.utils.DateRange('now-2days','now')}
    random.shuffle(TRUSTED_CHANNELS)
    
    with yt_dlp.YoutubeDL(ydl_opts) as ydl:
        for channel in TRUSTED_CHANNELS:
            try:
                res = ydl.extract_info(channel, download=False)
                if 'entries' in res and res['entries']:
                    for video in res['entries'][:5]:
                        title = video.get('title') or ""
                        v_url = video.get('url')
                        
                        if not v_url or not isinstance(v_url, str):
                            continue
                            
                        if any(w in title.lower() for w in ["stock","market","earnings"]):
                            continue
                            
                        v_hash = hashlib.sha256(title.encode()).hexdigest()
                        cursor.execute("SELECT hash FROM published WHERE hash=?", (v_hash,))
                        if cursor.fetchone():
                            continue 
                            
                        return {"title": title, "url": v_url, "hash": v_hash}
            except Exception as e:
                logger.warning(f"âš ï¸ ÙØ´Ù„ Ø§Ù„Ø¬Ù„Ø¨ Ù…Ù† {channel}: {e}")
                continue
    return None

# =========================================================
# ğŸ¬ VIDEO PROCESSING
# =========================================================
def process_video(url):
    logger.info("ğŸ¬ ØªØ­Ù…ÙŠÙ„ ÙˆÙ…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙÙŠØ¯ÙŠÙˆ...")
    output_raw = "raw_vid.mp4"
    output_final = "nasser_vid.mp4"
    
    ydl_opts = {'format':'bestvideo[ext=mp4]+bestaudio[ext=m4a]/best[ext=mp4]', 'outtmpl': output_raw, 'quiet': True}
    with yt_dlp.YoutubeDL(ydl_opts) as ydl: ydl.download([url])
    
    cmd = [
        "ffmpeg", "-y", "-i", output_raw, "-t", str(video_length_seconds),
        "-vf", "scale=1080:1920:force_original_aspect_ratio=increase,crop=1080:1920",
        "-c:v", "libx264", "-crf", "23", "-preset", "fast", "-c:a", "aac", output_final
    ]
    subprocess.run(cmd, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    return output_final

# =========================================================
# ğŸ¦ THREAD POSTING (WITH VIDEO)
# =========================================================
async def post_nasser_thread(title, video_path):
    prompt = f"Ø­ÙˆÙ„ Ù‡Ø°Ø§ Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹ Ø§Ù„ØªÙ‚Ù†ÙŠ Ø¥Ù„Ù‰ Ø³Ù„Ø³Ù„Ø© ØªØºØ±ÙŠØ¯Ø§Øª (Thread) Ø®Ù„ÙŠØ¬ÙŠØ© Ø¹Ù† Ø§Ù„Ø®Ø¨Ø§ÙŠØ§: {title}. Ù‚Ø³Ù…Ù‡Ø§ Ù„Ù€ {tweets_per_thread} ØªØºØ±ÙŠØ¯Ø§Øª."
    system = "Ø£Ù†Øª Ù†Ø§ØµØ±ØŒ Ø®Ø¨ÙŠØ± Ø®Ø¨Ø§ÙŠØ§ Ø§Ù„Ø£Ø¬Ù‡Ø²Ø© ÙˆØ£Ø³Ø±Ø§Ø± Ø§Ù„Ø¥Ù†ØªØ±Ù†Øª ÙˆØ§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ."
    raw_content = await brain.generate(prompt, system)
    tweets = [nasser_filter(t) for t in raw_content.split('\n\n') if t][:tweets_per_thread]
    
    if not tweets: return
    
    logger.info("ğŸ¦ Ø±ÙØ¹ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ ÙˆØ§Ù„ØªØºØ±ÙŠØ¯Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰...")
    media = api_v1.media_upload(video_path, media_category='tweet_video', chunked=True)
    
    for _ in range(15):
        try:
            status = api_v1.get_media_upload_status(media.media_id)
            if status.processing_info.get("state") == "succeeded":
                break
        except: pass
        logger.info("â³ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ù‚ÙŠØ¯ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¹Ù„Ù‰ Ù…Ù†ØµØ© X...")
        time.sleep(5)
    
    first_tweet = client_v2.create_tweet(text=tweets[0], media_ids=[media.media_id])
    last_id = first_tweet.data['id']
    
    for i in range(1, len(tweets)):
        reply = client_v2.create_tweet(text=tweets[i], in_reply_to_tweet_id=last_id)
        last_id = reply.data['id']
    
    logger.success("âœ… ØªÙ… Ù†Ø´Ø± Ø§Ù„Ø³Ù„Ø³Ù„Ø© Ø§Ù„ØªÙ‚Ù†ÙŠØ© (Ù…Ø¹ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ) Ø¨Ù†Ø¬Ø§Ø­!")

# =========================================================
# ğŸ“ TEXT ONLY FALLBACK
# =========================================================
async def post_text_only_thread():
    logger.info("ğŸ“ ØªÙØ¹ÙŠÙ„ Ø§Ù„Ø®Ø·Ø© Ø§Ù„Ø¨Ø¯ÙŠÙ„Ø©: Ø¬Ø§Ø±ÙŠ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø­ØªÙˆÙ‰ Ù†ØµÙŠ...")
    prompt = f"Ø§ÙƒØªØ¨ Ø³Ù„Ø³Ù„Ø© ØªØºØ±ÙŠØ¯Ø§Øª (Thread) Ø®Ù„ÙŠØ¬ÙŠØ© Ø¹Ù† Ø³Ø± ØªÙ‚Ù†ÙŠ Ø¬Ø¯ÙŠØ¯ØŒ Ù…ÙŠØ²Ø© Ù…Ø®ÙÙŠØ© ÙÙŠ Ø§Ù„Ù‡ÙˆØ§ØªÙØŒ Ø£Ùˆ Ø£Ø¯Ø§Ø© Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ù…ÙÙŠØ¯Ø© Ø¬Ø¯Ø§Ù‹. Ø§Ø¬Ø¹Ù„Ù‡Ø§ Ù…Ø´ÙˆÙ‚Ø© ÙˆÙ‚Ø³Ù…Ù‡Ø§ Ù„Ù€ {tweets_per_thread} ØªØºØ±ÙŠØ¯Ø§Øª Ù…ØªØ±Ø§Ø¨Ø·Ø©."
    system = "Ø£Ù†Øª Ù†Ø§ØµØ±ØŒ Ø®Ø¨ÙŠØ± Ø®Ø¨Ø§ÙŠØ§ Ø§Ù„Ø£Ø¬Ù‡Ø²Ø© ÙˆØ£Ø³Ø±Ø§Ø± Ø§Ù„Ø¥Ù†ØªØ±Ù†Øª ÙˆØ§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ."
    
    raw_content = await brain.generate(prompt, system)
    tweets = [nasser_filter(t) for t in raw_content.split('\n\n') if t][:tweets_per_thread]
    
    if not tweets:
        logger.error("âŒ ÙØ´Ù„ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ÙÙŠ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù†Øµ Ø§Ù„Ø¨Ø¯ÙŠÙ„.")
        return
        
    logger.info("ğŸ¦ Ø¬Ø§Ø±ÙŠ Ù†Ø´Ø± Ø§Ù„Ø³Ù„Ø³Ù„Ø© Ø§Ù„Ù†ØµÙŠØ©...")
    first_tweet = client_v2.create_tweet(text=tweets[0])
    last_id = first_tweet.data['id']
    
    for i in range(1, len(tweets)):
        reply = client_v2.create_tweet(text=tweets[i], in_reply_to_tweet_id=last_id)
        last_id = reply.data['id']
        
    logger.success("âœ… ØªÙ… Ù†Ø´Ø± Ø§Ù„Ø³Ù„Ø³Ù„Ø© Ø§Ù„Ù†ØµÙŠØ© Ø§Ù„Ø¨Ø¯ÙŠÙ„Ø© Ø¨Ù†Ø¬Ø§Ø­!")

# =========================================================
# ğŸš€ EXECUTION FLOW
# =========================================================
async def run_daily_task():
    for _ in range(daily_videos_count):
        video_data = fetch_tech_video()
        
        # Ø§Ù„Ø®Ø·Ø© Ø§Ù„Ø¨Ø¯ÙŠÙ„Ø© 1: Ø¥Ø°Ø§ Ù„Ù… ÙŠØ¬Ø¯ Ø§Ù„Ø³ÙƒØ±Ø¨Øª Ø£ÙŠ ÙÙŠØ¯ÙŠÙˆ Ø¬Ø¯ÙŠØ¯
        if not video_data: 
            logger.warning("âš ï¸ Ù„Ø§ ØªÙˆØ¬Ø¯ ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª Ø¬Ø¯ÙŠØ¯Ø© Ù„Ù… ØªÙÙ†Ø´Ø± Ø§Ù„ÙŠÙˆÙ….")
            await post_text_only_thread()
            return

        v_hash = video_data['hash']

        try:
            final_vid = process_video(video_data['url'])
            await post_nasser_thread(video_data['title'], final_vid)
            
            cursor.execute("INSERT INTO published VALUES (?,?)", (v_hash, datetime.utcnow().isoformat()))
            conn.commit()
            
            for f in ["raw_vid.mp4", "nasser_vid.mp4"]:
                if os.path.exists(f): os.remove(f)
                
        except Exception as e:
            # Ø§Ù„Ø®Ø·Ø© Ø§Ù„Ø¨Ø¯ÙŠÙ„Ø© 2: Ø¥Ø°Ø§ Ø­Ø¯Ø« Ø£ÙŠ Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªØ­Ù…ÙŠÙ„ØŒ Ø£Ùˆ Ù‚Øµ Ø§Ù„ÙÙŠØ¯ÙŠÙˆØŒ Ø£Ùˆ Ø§Ù„Ø±ÙØ¹
            logger.error(f"âŒ Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø£Ùˆ Ø±ÙØ¹ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ: {e}")
            await post_text_only_thread()

if __name__ == "__main__":
    logger.info("ğŸš€ Ø¨Ø¯Ø¡ ØªØ´ØºÙŠÙ„ Ø§Ù„Ø³ÙƒØ±Ø¨Øª Ù…Ù† GitHub Actions...")
    asyncio.run(run_daily_task())
    logger.info("ğŸ ØªÙ…Øª Ø§Ù„Ù…Ù‡Ù…Ø© ÙˆØ³ÙŠØªÙ… Ø¥ØºÙ„Ø§Ù‚ Ø§Ù„Ø³ÙƒØ±Ø¨Øª Ù„Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙˆØ§Ø±Ø¯.")
