import os, sqlite3, logging, hashlib, time, re, random
from datetime import datetime, timedelta
import tweepy, feedparser
from dotenv import load_dotenv
from openai import OpenAI
from tweepy.errors import TooManyRequests

# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª
load_dotenv()
DB_FILE = "news_master_2026.db"
LOG_FILE = "system_master.log"

# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø³Ø¬Ù„Ø§Øª (Logs)
logging.basicConfig(level=logging.INFO, format="ğŸ›¡ï¸ %(asctime)s - %(message)s", 
                    handlers=[logging.FileHandler(LOG_FILE), logging.StreamHandler()])

# 1. Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø§Ù„Ø±Ø³Ù…ÙŠØ© Ø§Ù„Ù†Ø®Ø¨ÙˆÙŠØ© (Elite Official Sources)
SOURCES = {
    "AI_Official": ["https://blog.google/technology/ai/rss/", "https://openai.com/news/rss/"],
    "CyberSecurity": ["https://thehackernews.com/feeds/posts/default", "https://krebsonsecurity.com/feed/"],
    "FinTech_Crypto": ["https://www.coindesk.com/arc/outboundfeeds/rss/", "https://www.theblock.co/rss.xml"],
    "Microsoft_Official": ["https://www.microsoft.com/en-us/microsoft-365/blog/feed/"],
    "Tech_Authority": ["https://arstechnica.com/feed/", "https://www.wired.com/feed/rss"]
}

# 2. Ø§Ù„Ù…Ø±Ø¬Ø¹ Ø§Ù„Ù…Ø¹Ø±ÙÙŠ ÙˆØ§Ù„Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„ØªØ­Ø±ÙŠØ±ÙŠØ©
KNOWLEDGE_BASE = {
    "microsoft": "Ø®Ø¨Ø§ÙŠØ§ Microsoft 365ØŒ Ø§Ø®ØªØµØ§Ø±Ø§Øª Ø§Ù„Ø¥Ù†ØªØ§Ø¬ÙŠØ©ØŒ Ù…ÙŠØ²Ø§Øª Windows 11.",
    "x_profit": "Ø£Ø±Ø¨Ø§Ø­ Ø§Ù„Ø±Ø¯ÙˆØ¯ (0.2 Ø³Ù†Øª)ØŒ Ù‚ÙˆØ© Ø§Ù„Ø­Ø³Ø§Ø¨Ø§Øª Ø§Ù„Ù…ÙˆØ«Ù‚Ø©ØŒ Ù‚Ø§Ø¹Ø¯Ø© Ø¢Ø®Ø± 20 Ù…Ù†Ø´ÙˆØ±.",
    "google_ai": "Ø³Ù„Ø³Ù„Ø© Ø£Ø³Ø¨ÙˆØ¹ÙŠØ© Ø¯ÙˆØ±ÙŠØ© ØªØ´Ø±Ø­ Ø£Ø¯ÙˆØ§Øª Ø¬ÙˆØ¬Ù„ Ù„Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø¨Ø§Ø­ØªØ±Ø§ÙÙŠØ©."
}

STRICT_SYSTEM_PROMPT = f"""
Ø£Ù†Øª Ø±Ø¦ÙŠØ³ ØªØ­Ø±ÙŠØ± (TechElite). ØµÙØº Ù…Ø­ØªÙˆÙ‰ ØªÙ‚Ù†ÙŠØ§Ù‹ Ø§Ø­ØªØ±Ø§ÙÙŠØ§Ù‹ Ø¬Ø¯Ø§Ù‹ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø§Ù„Ø±Ø³Ù…ÙŠØ© ÙÙ‚Ø·.
Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹ Ø§Ù„Ù…Ø¹ØªÙ…Ø¯Ø©: {KNOWLEDGE_BASE}
Ø§Ù„Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ù‚Ø·Ø¹ÙŠØ©:
1. Ù†ÙˆØ¹ Ø§Ù„Ø¹Ø±Ø¶ Ø¨ÙŠÙ† (Ø®Ø¨Ø± Ø¹Ø§Ø¬Ù„ØŒ Ø«Ø±ÙŠØ¯ ØªØ¹Ù„ÙŠÙ…ÙŠØŒ Ù‚Ø§Ø¦Ù…Ø© Ù†ØµØ§Ø¦Ø­ØŒ ØªØ­Ø°ÙŠØ± Ø£Ù…Ù†ÙŠ).
2. Ù…Ø«Ù„Ø« Ø§Ù„Ù‚ÙŠÙ…Ø©: [TWEET_1] Ø®ÙØ·Ù‘Ø§Ù Ø¬Ø°Ø§Ø¨ØŒ [TWEET_2] Ø¬ÙˆÙ‡Ø± Ø§Ù„Ø³Ø± Ø§Ù„ØªÙ‚Ù†ÙŠØŒ [POLL_QUESTION] ØªÙØ§Ø¹Ù„ Ø§Ù„Ù…ØªØ§Ø¨Ø¹ÙŠÙ†.
3. Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ÙˆØ¯ÙˆØ¯Ø© ÙˆØ±ØµÙŠÙ†Ø©ØŒ Ù…Ø¹ Ù…ØµØ·Ù„Ø­Ø§Øª Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ© Ø¨ÙŠÙ† Ù‚ÙˆØ³ÙŠÙ† (Term).
4. ØªÙˆÙ„ÙŠØ¯ 3 Ù‡Ø§Ø´ØªØ§ØºØ§Øª Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠØ© Ø°ÙƒÙŠØ© ÙÙŠ Ù†Ù‡Ø§ÙŠØ© Ø§Ù„Ø«Ø±ÙŠØ¯.
5. Ù…Ù†Ø¹ Ø§Ù„Ø±Ù…ÙˆØ² Ø§Ù„ØµÙŠÙ†ÙŠØ© Ø£Ùˆ HTML ØªÙ…Ø§Ù…Ø§Ù‹.
"""

class TechEliteIntegrated2026:
    def __init__(self):
        self.max_daily = 4
        self._init_db()
        self._init_clients()

    def _init_db(self):
        with sqlite3.connect(DB_FILE) as conn:
            conn.execute("""
                CREATE TABLE IF NOT EXISTS news (
                    hash TEXT PRIMARY KEY, title TEXT, category TEXT, 
                    keywords TEXT, hashtags TEXT, published_at TEXT
                )
            """)

    def _init_clients(self):
        self.x_client = tweepy.Client(
            bearer_token=os.getenv("X_BEARER_TOKEN"),
            consumer_key=os.getenv("X_API_KEY"),
            consumer_secret=os.getenv("X_API_SECRET"),
            access_token=os.getenv("X_ACCESS_TOKEN"),
            access_token_secret=os.getenv("X_ACCESS_SECRET")
        )
        self.ai_client = OpenAI(base_url="https://openrouter.ai/api/v1", api_key=os.getenv("OPENROUTER_API_KEY"))

    def _extract_semantic_keywords(self, title, summary):
        """ØªØ­Ù„ÙŠÙ„ Ø¯Ù„Ø§Ù„ÙŠ Ù„Ù„Ø¹Ù†ÙˆØ§Ù† ÙˆØ§Ù„Ù…Ù„Ø®Øµ Ù„Ù…Ù†Ø¹ Ø§Ù„ØªÙƒØ±Ø§Ø±"""
        combined = f"{title} {summary}".lower()
        words = re.findall(r'\w+', combined)
        stop_words = {'the', 'with', 'update', 'launch', 'official', 'google', 'microsoft', 'tech'}
        important = [w for w in words if len(w) > 3 and w not in stop_words]
        return ",".join(list(set(important))[:8])

    def _is_duplicate_semantic(self, new_keywords):
        """Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØªÙƒØ±Ø§Ø± Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹ Ø¯Ù„Ø§Ù„ÙŠØ§Ù‹ Ø®Ù„Ø§Ù„ 48 Ø³Ø§Ø¹Ø©"""
        if not new_keywords: return False
        with sqlite3.connect(DB_FILE) as conn:
            limit_date = (datetime.now() - timedelta(days=2)).isoformat()
            cursor = conn.execute("SELECT keywords FROM news WHERE published_at > ?", (limit_date,))
            new_set = set(new_keywords.split(','))
            for row in cursor.fetchall():
                if not row[0]: continue
                existing_set = set(row[0].split(','))
                if len(new_set.intersection(existing_set)) >= 4: return True
        return False

    def post_thread_with_retry(self, ai_text, url, title, cat, keywords):
        """Ù†Ø¸Ø§Ù… Ø§Ù„Ù†Ø´Ø± Ù…Ø¹ Retry & Backoff Ø°ÙƒÙŠ"""
        parts = re.findall(r'\[.*?\](.*?)(?=\[|$)', ai_text, re.S)
        if len(parts) < 3: return False
        hashtags = " ".join(re.findall(r'#\w+', ai_text))
        last_id = None
        
        for i, content in enumerate(parts[:3]):
            text = f"{i+1}/ {content.strip()}"
            if i == 1: text += f"\n\nğŸ”— Ø§Ù„Ù…ØµØ¯Ø± Ø§Ù„Ø±Ø³Ù…ÙŠ: {url}"
            
            for attempt in range(3):
                try:
                    if i == 2 and len(parts) >= 4:
                        opts = [o.strip() for o in parts[3].split('-') if o.strip()][:4]
                        res = self.x_client.create_tweet(text=text[:280], poll_options=opts, poll_duration_minutes=1440, in_reply_to_tweet_id=last_id)
                    else:
                        res = self.x_client.create_tweet(text=text[:280], in_reply_to_tweet_id=last_id)
                    last_id = res.data["id"]
                    time.sleep(75)
                    break
                except TooManyRequests as e:
                    wait = int(e.response.headers.get('Retry-After', 300))
                    time.sleep(wait)
                except Exception as e:
                    logging.error(f"âŒ Ø®Ø·Ø£ Ù…Ø­Ø§ÙˆÙ„Ø© {attempt+1}: {e}")
                    time.sleep(30 * (attempt + 1))
        
        with sqlite3.connect(DB_FILE) as conn:
            conn.execute("INSERT OR REPLACE INTO news VALUES (?, ?, ?, ?, ?, ?)", 
                         (hashlib.sha256(title.encode()).hexdigest(), title, cat, keywords, hashtags, datetime.now().isoformat()))
        return True

    def post_weekly_report(self):
        """Ù†Ø´Ø± ØªÙ‚Ø±ÙŠØ± Ø£Ø¯Ø§Ø¡ Ø£Ø³Ø¨ÙˆØ¹ÙŠ Ù„Ù„Ù…ØªØ§Ø¨Ø¹ÙŠÙ† ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹"""
        with sqlite3.connect(DB_FILE) as conn:
            last_week = (datetime.now() - timedelta(days=7)).isoformat()
            data = conn.execute("SELECT category, COUNT(*) FROM news WHERE published_at > ? GROUP BY category", (last_week,)).fetchall()
            if not data: return
            
            report = "ğŸ“Š Ø­ØµØ§Ø¯ Ø§Ù„Ø£Ø³Ø¨ÙˆØ¹ Ø§Ù„ØªÙ‚Ù†ÙŠ ÙÙŠ TechElite:\n\n"
            for cat, count in data: report += f"ğŸ”¹ {cat}: {count} ØªØºØ±ÙŠØ¯Ø©\n"
            report += "\nÙ†Ù‚Ø¯Ù… Ù„ÙƒÙ… Ø§Ù„Ø­Ù‚Ø§Ø¦Ù‚ Ù…Ù† Ù…ØµØ§Ø¯Ø±Ù‡Ø§ Ø§Ù„Ø±Ø³Ù…ÙŠØ©. #Tech_Report #AI"
            self.x_client.create_tweet(text=report)

    def run_cycle(self):
        published_count = 0
        current_day = datetime.now().strftime('%A')
        
        # Ø§Ù„Ø³Ø¨Øª: Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ø£Ø³Ø¨ÙˆØ¹ÙŠ
        if current_day == "Saturday": self.post_weekly_report()

        # Ø§Ù„Ø£Ø±Ø¨Ø¹Ø§Ø¡: Ø³Ù„Ø³Ù„Ø© Ø¬ÙˆØ¬Ù„ AI
        if current_day == "Wednesday":
            ai_text = self._generate_ai("Ø³Ù„Ø³Ù„Ø© Ø§Ù„Ø£Ø³Ø¨ÙˆØ¹: Ø£Ø¯Ø§Ø© Google AI Ø§Ù„Ø±Ø³Ù…ÙŠØ© ÙˆÙƒÙŠÙÙŠØ© Ø§Ø³ØªØºÙ„Ø§Ù„Ù‡Ø§ Ø§Ø­ØªØ±Ø§ÙÙŠØ§Ù‹.")
            if ai_text and self.post_thread_with_retry(ai_text, "https://ai.google/", "Google AI Series", "Education", "google,ai,official"):
                published_count += 1

        # Ø§Ù„Ù†Ø´Ø± Ø§Ù„ÙŠÙˆÙ…ÙŠ
        categories = list(SOURCES.keys())
        random.shuffle(categories)
        for cat in categories:
            if published_count >= self.max_daily: break
            for url in SOURCES[cat]:
                feed = feedparser.parse(url)
                for e in feed.entries[:5]:
                    if published_count >= self.max_daily: break
                    keywords = self._extract_semantic_keywords(e.title, getattr(e, 'summary', ''))
                    if not self._is_duplicate_semantic(keywords):
                        ai_text = self._generate_ai(f"Ø§Ù„ØªØµÙ†ÙŠÙ: {cat}\nØ§Ù„Ù…ÙˆØ¶ÙˆØ¹: {e.title}\nØ§Ù„ØªÙØ§ØµÙŠÙ„: {getattr(e, 'summary', '')}")
                        if ai_text and self.post_thread_with_retry(ai_text, e.link, e.title, cat, keywords):
                            published_count += 1
                            break

    def _generate_ai(self, context):
        try:
            r = self.ai_client.chat.completions.create(
                model="qwen/qwen-2.5-72b-instruct",
                messages=[{"role":"system","content":STRICT_SYSTEM_PROMPT}, {"role":"user","content":context}],
                temperature=0.1
            )
            return r.choices[0].message.content.strip()
        except: return None

if __name__ == "__main__":
    TechEliteIntegrated2026().run_cycle()
