import os
import asyncio
import httpx
import tweepy
import sqlite3
import hashlib
import random
import re
import subprocess
import yt_dlp
import time
from datetime import datetime
from loguru import logger

# =========================================================
# ðŸ” KEYS & AUTH
# =========================================================
OPENAI_KEY = os.getenv("OPENAI_API_KEY")
XAI_KEY = os.getenv("XAI_API_KEY")        
QWEN_KEY = os.getenv("QWEN_API_KEY")
GROQ_KEY = os.getenv("GROQ_API_KEY")
OPENROUTER_KEY = os.getenv("OPENROUTER_API_KEY")
GEMINI_KEY = os.getenv("GEMINI_KEY")

X_KEY = os.getenv("X_API_KEY")
X_SECRET = os.getenv("X_API_SECRET")
X_TOKEN = os.getenv("X_ACCESS_TOKEN")
X_ACCESS_S = os.getenv("X_ACCESS_SECRET")
BEARER_TOKEN = os.getenv("X_BEARER_TOKEN")

auth = tweepy.OAuth1UserHandler(X_KEY, X_SECRET, X_TOKEN, X_ACCESS_S)
api_v1 = tweepy.API(auth)
client_v2 = tweepy.Client(
    bearer_token=BEARER_TOKEN,
    consumer_key=X_KEY, consumer_secret=X_SECRET,
    access_token=X_TOKEN, access_token_secret=X_ACCESS_S,
    wait_on_rate_limit=True 
)

# =========================================================
# ðŸ—„ DATABASE
# =========================================================
conn = sqlite3.connect("tech_sovereign_flexible.db")
cursor = conn.cursor()
cursor.execute("CREATE TABLE IF NOT EXISTS published (hash TEXT PRIMARY KEY, time TEXT)")
conn.commit()

# =========================================================
# âš™ï¸ CONFIGURABLE PARAMETERS
# =========================================================
daily_videos_count = 1           
video_length_seconds = 45        
tweets_per_thread = 3            

# =========================================================
# ðŸ›¡ IMPROVED FILTER
# =========================================================
def content_filter(text):
    if not text: return ""
    banned = [
        "stock","market","investment","funding","revenue","profit","Ø³Ù‡Ù…","ØªØ¯Ø§ÙˆÙ„","Ø¹Ù…Ù„Ø©","cryptocurrency","Ø¨ÙŠØªÙƒÙˆÙŠÙ†", "Ù†Ø§ØµØ±", "Ø®Ø¨ÙŠØ±"
    ]
    for word in banned: 
        text = re.sub(rf"\b{word}\b", "", text, flags=re.IGNORECASE)
    text = re.sub(r'^(Ø§Ù„ØªØºØ±ÙŠØ¯Ø© \d+:|ØªØºØ±ÙŠØ¯Ø© \d+)\s*', '', text, flags=re.IGNORECASE).strip()
    return text

# =========================================================
# ðŸ§  SOVEREIGN BRAIN
# =========================================================
class SovereignBrain:
    async def generate(self, prompt, system_msg):
        brains = []
        if GEMINI_KEY: brains.append(("GEMINI", f"https://generativelanguage.googleapis.com/v1beta/openai/chat/completions", {"Authorization": f"Bearer {GEMINI_KEY}"}, "gemini-2.5-flash"))
        if GROQ_KEY: brains.append(("GROQ", "https://api.groq.com/openai/v1/chat/completions", {"Authorization": f"Bearer {GROQ_KEY}"}, "llama-3.3-70b-versatile"))
        if XAI_KEY: brains.append(("GROK", "https://api.x.ai/v1/chat/completions", {"Authorization": f"Bearer {XAI_KEY}"}, "grok-2-latest"))
        if OPENROUTER_KEY: brains.append(("OPENROUTER", "https://openrouter.ai/api/v1/chat/completions", {"Authorization": f"Bearer {OPENROUTER_KEY}"}, "google/gemini-2.5-flash"))
        if OPENAI_KEY: brains.append(("OPENAI", "https://api.openai.com/v1/chat/completions", {"Authorization": f"Bearer {OPENAI_KEY}"}, "gpt-4o-mini"))

        for name, url, headers, model in brains:
            try:
                async with httpx.AsyncClient(timeout=60) as client:
                    r = await client.post(url, headers=headers, json={
                        "model": model,
                        "messages": [{"role": "system", "content": system_msg}, {"role": "user", "content": prompt}]
                    })
                    r.raise_for_status()
                    return r.json()['choices'][0]['message']['content']
            except Exception as e:
                logger.warning(f"âš ï¸ Brain {name} failed: {e}")
                continue
        logger.error("âŒ ÙØ´Ù„Øª Ø¬Ù…ÙŠØ¹ Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ÙÙŠ Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø©!")
        return None

brain = SovereignBrain()

# =========================================================
# ðŸŽ¥ MULTI-SOURCE RADAR
# =========================================================
TRUSTED_CHANNELS = [
    "https://www.youtube.com/@mkbhd",
    "https://www.youtube.com/@Mrwhosetheboss",
    "https://www.youtube.com/@ProperHonestTech",
    "https://www.youtube.com/@HowToMen",
    "https://www.youtube.com/@MattWolfe",
    "https://www.youtube.com/@TheAIAdvantage",
    "https://www.youtube.com/@ThioJoe",
    "https://www.youtube.com/@zoneoftech",
    "https://www.youtube.com/@TechSpurt",
    "https://www.youtube.com/@AndroidAuthority",
    "https://www.youtube.com/@TheVerge",
    "https://www.youtube.com/@cnet"
]

SEARCH_QUERIES = [
    "ytsearch10: tech tips and tricks",
    "ytsearch10: hidden smartphone features shorts",
    "ytsearch10: secret iphone tricks",
    "ytsearch10: best AI tools tutorial",
    "ytsearch10: android hacks shorts",
    "ytsearch10: cool tech gadgets"
]

def fetch_tech_video():
    logger.info("ðŸ”Ž Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø®Ø¨Ø§ÙŠØ§ ØªÙ‚Ù†ÙŠØ© Ø¬Ø¯ÙŠØ¯Ø© Ù…Ù† Ø§Ù„Ù‚Ù†ÙˆØ§Øª Ø§Ù„Ù…ÙˆØ«ÙˆÙ‚Ø©...")
    ydl_opts_channels = {'quiet': True, 'extract_flat': True, 'daterange': yt_dlp.utils.DateRange('now-3days','now')}
    random.shuffle(TRUSTED_CHANNELS)
    
    with yt_dlp.YoutubeDL(ydl_opts_channels) as ydl:
        for channel in TRUSTED_CHANNELS:
            try:
                res = ydl.extract_info(channel, download=False)
                if 'entries' in res and res['entries']:
                    for video in res['entries'][:5]:
                        title = video.get('title') or ""
                        v_url = video.get('url') or (f"https://www.youtube.com/watch?v={video.get('id')}" if video.get('id') else None)
                        
                        if not v_url: continue
                        if any(w in title.lower() for w in ["stock","market","earnings","review","podcast"]): continue
                            
                        v_hash = hashlib.sha256(title.encode()).hexdigest()
                        cursor.execute("SELECT hash FROM published WHERE hash=?", (v_hash,))
                        if cursor.fetchone(): continue 
                            
                        return {"title": title, "url": v_url, "hash": v_hash}
            except Exception:
                continue

    logger.info("âš ï¸ Ù„Ù… Ù†Ø¬Ø¯ ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª Ø¬Ø¯ÙŠØ¯Ø© ÙÙŠ Ø§Ù„Ù‚Ù†ÙˆØ§ØªØŒ Ø¬Ø§Ø±ÙŠ ØªÙØ¹ÙŠÙ„ Ù…Ø­Ø±Ùƒ Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù…ÙØªÙˆØ­ Ø§Ù„Ø´Ø§Ù…Ù„...")
    ydl_opts_search = {'quiet': True, 'extract_flat': True} 
    random.shuffle(SEARCH_QUERIES)
    
    with yt_dlp.YoutubeDL(ydl_opts_search) as ydl:
        for query in SEARCH_QUERIES:
            try:
                res = ydl.extract_info(query, download=False)
                if 'entries' in res and res['entries']:
                    for video in res['entries']:
                        title = video.get('title') or ""
                        v_url = video.get('url') or (f"https://www.youtube.com/watch?v={video.get('id')}" if video.get('id') else None)
                        
                        if not v_url: continue
                        if any(w in title.lower() for w in ["stock","market","earnings","review","podcast"]): continue
                            
                        v_hash = hashlib.sha256(title.encode()).hexdigest()
                        cursor.execute("SELECT hash FROM published WHERE hash=?", (v_hash,))
                        if cursor.fetchone(): continue 
                            
                        return {"title": title, "url": v_url, "hash": v_hash}
            except Exception as e:
                logger.warning(f"âš ï¸ ÙØ´Ù„ Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… {query}: {e}")
                continue

    return None

def process_video(url):
    logger.info("ðŸŽ¬ ØªØ­Ù…ÙŠÙ„ ÙˆÙ…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙÙŠØ¯ÙŠÙˆ...")
    output_raw = "raw_vid.mp4"
    output_final = "tech_vid.mp4"
    
    ydl_opts = {'format':'bestvideo[ext=mp4]+bestaudio[ext=m4a]/best[ext=mp4]', 'outtmpl': output_raw, 'quiet': True}
    with yt_dlp.YoutubeDL(ydl_opts) as ydl: ydl.download([url])
    
    cmd = [
        "ffmpeg", "-y", "-i", output_raw, "-t", str(video_length_seconds),
        "-vf", "scale=1080:1920:force_original_aspect_ratio=increase,crop=1080:1920",
        "-c:v", "libx264", "-crf", "23", "-preset", "fast", "-c:a", "aac", output_final
    ]
    subprocess.run(cmd, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    return output_final

# =========================================================
# ðŸ¦ THREAD POSTING (WITH VIDEO)
# =========================================================
async def post_video_thread(title, video_path):
    prompt = f"""Ø§ÙƒØªØ¨ Ø³Ù„Ø³Ù„Ø© Ù…Ù† {tweets_per_thread} ØªØºØ±ÙŠØ¯Ø§Øª ØªÙØµÙŠÙ„ÙŠØ© ØªØ´Ø±Ø­ Ù‡Ø°Ø§ Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹ Ø§Ù„ØªÙ‚Ù†ÙŠ: ({title}).
ÙŠØ¬Ø¨ Ø£Ù† ØªØ­ØªÙˆÙŠ Ø§Ù„Ø³Ù„Ø³Ù„Ø© Ø¹Ù„Ù‰ Ù‚ÙŠÙ…Ø© Ù…Ø¶Ø§ÙØ© Ø­Ù‚ÙŠÙ‚ÙŠØ© ÙˆÙ…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¯Ø³Ù…Ø© (Ø§Ø³ØªØºÙ„ Ù…Ø³Ø§Ø­Ø© X Premium).

Ø§Ù„Ù‚Ø§Ù„Ø¨ Ø§Ù„Ø¥Ù„Ø²Ø§Ù…ÙŠ:
Ø§Ù„ØªØºØ±ÙŠØ¯Ø© 1: Ø§Ø¯Ø®Ù„ ÙÙŠ ØµÙ„Ø¨ Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹ Ù…Ø¨Ø§Ø´Ø±Ø©. Ø§Ø°ÙƒØ± Ø§Ø³Ù… Ø§Ù„ØªÙ‚Ù†ÙŠØ©/Ø§Ù„Ù…ÙŠØ²Ø© ÙˆØ§Ù„ÙØ§Ø¦Ø¯Ø© Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠØ© Ø§Ù„ØªÙŠ Ø³ØªÙ‚Ø¯Ù…Ù‡Ø§ Ù„Ù„Ù…Ø³ØªØ®Ø¯Ù… (Ø¨Ø¯ÙˆÙ† Ù…Ù‚Ø¯Ù…Ø§Øª Ø·ÙˆÙŠÙ„Ø©).
Ø§Ù„ØªØºØ±ÙŠØ¯Ø© 2: Ø§Ø´Ø±Ø­ "ÙƒÙŠÙ ØªØ¹Ù…Ù„" Ù‡Ø°Ù‡ Ø§Ù„ØªÙ‚Ù†ÙŠØ© Ø¨Ø§Ù„ØªÙØµÙŠÙ„ Ø£Ùˆ Ø§Ø°ÙƒØ± Ù…Ù…ÙŠØ²Ø§ØªÙ‡Ø§ Ø§Ù„Ù‚ÙˆÙŠØ© Ø¨Ø£Ù…Ø«Ù„Ø© Ø¹Ù…Ù„ÙŠØ©.
Ø§Ù„ØªØºØ±ÙŠØ¯Ø© 3: Ø§Ø°ÙƒØ± "ÙƒÙŠÙÙŠØ© Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…" Ø£Ùˆ Ø§Ù„Ø®Ø·ÙˆØ§Øª Ø§Ù„Ø¹Ù…Ù„ÙŠØ© Ù„Ù„Ø§Ø³ØªÙØ§Ø¯Ø© Ù…Ù†Ù‡Ø§ØŒ ÙˆØ§Ø®ØªÙ… Ø¨Ù€ 2 Ù‡Ø§Ø´ØªØ§Ø¬Ø§Øª.

ÙŠØ¬Ø¨ Ø§Ù„ÙØµÙ„ Ø¨ÙŠÙ† ÙƒÙ„ ØªØºØ±ÙŠØ¯Ø© ÙˆØ£Ø®Ø±Ù‰ Ø¨Ø³Ø·Ø±ÙŠÙ† ÙØ§Ø±ØºÙŠÙ† (\\n\\n)."""

    system = "Ø£Ù†Øª Ø­Ø³Ø§Ø¨ ØªÙ‚Ù†ÙŠ Ø§Ø­ØªØ±Ø§ÙÙŠ ÙŠÙ‚Ø¯Ù… Ù…Ø­ØªÙˆÙ‰ Ø¹Ø§Ù„ÙŠ Ø§Ù„Ø¬ÙˆØ¯Ø©. Ù„Ø§ ØªØ³ØªØ®Ø¯Ù… Ø£ÙŠ Ø£Ø³Ù…Ø§Ø¡ Ø´Ø®ØµÙŠØ©. Ù„Ø§ ØªÙƒØªØ¨ Ù…Ù‚Ø¯Ù…Ø§Øª ÙØ§Ø±ØºØ© Ø£Ùˆ Ø«Ø±Ø«Ø±Ø©. Ù‚Ø¯Ù… Ù…Ø¹Ù„ÙˆÙ…Ø© ØªÙ‚Ù†ÙŠØ© Ù…Ø±ÙƒØ²Ø©ØŒ Ù…ÙØµÙ„Ø©ØŒ ÙˆÙ…ÙÙŠØ¯Ø© Ø¬Ø¯Ø§Ù‹ Ù„Ù„Ù‚Ø§Ø±Ø¦ Ø¨Ø£Ø³Ù„ÙˆØ¨ Ø®Ù„ÙŠØ¬ÙŠ ÙˆØ§Ø¶Ø­."
    
    raw_content = await brain.generate(prompt, system)
    if not raw_content: return
        
    tweets = [content_filter(t) for t in raw_content.split('\n\n') if len(t.strip()) > 10][:tweets_per_thread]
    if len(tweets) < tweets_per_thread: return
    
    logger.info("ðŸ¦ Ø±ÙØ¹ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ ÙˆØ§Ù„ØªØºØ±ÙŠØ¯Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰...")
    media = api_v1.media_upload(video_path, media_category='tweet_video', chunked=True)
    
    for _ in range(15):
        try:
            status = api_v1.get_media_upload_status(media.media_id)
            if status.processing_info.get("state") == "succeeded": break
        except: pass
        time.sleep(5)
    
    try:
        first_tweet = client_v2.create_tweet(text=tweets[0], media_ids=[media.media_id])
        last_id = first_tweet.data['id']
        
        for i in range(1, len(tweets)):
            # Ø£Ù†Ø³Ù†Ø©: ÙØ§ØµÙ„ Ø²Ù…Ù†ÙŠ Ø¹Ø´ÙˆØ§Ø¦ÙŠ Ø¨ÙŠÙ† 10 Ùˆ 25 Ø«Ø§Ù†ÙŠØ© Ù„ÙŠÙˆØ­ÙŠ Ø¨Ø£Ù† Ù‡Ù†Ø§Ùƒ Ù…Ù† ÙŠÙƒØªØ¨
            delay = random.randint(10, 25)
            logger.info(f"â³ (Ø£Ù†Ø³Ù†Ø©) Ø¬Ø§Ø±ÙŠ Ø§Ù„Ø§Ù†ØªØ¸Ø§Ø± Ù„Ù…Ø¯Ø© {delay} Ø«Ø§Ù†ÙŠØ© Ù‚Ø¨Ù„ Ù†Ø´Ø± Ø§Ù„ØªØºØ±ÙŠØ¯Ø© Ø§Ù„ØªØ§Ù„ÙŠØ©...")
            await asyncio.sleep(delay)
            
            reply = client_v2.create_tweet(text=tweets[i], in_reply_to_tweet_id=last_id)
            last_id = reply.data['id']
            
        logger.success("âœ… ØªÙ… Ù†Ø´Ø± Ø§Ù„Ø³Ù„Ø³Ù„Ø© Ø§Ù„ØªÙ‚Ù†ÙŠØ© (Ù…Ø¹ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ) Ø¨Ù†Ø¬Ø§Ø­!")
    except Exception as e:
        logger.error(f"âŒ ÙØ´Ù„ Ø§Ù„Ù†Ø´Ø± Ø¹Ù„Ù‰ Ù…Ù†ØµØ© X. Ø§Ù„Ø³Ø¨Ø¨: {e}")

# =========================================================
# ðŸ“ TEXT ONLY FALLBACK
# =========================================================
async def post_text_only_thread():
    logger.info("ðŸ“ ØªÙØ¹ÙŠÙ„ Ø§Ù„Ø®Ø·Ø© Ø§Ù„Ø¨Ø¯ÙŠÙ„Ø©: Ø¬Ø§Ø±ÙŠ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø­ØªÙˆÙ‰ Ù†ØµÙŠ...")
    
    tech_topics = [
        "Ù…ÙŠØ²Ø© Ù…Ø®ÙÙŠØ© ÙÙŠ Ø§Ù„Ø¢ÙŠÙÙˆÙ† Ù„Ø­Ù…Ø§ÙŠØ© Ø§Ù„Ø®ØµÙˆØµÙŠØ© Ù…Ù† Ø§Ù„ØªØ·Ø¨ÙŠÙ‚Ø§Øª Ø§Ù„ØªÙŠ ØªØªØ¬Ø³Ø³ Ø¹Ù„Ù‰ Ø§Ù„Ø­Ø§ÙØ¸Ø© (Clipboard).",
        "Ø·Ø±ÙŠÙ‚Ø© Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ù„ØªÙ„Ø®ÙŠØµ Ù…Ù„ÙØ§Øª PDF Ø§Ù„Ø·ÙˆÙŠÙ„Ø© Ø¬Ø¯Ø§Ù‹ ÙÙŠ Ø«ÙˆØ§Ù†Ù.",
        "ÙƒÙŠÙÙŠØ© ØªÙØ¹ÙŠÙ„ Ù…ÙŠØ²Ø© Ø¹Ø²Ù„ Ø§Ù„ØµÙˆØª Ø§Ù„Ù…Ø­ÙŠØ·ÙŠ ÙÙŠ Ø§Ù„Ù…ÙƒØ§Ù„Ù…Ø§Øª Ø§Ù„Ù…Ø²Ø¯Ø­Ù…Ø© Ù„Ù„Ø¢ÙŠÙÙˆÙ† ÙˆØ§Ù„Ø£Ù†Ø¯Ø±ÙˆÙŠØ¯.",
        "ØªØ·Ø¨ÙŠÙ‚ Ù…Ø®ÙÙŠ Ø£Ùˆ Ù…ÙŠØ²Ø© ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙØ© Ù„ØªØ­Ø³ÙŠÙ† Ø£Ø¯Ø§Ø¡ Ø¨Ø·Ø§Ø±ÙŠØ© Ø§Ù„Ù‡Ø§ØªÙ ÙˆØ¥ÙŠÙ‚Ø§Ù Ø§Ø³ØªÙ†Ø²Ø§Ù Ø§Ù„Ø®Ù„ÙÙŠØ©."
    ]
    topic = random.choice(tech_topics)

    prompt = f"""Ø§ÙƒØªØ¨ Ø³Ù„Ø³Ù„Ø© Ù…Ù† {tweets_per_thread} ØªØºØ±ÙŠØ¯Ø§Øª ØªÙØµÙŠÙ„ÙŠØ© ØªØ´Ø±Ø­ Ù‡Ø°Ø§ Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹: ({topic}).
ÙŠØ¬Ø¨ Ø£Ù† ØªÙ‚Ø¯Ù… Ø§Ù„Ø³Ù„Ø³Ù„Ø© Ù‚ÙŠÙ…Ø© Ù…Ø¶Ø§ÙØ© Ø­Ù‚ÙŠÙ‚ÙŠØ© ÙˆÙ…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¯Ù‚ÙŠÙ‚Ø© (Ø§Ø³ØªØºÙ„ Ù…Ø³Ø§Ø­Ø© X Premium).

Ø§Ù„Ù‚Ø§Ù„Ø¨ Ø§Ù„Ø¥Ù„Ø²Ø§Ù…ÙŠ:
Ø§Ù„ØªØºØ±ÙŠØ¯Ø© 1: Ø§Ø¯Ø®Ù„ ÙÙŠ ØµÙ„Ø¨ Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹ Ù…Ø¨Ø§Ø´Ø±Ø©. Ø§Ø´Ø±Ø­ Ø§Ù„ÙØ§Ø¦Ø¯Ø© Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠØ© ÙˆÙ„Ù…Ø§Ø°Ø§ ÙŠØ­ØªØ§Ø¬ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ù‡Ø°Ù‡ Ø§Ù„Ù…ÙŠØ²Ø© (ØªØ¬Ù†Ø¨ Ø§Ù„Ù…Ù‚Ø¯Ù…Ø§Øª Ø§Ù„ÙØ§Ø±ØºØ©).
Ø§Ù„ØªØºØ±ÙŠØ¯Ø© 2: Ø§Ø´Ø±Ø­ Ø¨ØªØ¹Ù…Ù‚ ÙƒÙŠÙ ØªØ¹Ù…Ù„ Ø§Ù„Ù…ÙŠØ²Ø©ØŒ ÙˆØ£ÙŠÙ† ØªÙˆØ¬Ø¯ Ø¨Ø§Ù„Ø¶Ø¨Ø· ÙÙŠ Ø§Ù„Ù†Ø¸Ø§Ù… Ø£Ùˆ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.
Ø§Ù„ØªØºØ±ÙŠØ¯Ø© 3: Ø§ÙƒØªØ¨ "Ø§Ù„Ø®Ø·ÙˆØ§Øª Ø§Ù„Ø¹Ù…Ù„ÙŠØ©" 1ØŒ 2ØŒ 3 Ø¨Ø´ÙƒÙ„ ÙˆØ§Ø¶Ø­ Ø¬Ø¯Ø§Ù‹ Ù„ÙŠØªÙ…ÙƒÙ† Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ù…Ù† ØªØ·Ø¨ÙŠÙ‚Ù‡Ø§ ÙÙˆØ±Ø§Ù‹ØŒ ÙˆØ§Ø®ØªÙ… Ø¨Ù‡Ø§Ø´ØªØ§Ø¬ÙŠÙ†.

ÙŠØ¬Ø¨ Ø§Ù„ÙØµÙ„ Ø¨ÙŠÙ† ÙƒÙ„ ØªØºØ±ÙŠØ¯Ø© ÙˆØ£Ø®Ø±Ù‰ Ø¨Ø³Ø·Ø±ÙŠÙ† ÙØ§Ø±ØºÙŠÙ† (\\n\\n)."""

    system = "Ø£Ù†Øª Ø­Ø³Ø§Ø¨ ØªÙ‚Ù†ÙŠ Ø§Ø­ØªØ±Ø§ÙÙŠ ÙŠÙ‚Ø¯Ù… Ù…Ø­ØªÙˆÙ‰ Ø¹Ø§Ù„ÙŠ Ø§Ù„Ø¬ÙˆØ¯Ø©. Ù„Ø§ ØªØ³ØªØ®Ø¯Ù… Ø£ÙŠ Ø£Ø³Ù…Ø§Ø¡ Ø´Ø®ØµÙŠØ©. Ù„Ø§ ØªÙƒØªØ¨ Ù…Ù‚Ø¯Ù…Ø§Øª ÙØ§Ø±ØºØ©. Ù‚Ø¯Ù… Ù…Ø¹Ù„ÙˆÙ…Ø© ØªÙ‚Ù†ÙŠØ© Ù…Ø±ÙƒØ²Ø©ØŒ Ù…ÙØµÙ„Ø©ØŒ ÙˆÙ…ÙÙŠØ¯Ø© Ø¬Ø¯Ø§Ù‹ Ù„Ù„Ù‚Ø§Ø±Ø¦ Ø¨Ø£Ø³Ù„ÙˆØ¨ Ø®Ù„ÙŠØ¬ÙŠ ÙˆØ§Ø¶Ø­ ÙˆÙ…Ø¨Ø§Ø´Ø±."
    
    raw_content = await brain.generate(prompt, system)
    if not raw_content: return
        
    tweets = [content_filter(t) for t in raw_content.split('\n\n') if len(t.strip()) > 10][:tweets_per_thread]
    if len(tweets) < tweets_per_thread: return
        
    logger.info("ðŸ¦ Ø¬Ø§Ø±ÙŠ Ù†Ø´Ø± Ø§Ù„Ø³Ù„Ø³Ù„Ø© Ø§Ù„Ù†ØµÙŠØ©... Ø¥Ù„ÙŠÙƒ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø°ÙŠ Ø³ÙŠØªÙ… Ù†Ø´Ø±Ù‡:")
    for idx, t in enumerate(tweets):
        logger.info(f"Ø§Ù„ØªØºØ±ÙŠØ¯Ø© {idx+1}:\n{t}\n---")
        
    try:
        first_tweet = client_v2.create_tweet(text=tweets[0])
        last_id = first_tweet.data['id']
        
        for i in range(1, len(tweets)):
            # Ø£Ù†Ø³Ù†Ø©: ÙØ§ØµÙ„ Ø²Ù…Ù†ÙŠ Ø¹Ø´ÙˆØ§Ø¦ÙŠ Ø¨ÙŠÙ† 10 Ùˆ 25 Ø«Ø§Ù†ÙŠØ© Ù„ÙŠÙˆØ­ÙŠ Ø¨Ø£Ù† Ù‡Ù†Ø§Ùƒ Ù…Ù† ÙŠÙƒØªØ¨
            delay = random.randint(10, 25)
            logger.info(f"â³ (Ø£Ù†Ø³Ù†Ø©) Ø¬Ø§Ø±ÙŠ Ø§Ù„Ø§Ù†ØªØ¸Ø§Ø± Ù„Ù…Ø¯Ø© {delay} Ø«Ø§Ù†ÙŠØ© Ù‚Ø¨Ù„ Ù†Ø´Ø± Ø§Ù„ØªØºØ±ÙŠØ¯Ø© Ø§Ù„ØªØ§Ù„ÙŠØ©...")
            await asyncio.sleep(delay)
            
            reply = client_v2.create_tweet(text=tweets[i], in_reply_to_tweet_id=last_id)
            last_id = reply.data['id']
            
        logger.success("âœ… ØªÙ… Ù†Ø´Ø± Ø§Ù„Ø³Ù„Ø³Ù„Ø© Ø§Ù„Ù†ØµÙŠØ© Ø§Ù„Ø¨Ø¯ÙŠÙ„Ø© Ø¨Ù†Ø¬Ø§Ø­!")
    except Exception as e:
        logger.error(f"âŒ ÙØ´Ù„ Ø§Ù„Ù†Ø´Ø± Ø¹Ù„Ù‰ Ù…Ù†ØµØ© X: {e}")

# =========================================================
# ðŸš€ EXECUTION FLOW
# =========================================================
async def run_daily_task():
    for _ in range(daily_videos_count):
        video_data = fetch_tech_video()
        
        if not video_data: 
            logger.warning("âš ï¸ Ù„Ø§ ØªÙˆØ¬Ø¯ ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª Ø¬Ø¯ÙŠØ¯Ø© Ù„Ù… ØªÙÙ†Ø´Ø± Ø§Ù„ÙŠÙˆÙ….")
            await post_text_only_thread()
            return

        v_hash = video_data['hash']

        try:
            final_vid = process_video(video_data['url'])
            await post_video_thread(video_data['title'], final_vid)
            
            cursor.execute("INSERT INTO published VALUES (?,?)", (v_hash, datetime.utcnow().isoformat()))
            conn.commit()
            
            for f in ["raw_vid.mp4", "tech_vid.mp4"]:
                if os.path.exists(f): os.remove(f)
                
        except Exception as e:
            logger.error(f"âŒ Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø£Ùˆ Ø±ÙØ¹ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ: {e}")
            await post_text_only_thread()

if __name__ == "__main__":
    logger.info("ðŸš€ Ø¨Ø¯Ø¡ ØªØ´ØºÙŠÙ„ Ø§Ù„Ø³ÙƒØ±Ø¨Øª Ù…Ù† GitHub Actions...")
    asyncio.run(run_daily_task())
    logger.info("ðŸ ØªÙ…Øª Ø§Ù„Ù…Ù‡Ù…Ø© ÙˆØ³ÙŠØªÙ… Ø¥ØºÙ„Ø§Ù‚ Ø§Ù„Ø³ÙƒØ±Ø¨Øª Ù„Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙˆØ§Ø±Ø¯.")
