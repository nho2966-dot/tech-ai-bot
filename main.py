import os
import asyncio
import logging
from datetime import datetime
import openai
import google.generativeai as genai
from loguru import logger
from dotenv import load_dotenv

# ======== Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø¨ÙŠØ¦Ø© ========
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
GEMINI_KEY = os.getenv("GEMINI_KEY")
openai.api_key = OPENAI_API_KEY
genai.configure(api_key=GEMINI_KEY)

# ======== Ø¥Ø¹Ø¯Ø§Ø¯ Logger ========
logger.add("bot_log.log", rotation="5 MB", level="INFO")

# ======== Ø¯ÙˆØ§Ù„ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ ========
async def call_gemini_model(model_name: str, prompt: str):
    """ØªÙˆÙ„ÙŠØ¯ Ù…Ø­ØªÙˆÙ‰ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Gemini"""
    try:
        response = genai.generate_text(model=model_name, prompt=prompt)
        return response.text
    except Exception as e:
        raise RuntimeError(f"Gemini Error: {e}")

async def call_openai_model(model_name: str, prompt: str):
    """ØªÙˆÙ„ÙŠØ¯ Ù…Ø­ØªÙˆÙ‰ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… OpenAI"""
    try:
        response = openai.ChatCompletion.create(
            model=model_name,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.7
        )
        return response.choices[0].message.content
    except Exception as e:
        raise RuntimeError(f"OpenAI Error: {e}")

async def get_available_gemini_models():
    """Ø¥Ø±Ø¬Ø§Ø¹ Ù‚Ø§Ø¦Ù…Ø© Ù†Ù…Ø§Ø°Ø¬ Gemini Ø§Ù„Ù…ØªØ§Ø­Ø©"""
    try:
        models = genai.list_models()
        return [m.name for m in models if "gemini" in m.name.lower()]
    except Exception as e:
        logger.warning(f"âš ï¸ Ø®Ø·Ø£ ÙÙŠ Ø¬Ù„Ø¨ Ù†Ù…Ø§Ø°Ø¬ Gemini: {e}")
        return []

# ======== Ø¯Ø§Ù„Ø© Ø§Ù„ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø°ÙƒÙŠØ© ========
async def generate_ultra_content(prompt: str, retries: int = 3):
    """ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ù…Ø¹ fallback Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠ"""
    gemini_models = await get_available_gemini_models()
    fallback_models = ["gpt-4.1", "gpt-3.5-turbo"]

    for attempt in range(1, retries + 1):
        logger.info(f"ğŸ› ï¸ Ù…Ø­Ø§ÙˆÙ„Ø© ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø±Ù‚Ù… {attempt}")
        # ØªØ¬Ø±Ø¨Ø© Ù†Ù…Ø§Ø°Ø¬ Gemini Ø£ÙˆÙ„Ø§Ù‹
        for model_name in gemini_models:
            try:
                content = await call_gemini_model(model_name, prompt)
                logger.info(f"âœ… ØªÙ… Ø§Ù„ØªÙˆÙ„ÙŠØ¯ Ø¨Ù†Ø¬Ø§Ø­ Ø¨ÙˆØ§Ø³Ø·Ø© {model_name}")
                return content
            except Exception as e:
                logger.error(f"âŒ Ø®Ø·Ø£ Gemini {model_name}: {e}")

        # Ø¥Ø°Ø§ ÙØ´Ù„ ÙƒÙ„ Gemini Ù†Ù†ØªÙ‚Ù„ Ø¥Ù„Ù‰ OpenAI
        for model_name in fallback_models:
            try:
                content = await call_openai_model(model_name, prompt)
                logger.info(f"âœ… ØªÙ… Ø§Ù„ØªÙˆÙ„ÙŠØ¯ Ø¨Ù†Ø¬Ø§Ø­ Ø¨ÙˆØ§Ø³Ø·Ø© {model_name}")
                return content
            except Exception as e:
                logger.error(f"âŒ Ø®Ø·Ø£ OpenAI {model_name}: {e}")

        await asyncio.sleep(2)  # ØªØ£Ø®ÙŠØ± Ù‚Ø¨Ù„ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„ØªØ§Ù„ÙŠØ©

    logger.error("âŒ ÙØ´Ù„ ÙƒÙ„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø¨Ø¹Ø¯ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø§Øª Ø§Ù„Ù…ØªØ¹Ø¯Ø¯Ø©")
    return None

# ======== Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© ========
async def main():
    logger.info("ğŸ”¥ ØªØ´ØºÙŠÙ„ Ù…Ø­Ø±Ùƒ Apex Ø§Ù„Ø°ÙƒÙŠ")
    prompt = "Ø§ÙƒØªØ¨ Ù…Ø­ØªÙˆÙ‰ ØªÙ‚Ù†ÙŠ Ù…ØªÙ†ÙˆØ¹ Ø¬Ø§Ù‡Ø² Ù„Ù„Ù†Ø´Ø± Ø¹Ù„Ù‰ ØªÙˆÙŠØªØ± ÙˆTelegram"
    content = await generate_ultra_content(prompt)

    if content:
        logger.info(f"ğŸ“ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ:\n{content}")
        # Ù‡Ù†Ø§ ÙŠÙ…ÙƒÙ† Ø¥Ø¶Ø§ÙØ© Ù†Ø´Ø± Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø¹Ù„Ù‰ X Ø£Ùˆ Telegram
    else:
        logger.warning("âš ï¸ Ù„Ù… ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯ Ø£ÙŠ Ù…Ø­ØªÙˆÙ‰ Ù„Ù„Ù†Ø´Ø±")

    logger.info("ğŸ ØªÙ…Øª Ø§Ù„Ù…Ù‡Ù…Ø©.")

# ======== ØªØ´ØºÙŠÙ„ Ø§Ù„Ø¨ÙˆØª ========
if __name__ == "__main__":
    asyncio.run(main())
