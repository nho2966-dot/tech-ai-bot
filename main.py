import os
import asyncio
import httpx
import tweepy
import sqlite3
import hashlib
import random
import re
import subprocess
import yt_dlp
import time
from datetime import datetime
from loguru import logger

# =========================================================
# ğŸ” KEYS & AUTH
# =========================================================
OPENAI_KEY = os.getenv("OPENAI_API_KEY")
XAI_KEY = os.getenv("XAI_API_KEY")
QWEN_KEY = os.getenv("QWEN_API_KEY")
X_KEY = os.getenv("X_API_KEY")
X_SECRET = os.getenv("X_API_SECRET")
X_TOKEN = os.getenv("X_ACCESS_TOKEN")
X_ACCESS_S = os.getenv("X_ACCESS_SECRET")
BEARER_TOKEN = os.getenv("BEARER_TOKEN")

auth = tweepy.OAuth1UserHandler(X_KEY, X_SECRET, X_TOKEN, X_ACCESS_S)
api_v1 = tweepy.API(auth)
client_v2 = tweepy.Client(
    bearer_token=BEARER_TOKEN,
    consumer_key=X_KEY, consumer_secret=X_SECRET,
    access_token=X_TOKEN, access_token_secret=X_ACCESS_S,
    wait_on_rate_limit=True 
)

# =========================================================
# ğŸ—„ DATABASE
# =========================================================
conn = sqlite3.connect("nasser_sovereign_flexible.db")
cursor = conn.cursor()
cursor.execute("CREATE TABLE IF NOT EXISTS published (hash TEXT PRIMARY KEY, time TEXT)")
conn.commit()

# =========================================================
# âš™ï¸ CONFIG
# =========================================================
daily_videos_count = 1
video_length_seconds = 45
tweets_per_thread = 3

# =========================================================
# ğŸ›¡ FIXED FILTER (ØªÙ… Ø¥ØµÙ„Ø§Ø­ Ø§Ù„Ø®Ø·Ø£ Ù‡Ù†Ø§)
# =========================================================
def nasser_filter(text):
    if text is None or not isinstance(text, str): 
        return ""
    
    # Ø§Ø³ØªØ¨Ø¯Ø§Ù„ Ø§Ù„Ù…ØµØ·Ù„Ø­ Ø§Ù„Ù…ØªÙÙ‚ Ø¹Ù„ÙŠÙ‡
    text = text.replace("Ø§Ù„Ø«ÙˆØ±Ø© Ø§Ù„ØµÙ†Ø§Ø¹ÙŠØ© Ø§Ù„Ø±Ø§Ø¨Ø¹Ø©", "Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ÙˆØ£Ø­Ø¯Ø« Ø£Ø¯ÙˆØ§ØªÙ‡")
    
    # Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…Ù…Ù†ÙˆØ¹Ø©
    banned = ["stock","market","investment","funding","revenue","profit","Ø³Ù‡Ù…","ØªØ¯Ø§ÙˆÙ„","Ø¹Ù…Ù„Ø©","cryptocurrency","Ø¨ÙŠØªÙƒÙˆÙŠÙ†"]
    
    # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†Øµ Ù…Ù† Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…Ù…Ù†ÙˆØ¹Ø©
    for word in banned: 
        text = re.sub(rf"\b{word}\b", "", text, flags=re.IGNORECASE)
    
    # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø±Ù…ÙˆØ² Ø§Ù„ØºØ±ÙŠØ¨Ø© ÙˆØ§Ù„Ø¥Ø¨Ù‚Ø§Ø¡ Ø¹Ù„Ù‰ Ø§Ù„Ø­Ø±ÙˆÙ ÙˆØ§Ù„Ø£Ø±Ù‚Ø§Ù… Ù„Ø¶Ù…Ø§Ù† Ø¹Ø¯Ù… ØªØ¹Ø·Ù„ Ø§Ù„ØªØºØ±ÙŠØ¯Ø©
    text = re.sub(r'[^\u0600-\u06FFa-zA-Z0-9\s\.\!\?\(\)\ØŒ\:\-]', '', text)
    
    return text.strip()

# =========================================================
# ğŸ§  SOVEREIGN BRAIN
# =========================================================
class SovereignBrain:
    async def generate(self, prompt, system_msg):
        brains = [
            ("GROK", "https://api.x.ai/v1/chat/completions", {"Authorization": f"Bearer {XAI_KEY}"}, "grok-beta"),
            ("OPENAI", "https://api.openai.com/v1/chat/completions", {"Authorization": f"Bearer {OPENAI_KEY}"}, "gpt-4o-mini")
        ]
        for name, url, headers, model in brains:
            try:
                async with httpx.AsyncClient(timeout=60) as client:
                    payload = {
                        "model": model,
                        "messages": [{"role": "system", "content": system_msg}, {"role": "user", "content": prompt}]
                    }
                    r = await client.post(url, headers=headers, json=payload)
                    r.raise_for_status()
                    res = r.json()['choices'][0]['message']['content']
                    if res: return res
            except Exception as e:
                logger.warning(f"âš ï¸ Brain {name} failed: {e}")
                continue
        return "Ø®Ø¨Ø§ÙŠØ§ ØªÙ‚Ù†ÙŠØ© Ø¬Ø¯ÙŠØ¯Ø© Ù†ÙƒØ´ÙÙ‡Ø§ Ù„ÙƒÙ… ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ù…Ù‚Ø·Ø¹.."

brain = SovereignBrain()

# =========================================================
# ğŸ¥ MULTI-SOURCE RADAR
# =========================================================
TRUSTED_CHANNELS = [
    "https://www.youtube.com/@mkbhd",
    "https://www.youtube.com/@Mrwhosetheboss",
    "https://www.youtube.com/@ProperHonestTech",
    "https://www.youtube.com/@HowToMen",
    "https://www.youtube.com/@MattWolfe",
    "https://www.youtube.com/@TheAIAdvantage"
]

def fetch_tech_video():
    logger.info("ğŸ” Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø®Ø¨Ø§ÙŠØ§ ØªÙ‚Ù†ÙŠØ© Ø¬Ø¯ÙŠØ¯Ø©...")
    ydl_opts = {'quiet': True, 'extract_flat': True, 'daterange': yt_dlp.utils.DateRange('now-2days','now')}
    random.shuffle(TRUSTED_CHANNELS)
    
    with yt_dlp.YoutubeDL(ydl_opts) as ydl:
        for channel in TRUSTED_CHANNELS:
            try:
                res = ydl.extract_info(channel, download=False)
                if 'entries' in res and res['entries']:
                    for video in res['entries'][:5]:
                        title = video.get('title','')
                        v_url = video.get('url')
                        if not title or not v_url: continue
                        
                        if any(w in title.lower() for w in ["stock","market","earnings","invest"]):
                            continue
                            
                        v_hash = hashlib.sha256(title.encode()).hexdigest()
                        cursor.execute("SELECT hash FROM published WHERE hash=?", (v_hash,))
                        if cursor.fetchone(): continue 
                            
                        return {"title": title, "url": v_url, "hash": v_hash}
            except Exception as e:
                logger.warning(f"âš ï¸ ÙØ´Ù„ Ø§Ù„Ø¬Ù„Ø¨ Ù…Ù† {channel}: {e}")
    return None

# =========================================================
# ğŸ¬ VIDEO PROCESSING
# =========================================================
def process_video(url):
    logger.info("ğŸ¬ ØªØ­Ù…ÙŠÙ„ ÙˆÙ…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙÙŠØ¯ÙŠÙˆ...")
    output_raw = "raw_vid.mp4"
    output_final = "nasser_vid.mp4"
    
    if os.path.exists(output_raw): os.remove(output_raw)
    if os.path.exists(output_final): os.remove(output_final)
    
    ydl_opts = {'format':'bestvideo[ext=mp4]+bestaudio[ext=m4a]/best[ext=mp4]', 'outtmpl': output_raw, 'quiet': True}
    with yt_dlp.YoutubeDL(ydl_opts) as ydl: ydl.download([url])
    
    cmd = [
        "ffmpeg", "-y", "-i", output_raw, "-t", str(video_length_seconds),
        "-vf", "scale=1080:1920:force_original_aspect_ratio=increase,crop=1080:1920,fps=30",
        "-c:v", "libx264", "-crf", "23", "-preset", "fast", "-c:a", "aac", "-b:a", "128k", output_final
    ]
    subprocess.run(cmd, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    return output_final

# =========================================================
# ğŸ¦ THREAD POSTING
# =========================================================
async def post_nasser_thread(title, video_path):
    # ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù…Ø­ØªÙˆÙ‰
    prompt = f"Ø­ÙˆÙ„ Ù‡Ø°Ø§ Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹ Ø§Ù„ØªÙ‚Ù†ÙŠ Ø¥Ù„Ù‰ Ø³Ù„Ø³Ù„Ø© ØªØºØ±ÙŠØ¯Ø§Øª (Thread) Ø®Ù„ÙŠØ¬ÙŠØ© Ø¹Ù† Ø§Ù„Ø®Ø¨Ø§ÙŠØ§: {title}. Ù‚Ø³Ù…Ù‡Ø§ Ù„Ù€ {tweets_per_thread} ØªØºØ±ÙŠØ¯Ø§Øª Ù…Ù†ÙØµÙ„Ø© Ø¨Ø£Ø³Ø·Ø± ÙØ§Ø±ØºØ©."
    system = "Ø£Ù†Øª Ù†Ø§ØµØ±ØŒ Ø®Ø¨ÙŠØ± Ø®Ø¨Ø§ÙŠØ§ Ø§Ù„Ø£Ø¬Ù‡Ø²Ø© ÙˆØ£Ø³Ø±Ø§Ø± Ø§Ù„Ø¥Ù†ØªØ±Ù†Øª ÙˆØ§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ÙˆØ£Ø­Ø¯Ø« Ø£Ø¯ÙˆØ§ØªÙ‡. Ø§Ø³ØªØ¹Ù…Ù„ Ù„Ù‡Ø¬Ø© Ø®Ù„ÙŠØ¬ÙŠØ© Ù…Ø±Ù…ÙˆÙ‚Ø©."
    
    raw_content = await brain.generate(prompt, system)
    # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ ÙˆØªÙ‚Ø³ÙŠÙ…Ù‡
    raw_tweets = [t.strip() for t in raw_content.split('\n\n') if t.strip()]
    tweets = [nasser_filter(t) for t in raw_tweets][:tweets_per_thread]
    
    if not tweets:
        tweets = ["Ø®Ø¨Ø§ÙŠØ§ ØªÙ‚Ù†ÙŠØ© Ø¬Ø¯ÙŠØ¯Ø© Ù†ÙƒØ´ÙÙ‡Ø§ Ù„ÙƒÙ… ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ù…Ù‚Ø·Ø¹! #ØªÙ‚Ù†ÙŠØ©"]

    logger.info("ğŸ¦ Ø±ÙØ¹ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¥Ù„Ù‰ Ù…Ù†ØµØ© X...")
    media = api_v1.media_upload(video_path, media_category='tweet_video', chunked=True)
    
    # Ø§Ù†ØªØ¸Ø§Ø± Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© (Loop Ù…Ø­ØµÙ†)
    check_count = 0
    while check_count < 20:
        status = api_v1.get_media_upload_status(media.media_id)
        state = status.processing_info.get("state")
        if state == "succeeded":
            break
        elif state == "failed":
            raise Exception("ÙØ´Ù„Øª Ù…Ù†ØµØ© X ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙÙŠØ¯ÙŠÙˆ")
        
        logger.info(f"â³ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ù…Ø³ØªÙ…Ø±Ø©... (Ù…Ø­Ø§ÙˆÙ„Ø© {check_count+1})")
        time.sleep(10)
        check_count += 1
    
    # Ù†Ø´Ø± Ø§Ù„Ø³Ù„Ø³Ù„Ø©
    logger.info("ğŸš€ Ù†Ø´Ø± Ø§Ù„Ø³Ù„Ø³Ù„Ø©...")
    first_tweet = client_v2.create_tweet(text=tweets[0][:280], media_ids=[media.media_id])
    last_id = first_tweet.data['id']
    
    for i in range(1, len(tweets)):
        time.sleep(2) # ØªØ£Ø®ÙŠØ± Ø¨Ø³ÙŠØ· Ù„Ø¶Ù…Ø§Ù† Ø§Ù„ØªØ±ØªÙŠØ¨
        reply = client_v2.create_tweet(text=tweets[i][:280], in_reply_to_tweet_id=last_id)
        last_id = reply.data['id']
    
    logger.success("âœ… ØªÙ… Ù†Ø´Ø± Ø§Ù„Ø³Ù„Ø³Ù„Ø© Ø§Ù„ØªÙ‚Ù†ÙŠØ© Ø¨Ù†Ø¬Ø§Ø­!")

# =========================================================
# ğŸš€ EXECUTION
# =========================================================
async def run_daily_task():
    video_data = fetch_tech_video()
    if not video_data:
        logger.info("âš ï¸ Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ù…Ø­ØªÙˆÙ‰ Ø¬Ø¯ÙŠØ¯ ÙŠØ·Ø§Ø¨Ù‚ Ø§Ù„Ø´Ø±ÙˆØ· Ø­Ø§Ù„ÙŠØ§Ù‹.")
        return

    try:
        final_vid = process_video(video_data['url'])
        await post_nasser_thread(video_data['title'], final_vid)
        
        # ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ù†Ø¬Ø§Ø­
        cursor.execute("INSERT INTO published VALUES (?,?)", (video_data['hash'], datetime.utcnow().isoformat()))
        conn.commit()
        
        # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…Ù„ÙØ§Øª
        for f in ["raw_vid.mp4", "nasser_vid.mp4"]:
            if os.path.exists(f): os.remove(f)
            
    except Exception as e:
        logger.error(f"âŒ ÙØ´Ù„ Ø§Ù„Ø³ÙƒØ±Ø¨Øª: {e}")

if __name__ == "__main__":
    logger.info("ğŸš€ Ø¨Ø¯Ø¡ ØªØ´ØºÙŠÙ„ Ø§Ù„Ø³ÙƒØ±Ø¨Øª...")
    asyncio.run(run_daily_task())
    logger.info("ğŸ ØªÙ…Øª Ø§Ù„Ù…Ù‡Ù…Ø©.")
