import os, sqlite3, logging, hashlib, time, re, random
from datetime import datetime, timedelta
import tweepy, feedparser
from dotenv import load_dotenv
from openai import OpenAI

load_dotenv()
DB_FILE = "news.db"
LOG_FILE = "error.log"

# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø³Ø¬Ù„Ø§Øª
logging.basicConfig(level=logging.INFO, format="ğŸ›¡ï¸ %(asctime)s - %(message)s", 
                    handlers=[logging.FileHandler(LOG_FILE), logging.StreamHandler()])

class TechEliteSmartFilter:
    def __init__(self):
        self._init_db()
        self._init_clients()

    def _init_db(self):
        with sqlite3.connect(DB_FILE) as conn:
            # Ø¥Ø¶Ø§ÙØ© Ø¹Ù…ÙˆØ¯ keywords Ù„Ù…Ù†Ø¹ Ø§Ù„ØªÙƒØ±Ø§Ø± Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹ÙŠ
            conn.execute("""
                CREATE TABLE IF NOT EXISTS news (
                    hash TEXT PRIMARY KEY, 
                    title TEXT, 
                    keywords TEXT, 
                    published_at TEXT
                )
            """)

    def _init_clients(self):
        self.x_client = tweepy.Client(
            bearer_token=os.getenv("X_BEARER_TOKEN"),
            consumer_key=os.getenv("X_API_KEY"),
            consumer_secret=os.getenv("X_API_SECRET"),
            access_token=os.getenv("X_ACCESS_TOKEN"),
            access_token_secret=os.getenv("X_ACCESS_SECRET")
        )
        self.ai_client = OpenAI(base_url="https://openrouter.ai/api/v1", api_key=os.getenv("OPENROUTER_API_KEY"))

    def _extract_keywords(self, text):
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© Ù„Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ù…ÙˆØ¶ÙˆØ¹ÙŠØ§Ù‹"""
        # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†Øµ ÙˆØ§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù‡Ø§Ù…Ø© (ØªØ¨Ø³ÙŠØ· Ù„Ù„Ù…Ø«Ø§Ù„)
        words = re.findall(r'\w+', text.lower())
        stop_words = {'the', 'a', 'in', 'on', 'at', 'for', 'with', 'microsoft', 'google'}
        keywords = [w for w in words if len(w) > 3 and w not in stop_words]
        return ",".join(list(set(keywords))[:5]) # Ø­ÙØ¸ Ø£Ù‡Ù… 5 ÙƒÙ„Ù…Ø§Øª

    def _is_duplicate_topic(self, new_keywords):
        """Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ù…ÙˆØ¶ÙˆØ¹ Ù…Ø´Ø§Ø¨Ù‡ Ù†ÙØ´Ø± Ø®Ù„Ø§Ù„ Ø§Ù„Ù€ 24 Ø³Ø§Ø¹Ø© Ø§Ù„Ù…Ø§Ø¶ÙŠØ©"""
        with sqlite3.connect(DB_FILE) as conn:
            yesterday = (datetime.now() - timedelta(days=1)).isoformat()
            cursor = conn.execute("SELECT keywords FROM news WHERE published_at > ?", (yesterday,))
            existing_keywords = cursor.fetchall()
            
            new_set = set(new_keywords.split(','))
            for row in existing_keywords:
                existing_set = set(row[0].split(','))
                # Ø¥Ø°Ø§ ÙˆØ¬Ø¯ ØªØ·Ø§Ø¨Ù‚ ÙÙŠ 3 ÙƒÙ„Ù…Ø§Øª Ù…ÙØªØ§Ø­ÙŠØ© Ø£Ùˆ Ø£ÙƒØ«Ø±ØŒ Ù†Ø¹ØªØ¨Ø±Ù‡ Ù…ÙƒØ±Ø±Ø§Ù‹
                if len(new_set.intersection(existing_set)) >= 3:
                    return True
        return False

    def run_cycle(self):
        # ... (Ø¬Ù„Ø¨ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ù…Ù† Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø§Ù„Ù…Ø°ÙƒÙˆØ±Ø© Ø³Ø§Ø¨Ù‚Ø§Ù‹)
        for url in SOURCES:
            feed = feedparser.parse(url)
            for e in feed.entries[:5]:
                h = hashlib.sha256(e.title.encode()).hexdigest()
                current_keywords = self._extract_keywords(e.title)

                # ÙÙ„ØªØ±Ø© Ù…Ø²Ø¯ÙˆØ¬Ø©: (Ø¨Ø§Ù„Ø±Ø§Ø¨Ø· Ø§Ù„Ù€ Hash) + (Ø¨Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹ Keywords)
                if not self._is_duplicate_topic(current_keywords):
                    ai_text = self._generate_ai(f"Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹: {e.title}")
                    if ai_text and self.post_thread(ai_text, e.link, e.title):
                        # Ø­ÙØ¸ Ø§Ù„Ø®Ø¨Ø± Ù…Ø¹ Ø¨ØµÙ…ØªÙ‡ Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹ÙŠØ©
                        with sqlite3.connect(DB_FILE) as conn:
                            conn.execute("INSERT INTO news VALUES (?, ?, ?, ?)", 
                                         (h, e.title, current_keywords, datetime.now().isoformat()))
                        break # Ù†Ø´Ø± Ø®Ø¨Ø± ÙˆØ§Ø­Ø¯ ÙˆØ§Ù„Ø§Ù†ØªÙ‚Ø§Ù„ Ù„Ù„Ø¯ÙˆØ±Ø© Ø§Ù„ØªØ§Ù„ÙŠØ© Ù„Ø¶Ù…Ø§Ù† Ø§Ù„ØªÙ‡Ø¯Ø¦Ø©
