import os
import asyncio
import httpx
import tweepy
import sqlite3
import hashlib
import random
import re
import subprocess
import yt_dlp
import time
from datetime import datetime
from loguru import logger
import schedule

# =========================================================
# ğŸ” KEYS & AUTH
# =========================================================
OPENAI_KEY = os.getenv("OPENAI_API_KEY")
XAI_KEY = os.getenv("XAI_API_KEY")        # Grok
QWEN_KEY = os.getenv("QWEN_API_KEY")
X_KEY = os.getenv("X_API_KEY")
X_SECRET = os.getenv("X_API_SECRET")
X_TOKEN = os.getenv("X_ACCESS_TOKEN")
X_ACCESS_S = os.getenv("X_ACCESS_SECRET")
BEARER_TOKEN = os.getenv("BEARER_TOKEN")

auth = tweepy.OAuth1UserHandler(X_KEY, X_SECRET, X_TOKEN, X_ACCESS_S)
api_v1 = tweepy.API(auth)
client_v2 = tweepy.Client(
    bearer_token=BEARER_TOKEN,
    consumer_key=X_KEY, consumer_secret=X_SECRET,
    access_token=X_TOKEN, access_token_secret=X_ACCESS_S,
    wait_on_rate_limit=True
)

# =========================================================
# ğŸ—„ DATABASE
# =========================================================
conn = sqlite3.connect("nasser_sovereign_flexible.db")
cursor = conn.cursor()
cursor.execute("CREATE TABLE IF NOT EXISTS published (hash TEXT PRIMARY KEY, time TEXT)")
conn.commit()

# =========================================================
# âš™ï¸ CONFIGURABLE PARAMETERS
# =========================================================
daily_videos_count = 2           # Ø¹Ø¯Ø¯ Ø§Ù„ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª Ø§Ù„ÙŠÙˆÙ…ÙŠØ©
video_length_seconds = 45        # Ù…Ø¯Ø© Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ø§Ù„Ø«ÙˆØ§Ù†ÙŠ
tweets_per_thread = 3            # Ø¹Ø¯Ø¯ Ø§Ù„ØªØºØ±ÙŠØ¯Ø§Øª Ù„ÙƒÙ„ Ø³Ù„Ø³Ù„Ø©

# =========================================================
# ğŸ›¡ IMPROVED FILTER
# =========================================================
def nasser_filter(text):
    if not text: return ""
    text = text.replace("Ø§Ù„Ø«ÙˆØ±Ø© Ø§Ù„ØµÙ†Ø§Ø¹ÙŠØ© Ø§Ù„Ø±Ø§Ø¨Ø¹Ø©", "Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ÙˆØ£Ø­Ø¯Ø« Ø£Ø¯ÙˆØ§ØªÙ‡")
    
    # Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…Ù…Ù†ÙˆØ¹Ø©
    banned = [
        "stock","market","investment","funding","revenue","profit","Ø³Ù‡Ù…","ØªØ¯Ø§ÙˆÙ„","Ø¹Ù…Ù„Ø©","cryptocurrency","Ø¨ÙŠØªÙƒÙˆÙŠÙ†"
    ]
    for word in banned: 
        text = re.sub(rf"\b{word}\b", "", text, flags=re.IGNORECASE)
        
    return text.strip()

# =========================================================
# ğŸ§  SOVEREIGN BRAIN
# =========================================================
class SovereignBrain:
    async def generate(self, prompt, system_msg):
        brains = [
            ("GROK", "https://api.x.ai/v1/chat/completions", {"Authorization": f"Bearer {XAI_KEY}"}, "grok-beta"),
            ("OPENAI", "https://api.openai.com/v1/chat/completions", {"Authorization": f"Bearer {OPENAI_KEY}"}, "gpt-4o-mini"),
            ("QWEN", "https://api.labs.qwen.ai/v1/chat/completions", {"Authorization": f"Bearer {QWEN_KEY}"}, "qwen-7b")
        ]
        for name, url, headers, model in brains:
            try:
                async with httpx.AsyncClient(timeout=60) as client:
                    r = await client.post(url, headers=headers, json={
                        "model": model,
                        "messages": [{"role": "system", "content": system_msg}, {"role": "user", "content": prompt}]
                    })
                    r.raise_for_status()
                    return r.json()['choices'][0]['message']['content']
            except Exception as e:
                logger.warning(f"âš ï¸ Brain {name} failed: {e}")
                continue
        return "Ø³Ø± ØªÙ‚Ù†ÙŠ Ø¬Ø¯ÙŠØ¯ ÙÙŠ Ø§Ù„Ø·Ø±ÙŠÙ‚ Ø¥Ù„ÙŠÙƒÙ….."

brain = SovereignBrain()

# =========================================================
# ğŸ¥ MULTI-SOURCE RADAR
# =========================================================
TRUSTED_CHANNELS = [
    "https://www.youtube.com/@mkbhd",
    "https://www.youtube.com/@Mrwhosetheboss",
    "https://www.youtube.com/@ProperHonestTech",
    "https://www.youtube.com/@HowToMen",
    "https://www.youtube.com/@MattWolfe",
    "https://www.youtube.com/@TheAIAdvantage"
]

def fetch_tech_video():
    logger.info("ğŸ” Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø®Ø¨Ø§ÙŠØ§ ØªÙ‚Ù†ÙŠØ© Ø¬Ø¯ÙŠØ¯Ø© Ù„Ù… ØªÙÙ†Ø´Ø± Ù…Ù† Ù‚Ø¨Ù„...")
    # Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø§Ù„ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª Ø§Ù„Ù…Ù†Ø´ÙˆØ±Ø© Ø®Ù„Ø§Ù„ Ø§Ù„ÙŠÙˆÙ…ÙŠÙ† Ø§Ù„Ù…Ø§Ø¶ÙŠÙŠÙ†
    ydl_opts = {'quiet': True, 'extract_flat': True, 'daterange': yt_dlp.utils.DateRange('now-2days','now')}
    random.shuffle(TRUSTED_CHANNELS)
    
    with yt_dlp.YoutubeDL(ydl_opts) as ydl:
        for channel in TRUSTED_CHANNELS:
            try:
                res = ydl.extract_info(channel, download=False)
                if 'entries' in res and res['entries']:
                    # Ø§Ù„Ù…Ø±ÙˆØ± Ø¹Ù„Ù‰ Ø£ÙˆÙ„ 5 ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† Ø§Ù„Ø£ÙˆÙ„ ÙÙ‚Ø·
                    for video in res['entries'][:5]:
                        title = video.get('title','')
                        v_url = video.get('url')
                        
                        # ØªØ®Ø·ÙŠ Ø§Ù„ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª Ø§Ù„ØªÙŠ ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ ÙƒÙ„Ù…Ø§Øª Ù…Ù…Ù†ÙˆØ¹Ø© ÙÙŠ Ø§Ù„Ø¹Ù†ÙˆØ§Ù†
                        if any(w in title.lower() for w in ["stock","market","earnings"]):
                            continue
                            
                        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
                        v_hash = hashlib.sha256(title.encode()).hexdigest()
                        cursor.execute("SELECT hash FROM published WHERE hash=?", (v_hash,))
                        if cursor.fetchone():
                            continue # Ø§Ù„ÙÙŠØ¯ÙŠÙˆ ØªÙ… Ù†Ø´Ø±Ù‡ØŒ Ù†Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ø°ÙŠ ÙŠÙ„ÙŠÙ‡
                            
                        return {"title": title, "url": v_url, "hash": v_hash}
            except Exception as e:
                logger.warning(f"âš ï¸ ÙØ´Ù„ Ø§Ù„Ø¬Ù„Ø¨ Ù…Ù† {channel}: {e}")
                continue
    return None

# =========================================================
# ğŸ¬ VIDEO PROCESSING
# =========================================================
def process_video(url):
    logger.info("ğŸ¬ ØªØ­Ù…ÙŠÙ„ ÙˆÙ…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙÙŠØ¯ÙŠÙˆ...")
    output_raw = "raw_vid.mp4"
    output_final = "nasser_vid.mp4"
    
    ydl_opts = {'format':'bestvideo[ext=mp4]+bestaudio[ext=m4a]/best[ext=mp4]', 'outtmpl': output_raw, 'quiet': True}
    with yt_dlp.YoutubeDL(ydl_opts) as ydl: ydl.download([url])
    
    cmd = [
        "ffmpeg", "-y", "-i", output_raw, "-t", str(video_length_seconds),
        "-vf", "scale=1080:1920:force_original_aspect_ratio=increase,crop=1080:1920",
        "-c:v", "libx264", "-crf", "23", "-preset", "fast", "-c:a", "aac", output_final
    ]
    subprocess.run(cmd, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    return output_final

# =========================================================
# ğŸ¦ THREAD POSTING
# =========================================================
async def post_nasser_thread(title, video_path):
    prompt = f"Ø­ÙˆÙ„ Ù‡Ø°Ø§ Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹ Ø§Ù„ØªÙ‚Ù†ÙŠ Ø¥Ù„Ù‰ Ø³Ù„Ø³Ù„Ø© ØªØºØ±ÙŠØ¯Ø§Øª (Thread) Ø®Ù„ÙŠØ¬ÙŠØ© Ø¹Ù† Ø§Ù„Ø®Ø¨Ø§ÙŠØ§: {title}. Ù‚Ø³Ù…Ù‡Ø§ Ù„Ù€ {tweets_per_thread} ØªØºØ±ÙŠØ¯Ø§Øª."
    system = "Ø£Ù†Øª Ù†Ø§ØµØ±ØŒ Ø®Ø¨ÙŠØ± Ø®Ø¨Ø§ÙŠØ§ Ø§Ù„Ø£Ø¬Ù‡Ø²Ø© ÙˆØ£Ø³Ø±Ø§Ø± Ø§Ù„Ø¥Ù†ØªØ±Ù†Øª ÙˆØ§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ."
    raw_content = await brain.generate(prompt, system)
    tweets = [nasser_filter(t) for t in raw_content.split('\n\n') if t][:tweets_per_thread]
    
    logger.info("ğŸ¦ Ø±ÙØ¹ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ ÙˆØ§Ù„ØªØºØ±ÙŠØ¯Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰...")
    # Ø§Ø³ØªØ®Ø¯Ø§Ù… chunked=True Ù„Ø¶Ù…Ø§Ù† Ø±ÙØ¹ Ø§Ù„ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª Ø¯ÙˆÙ† Ù…Ø´Ø§ÙƒÙ„
    media = api_v1.media_upload(video_path, media_category='tweet_video', chunked=True)
    
    # Ø§Ù†ØªØ¸Ø§Ø± Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø°ÙƒÙŠ Ø¹Ù„Ù‰ Ø³ÙŠØ±ÙØ±Ø§Øª X
    for _ in range(15):
        try:
            status = api_v1.get_media_upload_status(media.media_id)
            if status.processing_info.get("state") == "succeeded":
                break
        except: pass
        logger.info("â³ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ù‚ÙŠØ¯ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¹Ù„Ù‰ X...")
        time.sleep(5)
    
    first_tweet = client_v2.create_tweet(text=tweets[0], media_ids=[media.media_id])
    last_id = first_tweet.data['id']
    
    for i in range(1, len(tweets)):
        reply = client_v2.create_tweet(text=tweets[i], in_reply_to_tweet_id=last_id)
        last_id = reply.data['id']
    
    logger.success("âœ… ØªÙ… Ù†Ø´Ø± Ø§Ù„Ø³Ù„Ø³Ù„Ø© Ø§Ù„ØªÙ‚Ù†ÙŠØ© Ø¨Ù†Ø¬Ø§Ø­!")

# =========================================================
# ğŸ”„ DAILY FLEXIBLE EXECUTION
# =========================================================
async def run_daily_task():
    for _ in range(daily_videos_count):
        video_data = fetch_tech_video()
        if not video_data: 
            logger.info("âš ï¸ Ù„Ø§ ØªÙˆØ¬Ø¯ ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª Ø¬Ø¯ÙŠØ¯Ø© ØºÙŠØ± Ù…Ù†Ø´ÙˆØ±Ø© Ø§Ù„ÙŠÙˆÙ….")
            return

        v_hash = video_data['hash']

        try:
            final_vid = process_video(video_data['url'])
            await post_nasser_thread(video_data['title'], final_vid)
            
            cursor.execute("INSERT INTO published VALUES (?,?)", (v_hash, datetime.utcnow().isoformat()))
            conn.commit()
            
            for f in ["raw_vid.mp4", "nasser_vid.mp4"]:
                if os.path.exists(f): os.remove(f)
        except Exception as e:
            logger.error(f"âŒ Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø£Ùˆ Ø§Ù„Ù†Ø´Ø±: {e}")

def schedule_daily(hour=10, minute=0):
    schedule.clear()
    schedule.every().day.at(f"{hour:02d}:{minute:02d}").do(lambda: asyncio.run(run_daily_task()))
    logger.info(f"ğŸ•’ ØªÙ… Ø¬Ø¯ÙˆÙ„Ø© Ø§Ù„Ù…Ù‡Ù…Ø© Ø§Ù„ÙŠÙˆÙ…ÙŠØ© Ø§Ù„Ø³Ø§Ø¹Ø© {hour:02d}:{minute:02d}")
    
    while True:
        schedule.run_pending()
        time.sleep(30)

# =========================================================
# ğŸš€ START FLEXIBLE DAILY SCHEDULER
# =========================================================
if __name__ == "__main__":
    # Ø¶Ø¨Ø· Ø§Ù„ÙˆÙ‚Øª Ø§Ù„ÙŠÙˆÙ…ÙŠ (Ù…Ø«Ù„Ø§Ù‹ Ø§Ù„Ø³Ø§Ø¹Ø© 10:00 ØµØ¨Ø§Ø­Ù‹Ø§)
    schedule_daily(hour=10, minute=0)
