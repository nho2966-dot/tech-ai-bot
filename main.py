import os
import asyncio
import httpx
import tweepy
import sqlite3
import hashlib
import random
import re
import subprocess
import yt_dlp
import time
from datetime import datetime
from loguru import logger

# =========================================================
# ğŸ” KEYS & AUTH (Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ù…ÙØ§ØªÙŠØ­ ÙˆØ§Ù„Ø±Ù…ÙˆØ²)
# =========================================================
GEMINI_KEY = os.getenv("GEMINI_KEY")
X_KEY = os.getenv("X_API_KEY")
X_SECRET = os.getenv("X_API_SECRET")
X_TOKEN = os.getenv("X_ACCESS_TOKEN")
X_ACCESS_S = os.getenv("X_ACCESS_SECRET")
BEARER_TOKEN = os.getenv("X_BEARER_TOKEN")

# ØªÙ„ÙŠØ¬Ø±Ø§Ù… Ù„Ù„Ø¥Ø´Ø¹Ø§Ø±Ø§Øª (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)
TG_TOKEN = os.getenv("TG_TOKEN")
TELEGRAM_CHAT_ID = os.getenv("TELEGRAM_CHAT_ID")

auth = tweepy.OAuth1UserHandler(X_KEY, X_SECRET, X_TOKEN, X_ACCESS_S)
api_v1 = tweepy.API(auth)
client_v2 = tweepy.Client(
    bearer_token=BEARER_TOKEN,
    consumer_key=X_KEY, consumer_secret=X_SECRET,
    access_token=X_TOKEN, access_token_secret=X_ACCESS_S,
    wait_on_rate_limit=True 
)

# =========================================================
# ğŸ—„ DATABASE (Ù…Ù†Ø¹ Ø§Ù„ØªÙƒØ±Ø§Ø± ÙˆØ§Ù„Ø±Ø¯ÙˆØ¯ Ø§Ù„Ù…ØªÙƒØ±Ø±Ø©)
# =========================================================
conn = sqlite3.connect("nasser_scoops_final.db")
cursor = conn.cursor()
cursor.execute("CREATE TABLE IF NOT EXISTS published (hash TEXT PRIMARY KEY, topic TEXT, time TEXT)")
cursor.execute("CREATE TABLE IF NOT EXISTS interactions (tweet_id TEXT, user_id TEXT, PRIMARY KEY(tweet_id, user_id))")
conn.commit()

# =========================================================
# ğŸ›¡ï¸ THE NASSER FILTER (Ø§Ù„ÙÙ„ØªØ± Ø§Ù„Ø³ÙŠØ§Ø¯ÙŠ)
# =========================================================
def nasser_filter(text):
    if not text: return ""
    # Ø§Ù„Ø§Ù„ØªØ²Ø§Ù… Ø¨Ø§Ù„Ù…ØµØ·Ù„Ø­ Ø§Ù„Ù…ØªÙÙ‚ Ø¹Ù„ÙŠÙ‡
    text = text.replace("Ø§Ù„Ø«ÙˆØ±Ø© Ø§Ù„ØµÙ†Ø§Ø¹ÙŠØ© Ø§Ù„Ø±Ø§Ø¨Ø¹Ø©", "Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ÙˆØ£Ø­Ø¯Ø« Ø£Ø¯ÙˆØ§ØªÙ‡")
    
    # Ù…Ù†Ø¹ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…Ø§Ù„ÙŠØ© ÙˆØ§Ù„Ø±Ù…ÙˆØ² Ø§Ù„ØºØ±ÙŠØ¨Ø© (Ø§Ù„Ø³Ù…Ø§Ø­ Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØŒ Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØŒ Ø§Ù„Ø£Ø±Ù‚Ø§Ù…ØŒ ÙˆØ§Ù„Ø¥ÙŠÙ…ÙˆØ¬ÙŠ ÙÙ‚Ø·)
    banned = ["stock","market","investment","Ø³Ù‡Ù…","ØªØ¯Ø§ÙˆÙ„","Ø¹Ù…Ù„Ø©","crypto"]
    for word in banned:
        text = re.sub(rf"\b{word}\b", "", text, flags=re.IGNORECASE)
    
    # Ø¥Ø²Ø§Ù„Ø© Ù…Ù‚Ø¯Ù…Ø§Øª Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø§Ù„ØªÙ‚Ù„ÙŠØ¯ÙŠØ©
    text = re.sub(r'^(Ø§Ù„ØªØºØ±ÙŠØ¯Ø© \d+:|ØªØºØ±ÙŠØ¯Ø© \d+)\s*', '', text, flags=re.IGNORECASE).strip()
    return text

# =========================================================
# ğŸ§  SCOOP BRAIN (ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø®Ø¨Ø§ÙŠØ§ ÙˆØ§Ù„ØªØ³Ø±ÙŠØ¨Ø§Øª)
# =========================================================
class SovereignBrain:
    async def generate(self, prompt, system_msg):
        # Ø§Ù„ØªØ±ÙƒÙŠØ² Ø¹Ù„Ù‰ Gemini Ù„Ù‚ÙˆØªÙ‡ ÙÙŠ Ø§Ù„Ù„Ù‡Ø¬Ø© Ø§Ù„Ø®Ù„ÙŠØ¬ÙŠØ© ÙˆØ¹Ø¯Ù… Ø§Ù„Ù‡Ù„ÙˆØ³Ø©
        url = "https://generativelanguage.googleapis.com/v1beta/openai/chat/completions"
        headers = {"Authorization": f"Bearer {GEMINI_KEY}"}
        
        # ØªØ²ÙˆÙŠØ¯ Ø§Ù„Ù€ AI Ø¨Ø¢Ø®Ø± 5 Ù…ÙˆØ¶ÙˆØ¹Ø§Øª Ù„Ù…Ù†Ø¹ Ø§Ù„ØªÙƒØ±Ø§Ø± Ø¨ØµÙŠØº Ù…Ø®ØªÙ„ÙØ©
        cursor.execute("SELECT topic FROM published ORDER BY time DESC LIMIT 5")
        past_topics = [row[0] for row in cursor.fetchall()]
        
        full_system = f"{system_msg} | Ø§Ù„Ù…ÙˆØ§Ø¶ÙŠØ¹ Ø§Ù„Ø³Ø§Ø¨Ù‚Ø© (ÙŠÙ…Ù†Ø¹ ØªÙƒØ±Ø§Ø±Ù‡Ø§): {past_topics} | Ø§Ù„Ù„Ù‡Ø¬Ø©: Ø®Ù„ÙŠØ¬ÙŠØ© Ø¨ÙŠØ¶Ø§Ø¡ | Ø§Ù„ØªØ±ÙƒÙŠØ²: Ø®Ø¨Ø§ÙŠØ§ ÙˆØªØ³Ø±ÙŠØ¨Ø§Øª ÙÙ‚Ø·."
        
        try:
            async with httpx.AsyncClient(timeout=60) as client:
                payload = {
                    "model": "gemini-2.5-flash",
                    "messages": [{"role": "system", "content": full_system}, {"role": "user", "content": prompt}]
                }
                r = await client.post(url, headers=headers, json=payload)
                return nasser_filter(r.json()['choices'][0]['message']['content'])
        except Exception as e:
            logger.error(f"âš ï¸ Brain Error: {e}")
            return None

brain = SovereignBrain()

# =========================================================
# ğŸ¥ LEAK RADAR (Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ø¬Ø¯ÙŠØ¯)
# =========================================================
SEARCH_QUERIES = [
    "ytsearch5: AI hidden features 2026",
    "ytsearch5: ChatGPT secret hacks shorts",
    "ytsearch5: new AI tools leaks",
    "ytsearch5: hidden productivity AI tricks"
]

def fetch_leak_video():
    ydl_opts = {'quiet': True, 'extract_flat': True}
    query = random.choice(SEARCH_QUERIES)
    with yt_dlp.YoutubeDL(ydl_opts) as ydl:
        try:
            res = ydl.extract_info(query, download=False)
            for video in res['entries']:
                v_hash = hashlib.md5(video['title'].encode()).hexdigest()
                cursor.execute("SELECT 1 FROM published WHERE hash=?", (v_hash,))
                if not cursor.fetchone():
                    return {"title": video['title'], "url": f"https://www.youtube.com/watch?v={video['id']}", "hash": v_hash}
        except: return None
    return None

# =========================================================
# ğŸ¦ Ù†Ø´Ø± "Ø§Ù„Ø®Ø¨Ø§ÙŠØ§" (Thread Posting)
# =========================================================
async def post_scoop_thread():
    video_data = fetch_leak_video()
    
    # Ø§Ø®ØªÙŠØ§Ø± "Ø²Ø§ÙˆÙŠØ©" Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ù„Ø¶Ù…Ø§Ù† Ø§Ù„ØªÙ†ÙˆØ¹
    angle = random.choice(["ØªØ³Ø±ÙŠØ¨ Ø­ØµØ±ÙŠ", "Ø®Ø¨Ø§ÙŠØ§ Ù…Ø®ÙÙŠØ©", "Ù‚Ù†Ø¨Ù„Ø© ØªÙ‚Ù†ÙŠØ©", "Ù…ÙŠØ²Ø© Ø³Ø±ÙŠØ©"])
    
    prompt = f"Ø§ÙƒØªØ¨ Ø³Ù„Ø³Ù„Ø© Ù…Ù† 3 ØªØºØ±ÙŠØ¯Ø§Øª Ø¯Ø³Ù…Ø© Ø¨Ø£Ø³Ù„ÙˆØ¨ '{angle}' Ø¹Ù†: {video_data['title'] if video_data else 'Ø£Ø­Ø¯Ø« Ø£Ø¯Ø§Ø© AI Ù„Ù„Ø£ÙØ±Ø§Ø¯'}. Ø±ÙƒØ² Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù„ÙŠ Ù…Ø§ ÙŠØ¹Ø±ÙÙ‡Ø§ Ø§Ù„Ø¬Ù…ÙŠØ¹."
    system = "Ø£Ù†Øª Ù†Ø§ØµØ±ØŒ Ø§Ù„Ù…ØµØ¯Ø± Ø§Ù„Ø£ÙˆÙ„ Ù„Ù„Ø®Ø¨Ø§ÙŠØ§ Ø§Ù„ØªÙ‚Ù†ÙŠØ©. Ø£Ø³Ù„ÙˆØ¨Ùƒ Ù…Ø«ÙŠØ± ÙˆÙ…Ù‡Ù†ÙŠØŒ ØªØ¸Ù‡Ø± Ø¨Ù…Ø¸Ù‡Ø± Ø§Ù„Ù…Ø·Ù„Ø¹ Ø¹Ù„Ù‰ Ù…Ø§ ÙˆØ±Ø§Ø¡ Ø§Ù„ÙƒÙˆØ§Ù„ÙŠØ³. Ø§Ø³ØªØ®Ø¯Ù… Ù„Ù‡Ø¬Ø© Ø®Ù„ÙŠØ¬ÙŠØ© Ù…Ø±Ù…ÙˆÙ‚Ø©."

    raw_content = await brain.generate(prompt, system)
    if not raw_content: return

    tweets = [t.strip() for t in raw_content.split('\n\n') if len(t) > 10]
    
    try:
        # Ø¥Ø°Ø§ ÙˆØ¬Ø¯Ù†Ø§ ÙÙŠØ¯ÙŠÙˆØŒ Ù†Ø±ÙØ¹Ù‡ Ù…Ø¹ Ø£ÙˆÙ„ ØªØºØ±ÙŠØ¯Ø©
        media_ids = []
        if video_data:
            logger.info(f"ğŸ¬ Ù…Ø¹Ø§Ù„Ø¬Ø© ÙÙŠØ¯ÙŠÙˆ Ø§Ù„Ø®Ø¨Ø§ÙŠØ§: {video_data['title']}")
            # (Ù‡Ù†Ø§ ØªØªÙ… Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ­Ù…ÙŠÙ„ ÙˆØ§Ù„Ù‚Øµ Ø¨Ù€ ffmpeg ÙƒÙ…Ø§ ÙÙŠ ÙƒÙˆØ¯Ùƒ Ø§Ù„Ø³Ø§Ø¨Ù‚)
            # ØªÙ… Ø§Ø®ØªØµØ§Ø±Ù‡Ø§ Ù‡Ù†Ø§ Ù„Ù„ØªØ±ÙƒÙŠØ² Ø¹Ù„Ù‰ Ù…Ù†Ø·Ù‚ Ø§Ù„Ù†Ø´Ø±
            
        first_tweet = client_v2.create_tweet(text=tweets[0])
        last_id = first_tweet.data['id']
        
        for i in range(1, len(tweets)):
            await asyncio.sleep(random.randint(15, 30)) # Ø£Ù†Ø³Ù†Ø© Ø§Ù„ØªÙˆÙ‚ÙŠØª
            reply = client_v2.create_tweet(text=tweets[i], in_reply_to_tweet_id=last_id)
            last_id = reply.data['id']
            
        if video_data:
            cursor.execute("INSERT INTO published VALUES (?,?,?)", (video_data['hash'], angle, datetime.now().isoformat()))
            conn.commit()
        logger.success(f"âœ… ØªÙ… Ù†Ø´Ø± {angle} Ø¨Ù†Ø¬Ø§Ø­!")
    except Exception as e:
        logger.error(f"âŒ ÙØ´Ù„ Ù†Ø´Ø± Ø§Ù„Ø®Ø¨Ø§ÙŠØ§: {e}")

# =========================================================
# ğŸ’¬ Ø§Ù„Ø±Ø¯ÙˆØ¯ Ø§Ù„Ø°ÙƒÙŠØ© (Reply Later)
# =========================================================
async def smart_reply_cycle():
    me = client_v2.get_me()
    my_id = str(me.data.id)
    
    mentions = client_v2.get_users_mentions(id=my_id, max_results=5, expansions=['author_id'])
    if not mentions.data: return

    for tweet in mentions.data:
        author_id = str(tweet.author_id)
        if author_id == my_id: continue # Ù…Ù†Ø¹ Ø§Ù„Ø±Ø¯ Ø¹Ù„Ù‰ Ø§Ù„Ù†ÙØ³
        
        cursor.execute("SELECT 1 FROM interactions WHERE tweet_id=? AND user_id=?", (tweet.id, author_id))
        if cursor.fetchone(): continue # Ù…Ù†Ø¹ Ø§Ù„Ø±Ø¯ Ø§Ù„Ù…ÙƒØ±Ø± Ù„Ù†ÙØ³ Ø§Ù„Ø´Ø®Øµ

        prompt = f"Ø±Ø¯ Ø¨Ø§Ø®ØªØµØ§Ø± ÙˆØ°ÙƒØ§Ø¡ Ø¨Ù„Ù‡Ø¬Ø© Ø®Ù„ÙŠØ¬ÙŠØ© Ø¹Ù„Ù‰ Ù‡Ø°Ø§ Ø§Ù„Ù…ØªØ§Ø¨Ø¹ Ø¨Ø®ØµÙˆØµ Ø®ÙØ§ÙŠØ§ Ø§Ù„ØªÙ‚Ù†ÙŠØ©: {tweet.text}"
        reply = await brain.generate(prompt, "Ø£Ù†Øª Ù†Ø§ØµØ±ØŒ ØªØ±Ø¯ Ø¹Ù„Ù‰ Ø¬Ù…Ù‡ÙˆØ±Ùƒ Ø¨Ø°ÙƒØ§Ø¡ ÙˆØªÙˆØ§Ø¶Ø¹ Ø®Ø¨ÙŠØ±.")
        
        if reply:
            client_v2.create_tweet(text=reply, in_reply_to_tweet_id=tweet.id)
            cursor.execute("INSERT INTO interactions VALUES (?, ?)", (tweet.id, author_id))
            conn.commit()
            logger.info(f"âœ… ØªÙ… Ø§Ù„Ø±Ø¯ Ø°ÙƒÙŠØ§Ù‹ Ø¹Ù„Ù‰ {author_id}")

# =========================================================
# ğŸš€ Ø§Ù„Ø³ÙŠØ±ÙØ± Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ
# =========================================================
async def main():
    logger.info("ğŸŒŸ Ø¨Ø¯Ø¡ ØªØ´ØºÙŠÙ„ Ø¨ÙˆØª Ø§Ù„Ø®Ø¨Ø§ÙŠØ§ ÙˆØ§Ù„ØªØ³Ø±ÙŠØ¨Ø§Øª Ø§Ù„ØªÙ‚Ù†ÙŠØ©...")
    while True:
        # 1. Ø¯ÙˆØ±Ø© Ø§Ù„Ù†Ø´Ø± (Ø®Ø¨Ø§ÙŠØ§ Ø¬Ø¯ÙŠØ¯Ø©)
        await post_scoop_thread()
        
        # 2. Ø§Ù†ØªØ¸Ø§Ø± ØªÙØ§Ø¹Ù„ Ø§Ù„Ù†Ø§Ø³ Ø«Ù… Ø§Ù„Ø±Ø¯
        await asyncio.sleep(600)
        await smart_reply_cycle()
        
        # 3. Ù‚ÙŠÙ„ÙˆÙ„Ø© ØªÙ‚Ù†ÙŠØ© Ø·ÙˆÙŠÙ„Ø© (Ù„Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ø¬ÙˆØ¯Ø© Ø§Ù„Ø­Ø³Ø§Ø¨ Ùˆ Content Freshness)
        wait_time = random.randint(7200, 14400) # 2-4 Ø³Ø§Ø¹Ø§Øª
        logger.info(f"ğŸ’¤ Ù‚ÙŠÙ„ÙˆÙ„Ø© Ù„Ù…Ø¯Ø© {wait_time/3600:.1f} Ø³Ø§Ø¹Ø©...")
        await asyncio.sleep(wait_time)

if __name__ == "__main__":
    asyncio.run(main())
