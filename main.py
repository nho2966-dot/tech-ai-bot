import os
import sqlite3
import hashlib
import logging
import time
import random
import re
from datetime import datetime, date, timedelta
from collections import deque
from typing import Optional, List, Dict, Any

import tweepy
from openai import OpenAI
from google import genai
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type
import feedparser
from dateutil import parser as date_parser

logging.basicConfig(level=logging.INFO, format="ğŸ›¡ï¸ [Ù†Ø¸Ø§Ù… Ø§Ù„Ø³ÙŠØ§Ø¯Ø©]: %(message)s")


SYSTEM_PROMPT = r"""
Ø£Ù†Øª Ø®Ø¨ÙŠØ± ØªÙ‚Ù†ÙŠ Ø®Ù„ÙŠØ¬ÙŠ Ù…ØªØ®ØµØµ ÙÙŠ "Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ÙˆØ£Ø­Ø¯Ø« Ø£Ø¯ÙˆØ§ØªÙ‡ Ù„Ù„Ø£ÙØ±Ø§Ø¯". 
Ø£Ø³Ù„ÙˆØ¨Ùƒ ÙˆØ§Ø¶Ø­ØŒ Ù…Ù†Ø¸Ù…ØŒ Ø§Ø­ØªØ±Ø§ÙÙŠØŒ Ù…Ø¨Ø§Ø´Ø±ØŒ ÙˆÙ…ÙÙŠØ¯.

**Ù‚ÙˆØ§Ø¹Ø¯ ØµØ§Ø±Ù…Ø© Ù„Ø§ ØªÙÙ†Ù‚Ø¶:**
- Ø±ÙƒØ² Ø¹Ù„Ù‰ Ø§Ù„ÙˆÙƒÙ„Ø§Ø¡ Ø§Ù„Ø£Ø°ÙƒÙŠØ§Ø¡ (AI Agents) ÙˆØ§Ù„Ø£Ø¯ÙˆØ§Øª Ø§Ù„Ø¹Ù…Ù„ÙŠØ© Ù„Ø¹Ø§Ù… 2026.
- Ù„Ø§ Ù‡Ù„ÙˆØ³Ø©ØŒ Ù„Ø§ ÙƒØ°Ø¨ØŒ Ù„Ø§ Ø§ÙØªØ±Ø§Ø¶Ø§Øª. Ø¥Ø°Ø§ Ù„Ù… ØªØ¬Ø¯ Ø£Ø¯Ø§Ø© Ø­Ù‚ÙŠÙ‚ÙŠØ© Ù‚Ù„ "Ù„Ø§_Ù…Ø¹Ù„ÙˆÙ…Ø§Øª_Ù…ÙˆØ«ÙˆÙ‚Ø©".
- Ù…Ù…Ù†ÙˆØ¹ Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙƒÙ„Ù…Ø© "Ù‚Ø³Ù…" Ø£Ùˆ Ø£ÙŠ Ù„ÙØ¸ Ø¬Ù„Ø§Ù„Ø©.
- Ø§Ù„Ù†Øµ Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© (Ù„Ù‡Ø¬Ø© Ø®Ù„ÙŠØ¬ÙŠØ© Ø¨ÙŠØ¶Ø§Ø¡) ÙˆÙ„Ø§ ØªØ³ØªØ®Ø¯Ù… Ø£ÙŠ Ø±Ù…ÙˆØ² ØºØ±ÙŠØ¨Ø© Ø£Ùˆ Ù„ØºØ© ØµÙŠÙ†ÙŠØ©.
- Ø§Ù„Ù‡ÙŠÙƒÙ„: ÙØ§Ø¦Ø¯Ø© ØªÙ‚Ù†ÙŠØ© ÙˆØ§Ø¶Ø­Ø© â†’ Ø´Ø±Ø­/Ø£Ø¯Ø§Ø©/Ø®Ø·ÙˆØ§Øª â†’ Ø¯Ø¹ÙˆØ© ØªÙØ§Ø¹Ù„ Ù…Ù†Ø·Ù‚ÙŠØ©.

Ø§Ø®ØªØ± Ø´ÙƒÙ„Ù‹Ø§ Ù…ØªÙ†ÙˆØ¹Ù‹Ø§ ÙÙŠ ÙƒÙ„ Ù…Ø±Ø©:
- ØªØºØ±ÙŠØ¯Ø© ÙˆØ§Ø­Ø¯Ø© Ù…Ù†Ø¸Ù…Ø©
- thread Ù‚ØµÙŠØ± (ÙØµÙ„ Ø¨Ù€ "---")
- Ù…Ù‚Ø§Ø±Ù†Ø© ÙˆØ§Ø¶Ø­Ø©
- Ù‚Ø§Ø¦Ù…Ø© Ù…Ø®ØªØµØ±Ø©
- Ù†ØµÙŠØ­Ø© Ø®Ø·ÙˆØ© Ø¨Ø®Ø·ÙˆØ©

Ø§Ø¬Ø¹Ù„ Ø§Ù„Ù†Øµ Ù‚ØµÙŠØ±Ù‹Ø§ØŒ ÙˆØ§Ø¶Ø­Ù‹Ø§ØŒ ÙŠØ±ÙƒØ² Ø¹Ù„Ù‰ Ø§Ù„ÙØ§Ø¦Ø¯Ø© Ø§Ù„Ø¹Ù…Ù„ÙŠØ© Ø¨Ø¯ÙˆÙ† Ù…Ø¨Ø§Ù„ØºØ©.
ÙÙŠ Ø§Ù„Ù†Ù‡Ø§ÙŠØ© Ø£Ø¶Ù Ø³Ø·Ø±Ù‹Ø§ ÙˆØ§Ø­Ø¯Ù‹Ø§ ÙÙ‚Ø· ÙŠØ¨Ø¯Ø£ Ø¨Ù€ "ÙˆØµÙ_ØµÙˆØ±Ø©:" Ø«Ù… ÙˆØµÙ Ù…Ø®ØªØµØ± Ù…Ù†Ø§Ø³Ø¨.

Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ù„Ø§ ÙŠØ­Ù‚Ù‚ Ù‚ÙŠÙ…Ø© Ø¹Ù…Ù„ÙŠØ© Ø£Ùˆ Ù…Ø´Ø§Ø¨Ù‡ Ù„Ù„Ø³Ø§Ø¨Ù‚ â†’ Ø±Ø¯ ÙÙ‚Ø· Ø¨Ù€ "Ù„Ø§_Ù‚ÙŠÙ…Ø©".
"""


class SovereignUltimateBot:
    def __init__(self):
        self.db_path = "data/sovereign_final.db"
        self._init_db()
        self._setup_clients()
        self.reply_timestamps = deque(maxlen=50)
        self.replied_tweets_cache = set()
        self.last_mention_id = None
        self.recent_posts = deque(maxlen=10)  # Ø¢Ø®Ø± 10 Ù„Ù…Ù†Ø¹ Ø§Ù„ØªÙƒØ±Ø§Ø± Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠ

        self.rss_feeds = [
            "https://www.theverge.com/rss/index.xml",
            "https://techcrunch.com/feed/",
            "https://www.wired.com/feed/category/science/latest/rss",
            "https://arstechnica.com/category/tech/feed/",
            "https://www.engadget.com/rss.xml",
            "https://www.cnet.com/rss/news/",
            "https://www.technologyreview.com/feed/",
            "https://gizmodo.com/rss",
            "https://venturebeat.com/feed/",
            "https://thenextweb.com/feed",
            "https://www.artificialintelligence-news.com/feed/",
            "https://huggingface.co/blog/feed.xml",
            "https://www.deepmind.com/blog/rss.xml",
            "https://openai.com/blog/rss/",
            "https://www.tech-wd.com/wd-rss-feed.xml",
            "https://www.aitnews.com/feed/",
            "https://www.arageek.com/feed/tech",
            "https://arabhardware.net/feed",
            "https://www.tqniah.net/feed/",
            "https://www.arabtechs.net/feed",
            "https://www.taqniah.com/feed/",
            "https://www.youm7.com/rss/Technologia",
            "https://www.almasryalyoum.com/rss",
            "https://www.masrawy.com/rss/tech",
            "https://www.elbalad.news/rss/tech",
            "https://www.elwatannews.com/rss/section/6",
            "https://www.dostor.org/rss/technology",
            "https://www.vetogate.com/rss/technology",
            "https://www.cairo24.com/rss/technology",
            "https://sabq.org/feed",
            "https://www.aleqt.com/feed",
            "https://aawsat.com/rss/technologia",
            "https://www.okaz.com.sa/rss",
            "https://www.alriyadh.com/page/rss",
            "https://www.alyaum.com/rss",
            "https://www.albayan.ae/tech/rss",
            "https://www.emaratalyoum.com/rss/tech",
            "https://wam.ae/feed/technology",
            "https://qna.org.qa/ar-QA/RSS-Feeds/Technology",
            "https://www.alanba.com.kw/rss/tech",
            "https://kuwaitalyawm.media.gov.kw/rss",
            "https://www.bna.bh/rss",
            "https://omannews.gov.om/rss/technology",
        ]

    def _init_db(self):
        os.makedirs("data", exist_ok=True)
        with sqlite3.connect(self.db_path) as conn:
            conn.execute("CREATE TABLE IF NOT EXISTS history (hash TEXT PRIMARY KEY, ts DATETIME)")
            conn.execute("CREATE TABLE IF NOT EXISTS daily_stats (day TEXT PRIMARY KEY, count INTEGER)")
            conn.execute("CREATE TABLE IF NOT EXISTS replied_tweets (tweet_id TEXT PRIMARY KEY, ts DATETIME)")

    def _setup_all_brains(self):
        try:
            self.gemini_client = genai.Client(api_key=os.getenv("GEMINI_KEY"))
        except Exception as e:
            logging.error(f"ÙØ´Ù„ ØªÙ‡ÙŠØ¦Ø© Gemini: {e}")
            self.gemini_client = None

        self.x_client = tweepy.Client(
            bearer_token=os.getenv("X_BEARER_TOKEN"),
            consumer_key=os.getenv("X_API_KEY"),
            consumer_secret=os.getenv("X_API_SECRET"),
            access_token=os.getenv("X_ACCESS_TOKEN"),
            access_token_secret=os.getenv("X_ACCESS_SECRET")
        )

        try:
            me = self.x_client.get_me(user_auth=True)
            self.my_user_id = me.data.id
            logging.info(f"Bot user ID: {self.my_user_id}")
        except Exception as e:
            logging.error(f"ÙØ´Ù„ Ø¬Ù„Ø¨ user ID: {e}")
            self.my_user_id = None

        self.brains = {
            "Groq": OpenAI(api_key=os.getenv("GROQ_API_KEY"), base_url="https://api.groq.com/openai/v1"),
            "Gemini": self.gemini_client,
            "OpenAI": OpenAI(api_key=os.getenv("OPENAI_API_KEY")),
            "OpenRouter": OpenAI(api_key=os.getenv("OPENROUTER_API_KEY"), base_url="https://openrouter.ai/api/v1"),
        }

    @retry(
        stop=stop_after_attempt(5),
        wait=wait_exponential(multiplier=1.5, min=5, max=45),
        retry=retry_if_exception_type(Exception),
        reraise=True
    )
    def generate_text(self, prompt: str, system_msg: str) -> str:
        sequence = [
            ("Groq Llama 3.3", "Groq", "llama-3.3-70b-versatile"),
            ("Gemini Flash", "Gemini", "gemini-2.5-flash"),
            ("OpenAI 4o-mini", "OpenAI", "gpt-4o-mini"),
            ("OpenRouter Gemini", "OpenRouter", "google/gemini-2.5-flash"),
        ]

        for name, key, model in sequence:
            try:
                client = self.brains.get(key)
                if not client:
                    continue

                if key == "Gemini":
                    m = client.GenerativeModel(model)
                    res = m.generate_content(f"{system_msg}\n{prompt}")
                    text = res.text.strip()
                else:
                    res = client.chat.completions.create(
                        model=model,
                        messages=[
                            {"role": "system", "content": system_msg},
                            {"role": "user", "content": prompt}
                        ],
                        temperature=0.75,
                        max_tokens=420,
                        timeout=40
                    )
                    text = res.choices[0].message.content.strip()

                if text and len(text) > 80:
                    return text

            except Exception as e:
                logging.warning(f"{name} ÙØ´Ù„: {str(e)[:100]}")
                continue

        raise RuntimeError("ÙØ´Ù„ ÙƒÙ„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬")

    def clean_forbidden_words(self, text: str) -> str:
        forbidden_patterns = [
            r"Ù‚Ø³Ù…|Ø£Ù‚Ø³Ù…|Ø§Ù‚Ø³Ù…|Ù‚Ø³Ù‘Ù…|ØªÙ‚Ø³ÙŠÙ…|Ù‚Ø³Ù…Ù‡Ø§|Ù‚Ø³Ù…ÙˆØ§|Ù‚Ø³Ù… Ø¨Ø§Ù„Ù„Ù‡",
            r"Ø§Ù„Ù„Ù‡|ÙˆØ§Ù„Ù„Ù‡|Ø¨Ø§Ù„Ù„Ù‡|Ø¥Ù† Ø´Ø§Ø¡ Ø§Ù„Ù„Ù‡|Ø§Ù„Ø­Ù…Ø¯ Ù„Ù„Ù‡|Ø³Ø¨Ø­Ø§Ù† Ø§Ù„Ù„Ù‡|Ø¨Ø³Ù… Ø§Ù„Ù„Ù‡|ÙŠØ§ Ø±Ø¨|ÙŠØ§ Ø§Ù„Ù„Ù‡",
            r"[\u4e00-\u9fff]+",  # ØµÙŠÙ†ÙŠ
            r"[^\u0600-\u06FF\s0-9a-zA-Z!@#$%^&*()_+\-=\[\]{};':\"\\|,.<>/?`~]",  # Ø±Ù…ÙˆØ² ØºÙŠØ± Ø¹Ø±Ø¨ÙŠ/Ù„Ø§ØªÙŠÙ†ÙŠ/Ø£Ø±Ù‚Ø§Ù…/ØªØ±Ù‚ÙŠÙ…
        ]

        cleaned = text
        for pattern in forbidden_patterns:
            cleaned = re.sub(pattern, "", cleaned, flags=re.IGNORECASE | re.UNICODE)

        cleaned = ' '.join(cleaned.split())
        return cleaned.strip()

    def is_semantic_duplicate(self, new_text: str) -> bool:
        new_lower = new_text.lower().strip()
        new_words = set(re.findall(r'\w+', new_lower))

        for old_text in self.recent_posts:
            old_lower = old_text.lower().strip()
            old_words = set(re.findall(r'\w+', old_lower))

            common = len(new_words & old_words)
            similarity = common / max(len(new_words), len(old_words)) if new_words and old_words else 0

            if similarity > 0.60:
                logging.info(f"Ø§Ù„ØªÙƒØ±Ø§Ø± Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠ Ù…Ø±ØªÙØ¹ ({similarity:.2f}) â†’ Ø±ÙØ¶")
                return True

        return False

    def already_posted(self, content: str) -> bool:
        h = hashlib.sha256(content.encode('utf-8')).hexdigest()
        with sqlite3.connect(self.db_path) as conn:
            return bool(conn.execute("SELECT 1 FROM history WHERE hash = ?", (h,)).fetchone())

    def mark_posted(self, content: str):
        h = hashlib.sha256(content.encode('utf-8')).hexdigest()
        with sqlite3.connect(self.db_path) as conn:
            conn.execute("INSERT OR IGNORE INTO history (hash, ts) VALUES (?, datetime('now'))", (h,))
        self.recent_posts.append(content)

    def fetch_current_trends(self):
        """
        Ø¬Ù„Ø¨ Ø£Ø­Ø¯Ø« Ø§Ù„ØªØ±Ù†Ø¯Ø§Øª ÙÙŠ Ø§Ù„Ù…Ù†Ø·Ù‚Ø© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©/Ø§Ù„Ø®Ù„ÙŠØ¬ÙŠØ©
        """
        try:
            trends = self.x_client.get_place_trends(woeid=23424938)  # Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ© - ÙŠÙ…ÙƒÙ† ØªØºÙŠÙŠØ± Ø§Ù„Ù€ WOEID
            top_trends = [trend['name'] for trend in trends[0]['trends'][:5] if trend['tweet_volume'] is not None]
            logging.info(f"Ø£Ø­Ø¯Ø« Ø§Ù„ØªØ±Ù†Ø¯Ø§Øª: {top_trends}")
            return top_trends
        except Exception as e:
            logging.error(f"ÙØ´Ù„ Ø¬Ù„Ø¨ Ø§Ù„ØªØ±Ù†Ø¯Ø§Øª: {e}")
            return []

    def is_trend_relevant(self, trend: str) -> bool:
        ai_keywords = ["AI", "Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ", "ChatGPT", "Grok", "Gemini", "Claude", "ØªÙ‚Ù†ÙŠØ©", "ØªÙƒÙ†ÙˆÙ„ÙˆØ¬ÙŠØ§", "Ø£Ø¯Ø§Ø©", "Ø±Ù…Ø¶Ø§Ù†", "ØµÙŠØ§Ù…", "Ù‡Ø§ØªÙ", "Ø³Ø§Ø¹Ø©", "Ø¬Ù‡Ø§Ø²"]
        return any(kw.lower() in trend.lower() for kw in ai_keywords)

    def generate_trend_content(self, trend: str):
        task = f"Ø§Ù„ØªØ±Ù†Ø¯ Ø§Ù„Ø­Ø§Ù„ÙŠ: {trend}. Ø£Ù†Ø´Ø¦ Ù…Ø­ØªÙˆÙ‰ ÙŠØ±Ø¨Ø· Ù‡Ø°Ø§ Ø§Ù„ØªØ±Ù†Ø¯ Ø¨Ø£Ø¯Ø§Ø© Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø£Ùˆ Ù†ØµÙŠØ­Ø© ØªÙ‚Ù†ÙŠØ© Ù…ÙÙŠØ¯Ø© Ø¹Ù…Ù„ÙŠÙ‹Ø§ Ù„Ù„Ø£ÙØ±Ø§Ø¯ ÙÙŠ Ø§Ù„Ø­ÙŠØ§Ø© Ø§Ù„ÙŠÙˆÙ…ÙŠØ© Ø£Ùˆ Ø±Ù…Ø¶Ø§Ù†. Ø±ÙƒØ² Ø¹Ù„Ù‰ Ø§Ù„Ù‚ÙŠÙ…Ø© Ø§Ù„Ù…Ø¨Ø§Ø´Ø±Ø© (ØªÙˆÙÙŠØ± ÙˆÙ‚Øª/Ø¬Ù‡Ø¯/Ù…Ø§Ù„). Ø§Ø³ØªØ®Ø¯Ù… Ø£Ø³Ù„ÙˆØ¨Ù‹Ø§ Ù…Ù†Ø¶Ù…Ù‹Ø§ ÙˆØ§Ø­ØªØ±Ø§ÙÙŠÙ‹Ø§."
        return self.generate_text(task, SYSTEM_PROMPT)

    def fetch_hidden_gems(self):
        """
        Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† Ù‡Ù†Ø§Ùƒ ØªØ±Ù†Ø¯ Ø£Ùˆ Ø®Ø¨Ø± Ø¬Ø¯ÙŠØ¯ â†’ Ø§Ø¨Ø­Ø« Ø¹Ù† Ø®ÙØ§ÙŠØ§ ÙˆÙ…Ù…ÙŠØ²Ø§Øª Ø§Ù„Ø£Ø¬Ù‡Ø²Ø© Ø§Ù„Ø°ÙƒÙŠØ© ÙˆØ§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ
        """
        hidden_prompt = "Ø§Ø¨Ø­Ø« Ø¹Ù† Ø®ÙØ§ÙŠØ§ ÙˆÙ…Ù…ÙŠØ²Ø§Øª Ù…Ø®ÙÙŠØ© ÙÙŠ Ø§Ù„Ø£Ø¬Ù‡Ø²Ø© Ø§Ù„Ø°ÙƒÙŠØ© Ø£Ùˆ Ø£Ø¯ÙˆØ§Øª Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø§Ù„ØªÙŠ ÙŠØ¬Ù‡Ù„Ù‡Ø§ Ù…Ø¹Ø¸Ù… Ø§Ù„Ù†Ø§Ø³ØŒ ÙˆØ±ÙƒØ² Ø¹Ù„Ù‰ Ù…Ø§ ÙŠÙ‚Ø¯Ù… Ù‚ÙŠÙ…Ø© Ø¹Ù…Ù„ÙŠØ© ÙÙˆØ±ÙŠØ© (ØªÙˆÙÙŠØ± ÙˆÙ‚Øª/Ù…Ø§Ù„/Ø¬Ù‡Ø¯). Ø£Ø¹Ø·Ù Ø£Ù…Ø«Ù„Ø© Ø­Ù‚ÙŠÙ‚ÙŠØ© ÙˆÙ…ÙˆØ«Ù‚Ø©."
        return self.generate_text(hidden_prompt, SYSTEM_PROMPT)

    def run(self):
        try:
            # 1. Ø¬Ù„Ø¨ Ø§Ù„ØªØ±Ù†Ø¯Ø§Øª Ø£ÙˆÙ„Ø§Ù‹
            trends = self.fetch_current_trends()
            selected_trend = None
            for trend in trends:
                if self.is_trend_relevant(trend):
                    selected_trend = trend
                    break

            context = ""
            if selected_trend:
                context += f"\n\nØ§Ø³ØªØºÙ„ Ø§Ù„ØªØ±Ù†Ø¯ Ø§Ù„Ø­Ø§Ù„ÙŠ: {selected_trend}\nØ£Ù†Ø´Ø¦ Ù…Ø­ØªÙˆÙ‰ ÙŠØ±Ø¨Ø·Ù‡ Ø¨Ø£Ø¯Ø§Ø© AI Ù…ÙÙŠØ¯Ø© Ø£Ùˆ Ù†ØµÙŠØ­Ø© Ø¹Ù…Ù„ÙŠØ©."

            # 2. Ø¬Ù„Ø¨ Ø£Ø®Ø¨Ø§Ø± RSS
            fresh_news = self.fetch_fresh_rss(max_per_feed=4, max_age_hours=36)
            if fresh_news:
                top = fresh_news[0]
                context += f"\nØ®Ø¨Ø± Ø­Ø¯ÙŠØ«: {top['title']} Ù…Ù† {top['source']} â€“ {top['summary'][:100]}... {top['link']}"

            # 3. Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† Ù‡Ù†Ø§Ùƒ ØªØ±Ù†Ø¯ Ø£Ùˆ Ø®Ø¨Ø± Ø¬Ø¯ÙŠØ¯ â†’ Ø§Ø¨Ø­Ø« Ø¹Ù† Ø®ÙØ§ÙŠØ§ ÙˆÙ…Ù…ÙŠØ²Ø§Øª
            if not selected_trend and not fresh_news:
                raw_output = self.fetch_hidden_gems()
            else:
                task = f"Ø£Ø¹Ø·Ù†ÙŠ Ù…Ø­ØªÙˆÙ‰ ØªÙ‚Ù†ÙŠ Ø¬Ø¯ÙŠØ¯ ÙˆÙ…ÙÙŠØ¯ Ù„Ù„Ø£ÙØ±Ø§Ø¯ Ø§Ù„ÙŠÙˆÙ….{context}"
                raw_output = self.generate_text(task, SYSTEM_PROMPT)

            cleaned_output = self.clean_forbidden_words(raw_output)

            if "Ù„Ø§_Ù‚ÙŠÙ…Ø©" in cleaned_output.strip() or "Ù„Ø§_Ù…Ø¹Ù„ÙˆÙ…Ø§Øª_Ù…ÙˆØ«ÙˆÙ‚Ø©" in cleaned_output.strip():
                logging.info("Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ù„Ø§ ÙŠØ¶ÙŠÙ Ù‚ÙŠÙ…Ø© Ø£Ùˆ ØºÙŠØ± Ù…ÙˆØ«ÙˆÙ‚ â†’ ØªØ®Ø·ÙŠ")
                return

            if self.already_posted(cleaned_output):
                logging.info("Ù…Ø­ØªÙˆÙ‰ Ù…ÙƒØ±Ø± Ø­Ø±ÙÙŠÙ‹Ø§ â†’ ØªØ®Ø·ÙŠ")
                return

            if self.is_semantic_duplicate(cleaned_output):
                logging.info("Ù…Ø­ØªÙˆÙ‰ Ù…Ø´Ø§Ø¨Ù‡ Ø¯Ù„Ø§Ù„ÙŠÙ‹Ø§ â†’ ØªØ®Ø·ÙŠ")
                return

            self.recent_posts.append(cleaned_output)

            image_desc = ""
            content = cleaned_output
            if "ÙˆØµÙ_ØµÙˆØ±Ø©:" in cleaned_output:
                parts = cleaned_output.rsplit("ÙˆØµÙ_ØµÙˆØ±Ø©:", 1)
                content = parts[0].strip()
                image_desc = parts[1].strip()

            tweets = [t.strip() for t in content.split("---") if t.strip()]

            prev_id = None
            for i, txt in enumerate(tweets):
                try:
                    kwargs = {"text": txt}
                    if i == 0 and image_desc:
                        logging.info(f"ØµÙˆØ±Ø© Ù…Ù‚ØªØ±Ø­Ø©: {image_desc}")
                    if prev_id:
                        kwargs["in_reply_to_tweet_id"] = prev_id
                    resp = self.x_client.create_tweet(**kwargs)
                    prev_id = resp.data["id"]
                    logging.info(f"Ù†Ø´Ø± ØªØºØ±ÙŠØ¯Ø© {i+1}/{len(tweets)} Ø¨Ù†Ø¬Ø§Ø­")
                    time.sleep(5 + random.random() * 10)
                except Exception as e:
                    logging.error(f"Ø®Ø·Ø£ ÙÙŠ Ù†Ø´Ø± ØªØºØ±ÙŠØ¯Ø© {i+1}: {e}")
                    continue

            self.handle_mentions()
            self.mark_posted(content)

        except Exception as e:
            logging.error(f"Ø®Ø·Ø£ Ø¹Ø§Ù… ÙÙŠ run(): {e}")


if __name__ == "__main__":
    bot = SovereignUltimateBot()
    bot.run()
