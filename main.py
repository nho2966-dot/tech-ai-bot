import os
import sqlite3
import hashlib
import logging
import time
import random
import re
from datetime import datetime, date, timedelta
from collections import deque
from typing import Optional, List, Dict, Any
from functools import lru_cache
import difflib  # Ù„Ù€ Levenshtein-like similarity

import tweepy
from openai import OpenAI
from google import genai
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type
import feedparser
from dateutil import parser as date_parser

logging.basicConfig(level=logging.INFO, format="ğŸ›¡ï¸ [Ù†Ø¸Ø§Ù… Ø§Ù„Ø³ÙŠØ§Ø¯Ø©]: %(message)s")


SYSTEM_PROMPT = r"""
Ø£Ù†Øª Ù…ØªØ®ØµØµ ØªÙ‚Ù†ÙŠ Ø¹Ø±Ø¨ÙŠ Ø¯Ù‚ÙŠÙ‚ ÙˆÙ…ÙˆØ«ÙˆÙ‚ 100%. Ù…Ù‡Ù…ØªÙƒ ØªÙˆÙ„ÙŠØ¯ Ù…Ø­ØªÙˆÙ‰ ØªÙ‚Ù†ÙŠ ÙÙ‚Ø· Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø­Ù‚ÙŠÙ‚ÙŠØ© ÙˆÙ…ÙˆØ«Ù‚Ø©ØŒ Ø¨Ø¯ÙˆÙ† Ø£ÙŠ Ø§ÙØªØ±Ø§Ø¶Ø§Øª Ø£Ùˆ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ØºÙŠØ± Ù…Ø¤ÙƒØ¯Ø©.

**Ù‚ÙˆØ§Ø¹Ø¯ Ø¥Ù„Ø²Ø§Ù…ÙŠØ© Ù„Ø§ ØªÙÙ†Ù‚Ø¶ Ø£Ø¨Ø¯Ù‹Ø§:**
- Ù„Ø§ ØªØ®ØªÙ„Ù‚ Ø£ÙŠ Ù…Ø¹Ù„ÙˆÙ…Ø©ØŒ Ø±Ù‚Ù…ØŒ Ø§Ø³Ù… Ø£Ø¯Ø§Ø©ØŒ ØªØ§Ø±ÙŠØ®ØŒ Ø£Ùˆ Ù…ÙŠØ²Ø© ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø© ÙØ¹Ù„ÙŠÙ‹Ø§ ÙÙŠ Ø§Ù„ÙˆØ§Ù‚Ø¹ Ø­ØªÙ‰ Ù„Ùˆ Ø¨Ø¯Ø§ Ù…Ù†Ø·Ù‚ÙŠÙ‹Ø§.
- Ø¥Ø°Ø§ Ù„Ù… ØªÙƒÙ† Ù…ØªØ£ÙƒØ¯Ù‹Ø§ 100% Ù…Ù† Ù…Ø¹Ù„ÙˆÙ…Ø© â†’ Ù„Ø§ ØªØ°ÙƒØ±Ù‡Ø§ØŒ ÙˆØ£Ø¹Ø¯ ÙÙ‚Ø· "Ù„Ø§_Ù…Ø¹Ù„ÙˆÙ…Ø§Øª_Ù…ÙˆØ«ÙˆÙ‚Ø©".
- Ø±ÙƒØ² ÙÙ‚Ø· Ø¹Ù„Ù‰ Ø£Ø¯ÙˆØ§Øª ÙˆÙ…Ù…ÙŠØ²Ø§Øª Ø­Ù‚ÙŠÙ‚ÙŠØ© Ù…ÙˆØ¬ÙˆØ¯Ø© Ø­Ø§Ù„ÙŠÙ‹Ø§ (2026)ØŒ Ù…Ø¹ Ø°ÙƒØ± Ù…ØµØ¯Ø±Ù‡Ø§ Ø¥Ù† Ø£Ù…ÙƒÙ† (Ù…Ø«Ù„ "Ø­Ø³Ø¨ ØªØ­Ø¯ÙŠØ« Android 16" Ø£Ùˆ "ÙÙŠ Gemini 2.5").
- Ù…Ù…Ù†ÙˆØ¹ Ø§Ù„ØªØ®Ù…ÙŠÙ† Ø£Ùˆ "Ø±Ø¨Ù…Ø§" Ø£Ùˆ "ÙŠÙØ¹ØªÙ‚Ø¯" Ø£Ùˆ "Ù…Ù† Ø§Ù„Ù…Ø­ØªÙ…Ù„" ÙÙŠ Ø£ÙŠ Ø³ÙŠØ§Ù‚ ØªÙ‚Ù†ÙŠ.
- Ø§Ù„Ù†Øµ ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙƒÙˆÙ† Ø¹Ø±Ø¨ÙŠÙ‹Ø§ ÙÙ‚Ø·ØŒ Ø¨Ø¯ÙˆÙ† Ø±Ù…ÙˆØ² ØºØ±ÙŠØ¨Ø© Ø£Ùˆ Ø­Ø±ÙˆÙ Ø£Ø¬Ù†Ø¨ÙŠØ© ØºÙŠØ± Ù…ÙÙ‡ÙˆÙ…Ø©.
- Ù…Ù…Ù†ÙˆØ¹ ÙƒÙ„Ù…Ø© "Ù‚Ø³Ù…" Ø£Ùˆ Ø£ÙŠ ØµÙŠØºØ© Ù…Ù†Ù‡Ø§ØŒ ÙˆÙ…Ù…Ù†ÙˆØ¹ Ø£ÙŠ Ù„ÙØ¸ Ø¬Ù„Ø§Ù„Ø© Ø£Ùˆ ÙƒÙ„Ù…Ø© Ø¯ÙŠÙ†ÙŠØ©.

Ø¨Ù†ÙŠØ© Ø§Ù„Ø±Ø¯ ÙŠØ¬Ø¨ Ø£Ù† ØªÙƒÙˆÙ† Ù…Ù†Ø¶Ù…Ø©:
- Ø§Ù„Ø¨Ø¯Ø§ÙŠØ©: Ø­Ù‚ÙŠÙ‚Ø© Ø£Ùˆ ÙØ§Ø¦Ø¯Ø© Ù…Ø«Ø¨ØªØ©.
- Ø§Ù„ÙˆØ³Ø·: Ø´Ø±Ø­ ÙˆØ§Ø¶Ø­ + Ø®Ø·ÙˆØ§Øª ØªØ·Ø¨ÙŠÙ‚ÙŠØ© (Ø¥Ù† ÙˆØ¬Ø¯Øª).
- Ø§Ù„Ù†Ù‡Ø§ÙŠØ©: Ø¯Ø¹ÙˆØ© ØªÙØ§Ø¹Ù„ Ù…Ù†Ø·Ù‚ÙŠØ© ("Ù…Ø§ Ø±Ø£ÙŠÙƒÙ…ØŸ"ØŒ "Ù‡Ù„ Ø¬Ø±Ø¨ØªÙ…ØŸ").

ÙÙŠ Ø§Ù„Ù†Ù‡Ø§ÙŠØ© Ø£Ø¶Ù "ÙˆØµÙ_ØµÙˆØ±Ø©:" + ÙˆØµÙ Ù…Ø®ØªØµØ± Ù…Ù†Ø§Ø³Ø¨ ÙÙ‚Ø· Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ù…Ø­ØªÙˆÙ‰ ÙŠØ­ØªØ§Ø¬ ØµÙˆØ±Ø©.

Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ù…Ø­ØªÙˆÙ‰ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø£ÙŠ Ø§Ø­ØªÙ…Ø§Ù„ Ù‡Ù„ÙˆØ³Ø© Ø£Ùˆ Ù…Ø¹Ù„ÙˆÙ…Ø© ØºÙŠØ± Ù…Ø¤ÙƒØ¯Ø© â†’ Ø±Ø¯ ÙÙ‚Ø· Ø¨Ù€ "Ù„Ø§_Ù…Ø¹Ù„ÙˆÙ…Ø§Øª_Ù…ÙˆØ«ÙˆÙ‚Ø©".
"""


class SovereignUltimateBot:
    def __init__(self):
        self.db_path = "data/sovereign_final.db"
        self._init_db()
        self._setup_clients()
        self.reply_timestamps = deque(maxlen=50)
        self.replied_tweets_cache = set()
        self.last_mention_id = None
        self.recent_posts = deque(maxlen=10)
        self.topic_blacklist = deque(maxlen=5)  # Ù…ÙˆØ§Ø¶ÙŠØ¹ Ù…ØªÙƒØ±Ø±Ø© Ù…Ø­Ø¸ÙˆØ±Ø© Ù…Ø¤Ù‚ØªÙ‹Ø§

        self.rss_feeds = [
            "https://www.theverge.com/rss/index.xml",
            "https://techcrunch.com/feed/",
            "https://www.wired.com/feed/category/science/latest/rss",
            "https://arstechnica.com/category/tech/feed/",
            "https://www.engadget.com/rss.xml",
            "https://www.cnet.com/rss/news/",
            "https://www.technologyreview.com/feed/",
            "https://gizmodo.com/rss",
            "https://venturebeat.com/feed/",
            "https://thenextweb.com/feed",
            "https://www.artificialintelligence-news.com/feed/",
            "https://huggingface.co/blog/feed.xml",
            "https://www.deepmind.com/blog/rss.xml",
            "https://openai.com/blog/rss/",
            "https://www.tech-wd.com/wd-rss-feed.xml",
            "https://www.aitnews.com/feed/",
            "https://www.arageek.com/feed/tech",
            "https://arabhardware.net/feed",
            "https://www.tqniah.net/feed/",
            "https://www.arabtechs.net/feed",
            "https://www.taqniah.com/feed/",
            "https://www.youm7.com/rss/Technologia",
            "https://www.almasryalyoum.com/rss",
            "https://www.masrawy.com/rss/tech",
            "https://www.elbalad.news/rss/tech",
            "https://www.elwatannews.com/rss/section/6",
            "https://www.dostor.org/rss/technology",
            "https://www.vetogate.com/rss/technology",
            "https://www.cairo24.com/rss/technology",
            "https://sabq.org/feed",
            "https://www.aleqt.com/feed",
            "https://aawsat.com/rss/technologia",
            "https://www.okaz.com.sa/rss",
            "https://www.alriyadh.com/page/rss",
            "https://www.alyaum.com/rss",
            "https://www.albayan.ae/tech/rss",
            "https://www.emaratalyoum.com/rss/tech",
            "https://wam.ae/feed/technology",
            "https://qna.org.qa/ar-QA/RSS-Feeds/Technology",
            "https://www.alanba.com.kw/rss/tech",
            "https://kuwaitalyawm.media.gov.kw/rss",
            "https://www.bna.bh/rss",
            "https://omannews.gov.om/rss/technology",
        ]

    def _init_db(self):
        os.makedirs("data", exist_ok=True)
        with sqlite3.connect(self.db_path) as conn:
            conn.execute("CREATE TABLE IF NOT EXISTS history (hash TEXT PRIMARY KEY, ts DATETIME)")
            conn.execute("CREATE TABLE IF NOT EXISTS daily_stats (day TEXT PRIMARY KEY, count INTEGER)")
            conn.execute("CREATE TABLE IF NOT EXISTS replied_tweets (tweet_id TEXT PRIMARY KEY, ts DATETIME)")

    def _setup_clients(self):
        try:
            self.gemini_client = genai.Client(api_key=os.getenv("GEMINI_KEY"))
        except Exception as e:
            logging.error(f"ÙØ´Ù„ ØªÙ‡ÙŠØ¦Ø© Gemini: {e}")
            self.gemini_client = None

        self.x_client = tweepy.Client(
            bearer_token=os.getenv("X_BEARER_TOKEN"),
            consumer_key=os.getenv("X_API_KEY"),
            consumer_secret=os.getenv("X_API_SECRET"),
            access_token=os.getenv("X_ACCESS_TOKEN"),
            access_token_secret=os.getenv("X_ACCESS_SECRET")
        )

        try:
            me = self.x_client.get_me(user_auth=True)
            self.my_user_id = me.data.id
            logging.info(f"Bot user ID: {self.my_user_id}")
        except Exception as e:
            logging.error(f"ÙØ´Ù„ Ø¬Ù„Ø¨ user ID: {e}")
            self.my_user_id = None

        self.brains = {
            "Groq": OpenAI(api_key=os.getenv("GROQ_API_KEY"), base_url="https://api.groq.com/openai/v1"),
            "Gemini": self.gemini_client,
            "OpenAI": OpenAI(api_key=os.getenv("OPENAI_API_KEY")),
            "OpenRouter": OpenAI(api_key=os.getenv("OPENROUTER_API_KEY"), base_url="https://openrouter.ai/api/v1"),
        }

    @retry(
        stop=stop_after_attempt(5),
        wait=wait_exponential(multiplier=1.5, min=5, max=45),
        retry=retry_if_exception_type(Exception),
        reraise=True
    )
    def generate_text(self, prompt: str, system_msg: str) -> str:
        sequence = [
            ("Groq Llama 3.3", "Groq", "llama-3.3-70b-versatile"),
            ("Gemini Flash", "Gemini", "gemini-2.5-flash"),
            ("OpenAI 4o-mini", "OpenAI", "gpt-4o-mini"),
            ("OpenRouter Gemini", "OpenRouter", "google/gemini-2.5-flash"),
        ]

        for name, key, model in sequence:
            try:
                client = self.brains.get(key)
                if not client:
                    continue

                if key == "Gemini":
                    m = client.GenerativeModel(model)
                    res = m.generate_content(f"{system_msg}\n{prompt}")
                    text = res.text.strip()
                else:
                    res = client.chat.completions.create(
                        model=model,
                        messages=[
                            {"role": "system", "content": system_msg},
                            {"role": "user", "content": prompt}
                        ],
                        temperature=0.75,
                        max_tokens=420,
                        timeout=40
                    )
                    text = res.choices[0].message.content.strip()

                if text and len(text) > 80:
                    return text

            except Exception as e:
                logging.warning(f"{name} ÙØ´Ù„: {str(e)[:100]}")
                continue

        raise RuntimeError("ÙØ´Ù„ ÙƒÙ„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬")

    def clean_forbidden_words(self, text: str) -> str:
        forbidden_patterns = [
            r"Ù‚Ø³Ù…|Ø£Ù‚Ø³Ù…|Ø§Ù‚Ø³Ù…|Ù‚Ø³Ù‘Ù…|ØªÙ‚Ø³ÙŠÙ…|Ù‚Ø³Ù…Ù‡Ø§|Ù‚Ø³Ù…ÙˆØ§|Ù‚Ø³Ù… Ø¨Ø§Ù„Ù„Ù‡",
            r"Ø§Ù„Ù„Ù‡|ÙˆØ§Ù„Ù„Ù‡|Ø¨Ø§Ù„Ù„Ù‡|Ø¥Ù† Ø´Ø§Ø¡ Ø§Ù„Ù„Ù‡|Ø§Ù„Ø­Ù…Ø¯ Ù„Ù„Ù‡|Ø³Ø¨Ø­Ø§Ù† Ø§Ù„Ù„Ù‡|Ø¨Ø³Ù… Ø§Ù„Ù„Ù‡|ÙŠØ§ Ø±Ø¨|ÙŠØ§ Ø§Ù„Ù„Ù‡",
            r"[\u4e00-\u9fff]+",  # ØµÙŠÙ†ÙŠ
            r"[^\u0600-\u06FF\s0-9a-zA-Z!@#$%^&*()_+\-=\[\]{};':\"\\|,.<>/?`~]",  # Ø±Ù…ÙˆØ² ØºÙŠØ± Ø¹Ø±Ø¨ÙŠ/Ù„Ø§ØªÙŠÙ†ÙŠ/Ø£Ø±Ù‚Ø§Ù…/ØªØ±Ù‚ÙŠÙ…
        ]

        cleaned = text
        for pattern in forbidden_patterns:
            cleaned = re.sub(pattern, "", cleaned, flags=re.IGNORECASE | re.UNICODE)

        cleaned = ' '.join(cleaned.split())
        return cleaned.strip()

    def detect_hallucination(self, text: str) -> bool:
        hallucination_indicators = [
            r"Ø±Ø¨Ù…Ø§|Ù…Ù† Ø§Ù„Ù…Ø­ØªÙ…Ù„|ÙŠÙØ¹ØªÙ‚Ø¯|Ù‚Ø¯ ÙŠÙƒÙˆÙ†|ÙŠÙÙ‚Ø§Ù„|Ø­Ø³Ø¨ Ù…Ø§ Ø£Ø¹Ø±Ù|ÙÙŠ Ø§Ø¹ØªÙ‚Ø§Ø¯ÙŠ|Ø±Ø¨Ù…Ø§|ÙŠØ¨Ø¯Ùˆ|Ù…Ù† Ø§Ù„Ù…Ù…ÙƒÙ†",
            r"ÙÙŠ 202[7-9]|ÙÙŠ Ø§Ù„Ù…Ø³ØªÙ‚Ø¨Ù„|Ù‚Ø±ÙŠØ¨Ù‹Ø§|Ø³ÙŠØµØ¯Ø±|Ø³ÙŠÙƒÙˆÙ† Ù…ØªØ§Ø­|Ù‚ÙŠØ¯ Ø§Ù„ØªØ·ÙˆÙŠØ±",
            r"Ø£Ø¯Ø§Ø© Ø¬Ø¯ÙŠØ¯Ø© Ù„Ù… ØªÙØ·Ù„Ù‚ Ø¨Ø¹Ø¯|Ù…ÙŠØ²Ø© ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø©|ØºÙŠØ± Ø±Ø³Ù…ÙŠ",
        ]

        for pattern in hallucination_indicators:
            if re.search(pattern, text, re.IGNORECASE):
                return True

        if "Ù„Ø§_Ù…Ø¹Ù„ÙˆÙ…Ø§Øª_Ù…ÙˆØ«ÙˆÙ‚Ø©" in text or "Ù„Ø§_Ù‚ÙŠÙ…Ø©" in text:
            return True

        return False

    def is_semantic_duplicate(self, new_text: str) -> bool:
        new_lower = new_text.lower().strip()
        new_words = set(re.findall(r'\w+', new_lower))

        # ÙƒÙ„Ù…Ø§Øª Ø±Ø¦ÙŠØ³ÙŠØ© Ù…ØªÙƒØ±Ø±Ø© Ù…Ø­Ø¸ÙˆØ±Ø© (Ù…ÙˆØ¶ÙˆØ¹ÙŠ)
        forbidden_repeated = ["ØªØ®ØµÙŠØµ", "Ø±Ø¯ÙˆØ¯", "Ø´Ø§Øª Ø¬ÙŠ Ø¨ÙŠ ØªÙŠ", "Ø´Ø§Øª", "ØªØ®ØµÙŠØµ Ø±Ø¯ÙˆØ¯", "ØªØ¬Ø±Ø¨Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…", "ØªØ®ØµÙŠØµ Ø§Ù„Ø±Ø¯ÙˆØ¯"]
        new_has_forbidden = any(kw in new_lower for kw in forbidden_repeated)

        for old_text in self.recent_posts:
            old_lower = old_text.lower().strip()
            old_words = set(re.findall(r'\w+', old_lower))

            common = len(new_words & old_words)
            similarity = common / max(len(new_words), len(old_words)) if new_words and old_words else 0

            old_has_forbidden = any(kw in old_lower for kw in forbidden_repeated)

            # Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹ Ù†ÙØ³Ù‡ (forbidden keywords) + ØªØ´Ø§Ø¨Ù‡ > 50%
            if new_has_forbidden and old_has_forbidden and similarity > 0.50:
                logging.info("ØªÙƒØ±Ø§Ø± Ù…ÙˆØ¶ÙˆØ¹ÙŠ ÙÙŠ Ù†ÙØ³ Ø§Ù„ÙÙƒØ±Ø© â†’ Ø±ÙØ¶")
                return True

            # ØªØ´Ø§Ø¨Ù‡ Ø¹Ø§Ù…
            if similarity > 0.65:
                logging.info(f"Ø§Ù„ØªÙƒØ±Ø§Ø± Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠ Ù…Ø±ØªÙØ¹ ({similarity:.2f}) â†’ Ø±ÙØ¶")
                return True

        return False

    def already_posted(self, content: str) -> bool:
        h = hashlib.sha256(content.encode('utf-8')).hexdigest()
        with sqlite3.connect(self.db_path) as conn:
            return bool(conn.execute("SELECT 1 FROM history WHERE hash = ?", (h,)).fetchone())

    def mark_posted(self, content: str):
        h = hashlib.sha256(content.encode('utf-8')).hexdigest()
        with sqlite3.connect(self.db_path) as conn:
            conn.execute("INSERT OR IGNORE INTO history (hash, ts) VALUES (?, datetime('now'))", (h,))
        self.recent_posts.append(content)

    def fetch_fresh_rss(self, max_per_feed: int = 3, max_age_hours: int = 48) -> List[Dict]:
        articles = []
        cutoff = datetime.utcnow() - timedelta(hours=max_age_hours)
        ua = "SovereignBot/1.0 (Arabic Tech News Bot)"

        for url in self.rss_feeds:
            try:
                feed = feedparser.parse(url, agent=ua)
                if feed.bozo:
                    continue

                source = feed.feed.get('title', url.split('//')[1].split('/')[0].replace('www.', ''))

                for entry in feed.entries[:max_per_feed]:
                    pub = entry.get('published_parsed') or entry.get('updated_parsed')
                    if not pub:
                        continue

                    pub_date = date_parser.parse(time.strftime("%Y-%m-%d %H:%M:%S", pub))
                    if pub_date < cutoff:
                        continue

                    title = (entry.get('title') or "").strip()
                    link = (entry.get('link') or "").strip()
                    summary = (entry.get('summary') or entry.get('description') or "")[:280].strip()

                    if not title or not link:
                        continue

                    content_for_hash = f"{title} {link}"
                    if self.already_posted(content_for_hash):
                        continue

                    text_lower = (title + summary).lower()
                    if not any(kw in text_lower for kw in ["Ø£Ø¯Ø§Ø©", "ØªØ·Ø¨ÙŠÙ‚", "ØªÙˆÙÙŠØ±", "Ù…Ø¬Ø§Ù†ÙŠ", "Ø¨Ø¯ÙŠÙ„", "ÙƒÙŠÙ", "Ø·Ø±ÙŠÙ‚Ø©", "Ø§Ø³ØªØ®Ø¯Ù…", "Ø¬Ø±Ù‘Ø¨", "Ø£ÙØ¶Ù„", "Ù†ØµÙŠØ­Ø©", "ØªØ­Ø³ÙŠÙ†"]):
                        continue

                    articles.append({
                        "source": source,
                        "title": title,
                        "link": link,
                        "summary": summary,
                        "pub_date": pub_date,
                        "hash": content_for_hash
                    })

            except Exception as e:
                logging.warning(f"ÙØ´Ù„ {url}: {str(e)[:120]}")

        articles.sort(key=lambda x: x["pub_date"], reverse=True)
        logging.info(f"Ø¬Ù„Ø¨ {len(articles)} Ø®Ø¨Ø± Ø¬Ø¯ÙŠØ¯ Ø°Ùˆ Ù‚ÙŠÙ…Ø© Ø¹Ù…Ù„ÙŠØ©")
        return articles[:8]

    def handle_mentions(self):
        if not self.my_user_id:
            return

        MAX_REPLIES = 2
        count = 0

        try:
            mentions = self.x_client.get_users_mentions(
                id=self.my_user_id,
                since_id=self.last_mention_id,
                max_results=5,
                tweet_fields=['conversation_id', 'author_id', 'created_at']
            )
        except tweepy.TooManyRequests:
            logging.warning("429 Too Many Requests ÙÙŠ Ø¬Ù„Ø¨ Ø§Ù„Ù…Ù†Ø´Ù†Ø§Øª â†’ ØªØ®Ø·ÙŠ")
            return
        except Exception as e:
            logging.error(f"ÙØ´Ù„ Ø¬Ù„Ø¨ Ù…Ù†Ø´Ù†Ø§Øª: {e}")
            return

        if not mentions.data:
            return

        for mention in mentions.data:
            if count >= MAX_REPLIES:
                break

            tid = mention.id
            aid = mention.author_id

            if aid == self.my_user_id:
                continue
            if tid in self.replied_tweets_cache or self.has_replied_to(tid):
                continue
            if not self.can_reply_now():
                continue

            try:
                u = self.x_client.get_user(id=aid, user_fields=['public_metrics'])
                if u.data.public_metrics['followers_count'] < 20:
                    continue
            except:
                continue

            reply_text = self.generate_text(
                f"Ø±Ø¯ Ø°ÙƒÙŠ Ù‚ØµÙŠØ± ÙˆÙ…ÙÙŠØ¯ Ø¹Ù„Ù‰: '{mention.text}'",
                "Ø±Ø¯ Ø¨Ø£Ø³Ù„ÙˆØ¨ Ø®Ù„ÙŠØ¬ÙŠ Ø¹ÙÙˆÙŠØŒ Ø°ÙƒÙŠØŒ Ù‚ØµÙŠØ±ØŒ ÙŠØ¶ÙŠÙ Ù‚ÙŠÙ…Ø©."
            )

            reply_text = self.clean_forbidden_words(reply_text)

            if not reply_text or len(reply_text) > 279:
                continue

            try:
                self.x_client.create_tweet(text=reply_text, in_reply_to_tweet_id=tid)
                self.mark_as_replied(tid)
                self.replied_tweets_cache.add(tid)
                count += 1
                time.sleep(180 + random.randint(0, 120))
            except tweepy.TooManyRequests:
                logging.warning("429 Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„Ù†Ø´Ø± â†’ ØªÙˆÙ‚Ù Ù…Ø¤Ù‚Øª")
                break
            except Exception as e:
                logging.error(f"ÙØ´Ù„ Ø±Ø¯ Ø¹Ù„Ù‰ {tid}: {e}")

        if mentions.data:
            self.last_mention_id = mentions.data[0].id

    def has_replied_to(self, tweet_id: str) -> bool:
        with sqlite3.connect(self.db_path) as conn:
            return bool(conn.execute("SELECT 1 FROM replied_tweets WHERE tweet_id = ?", (tweet_id,)).fetchone())

    def mark_as_replied(self, tweet_id: str):
        with sqlite3.connect(self.db_path) as conn:
            conn.execute("INSERT OR IGNORE INTO replied_tweets (tweet_id, ts) VALUES (?, datetime('now'))", (tweet_id,))

    def can_reply_now(self) -> bool:
        now = datetime.utcnow()
        recent = sum(1 for t in self.reply_timestamps if now - t < timedelta(minutes=5))
        if recent >= 5:
            return False
        self.reply_timestamps.append(now)
        return True

    def run(self):
        try:
            fresh_news = self.fetch_fresh_rss(max_per_feed=4, max_age_hours=36)

            context = ""
            if fresh_news:
                local_first = [a for a in fresh_news if any(x in a['source'].lower() for x in ['Ù…ØµØ±', 'youm7', 'masrawy', 'Ø§Ù„ÙŠÙˆÙ…', 'Ø§Ù„Ø¨ÙˆØ§Ø¨Ø©', 'Ø§Ù„ÙˆØ·Ù†', 'Ø³Ø¹ÙˆØ¯', 'Ø¥Ù…Ø§Ø±Ø§Øª', 'Ù‚Ø·Ø±', 'ÙƒÙˆÙŠØª'])]
                top = local_first[0] if local_first else fresh_news[0]

                context = (
                    f"\n\nØ®Ø¨Ø± Ø­Ø¯ÙŠØ« Ù…Ù‡Ù… Ù…Ù† {top['source']}:\n"
                    f"{top['title']}\n"
                    f"{top['summary'][:160]}...\nØ±Ø§Ø¨Ø·: {top['link']}\n"
                    "Ø§Ø³ØªØ®Ø¯Ù…Ù‡ ÙƒØ¥Ù„Ù‡Ø§Ù… Ø¥Ø°Ø§ ÙƒØ§Ù† ÙŠØ¶ÙŠÙ Ù‚ÙŠÙ…Ø© Ø¹Ù…Ù„ÙŠØ© Ù…Ø¨Ø§Ø´Ø±Ø©."
                )

            task = f"Ø£Ø¹Ø·Ù†ÙŠ Ø®Ø¨Ø± Ø£Ùˆ Ø£Ø¯Ø§Ø© Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø¬Ø¯ÙŠØ¯Ø© ÙƒÙ„ÙŠØ§Ù‹ ÙˆÙ…ÙÙŠØ¯Ø© Ù„Ù„Ø£ÙØ±Ø§Ø¯ Ø§Ù„ÙŠÙˆÙ….{context}"

            raw_output = self.generate_text(task, SYSTEM_PROMPT)

            cleaned_output = self.clean_forbidden_words(raw_output)

            if "Ù„Ø§_Ù‚ÙŠÙ…Ø©" in cleaned_output.strip():
                logging.info("Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ù„Ø§ ÙŠØ¶ÙŠÙ Ù‚ÙŠÙ…Ø© Ø¹Ù…Ù„ÙŠØ© â†’ ØªØ®Ø·ÙŠ")
                return

            if self.already_posted(cleaned_output):
                logging.info("Ù…Ø­ØªÙˆÙ‰ Ù…ÙƒØ±Ø± Ø­Ø±ÙÙŠÙ‹Ø§ â†’ ØªØ®Ø·ÙŠ")
                return

            if self.is_semantic_duplicate(cleaned_output):
                logging.info("Ù…Ø­ØªÙˆÙ‰ Ù…Ø´Ø§Ø¨Ù‡ Ø¯Ù„Ø§Ù„ÙŠÙ‹Ø§ â†’ ØªØ®Ø·ÙŠ")
                return

            self.recent_posts.append(cleaned_output)

            image_desc = ""
            content = cleaned_output
            if "ÙˆØµÙ_ØµÙˆØ±Ø©:" in cleaned_output:
                parts = cleaned_output.rsplit("ÙˆØµÙ_ØµÙˆØ±Ø©:", 1)
                content = parts[0].strip()
                image_desc = parts[1].strip()

            tweets = [t.strip() for t in content.split("---") if t.strip()]

            prev_id = None
            for i, txt in enumerate(tweets):
                try:
                    kwargs = {"text": txt}
                    if i == 0 and image_desc:
                        logging.info(f"ØµÙˆØ±Ø© Ù…Ù‚ØªØ±Ø­Ø©: {image_desc}")
                    if prev_id:
                        kwargs["in_reply_to_tweet_id"] = prev_id
                    resp = self.x_client.create_tweet(**kwargs)
                    prev_id = resp.data["id"]
                    logging.info(f"Ù†Ø´Ø± ØªØºØ±ÙŠØ¯Ø© {i+1}/{len(tweets)} Ø¨Ù†Ø¬Ø§Ø­")
                    time.sleep(5 + random.random() * 10)
                except Exception as e:
                    logging.error(f"Ø®Ø·Ø£ ÙÙŠ Ù†Ø´Ø± ØªØºØ±ÙŠØ¯Ø© {i+1}: {e}")
                    continue

            self.handle_mentions()
            self.mark_posted(content)

        except Exception as e:
            logging.error(f"Ø®Ø·Ø£ Ø¹Ø§Ù… ÙÙŠ run(): {e}")


if __name__ == "__main__":
    bot = SovereignUltimateBot()
    bot.run()
