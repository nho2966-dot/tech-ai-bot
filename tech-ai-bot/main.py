import os
import yaml
import time
import logging
import tweepy
from openai import OpenAI
from datetime import datetime

# â”€â”€â”€ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù„ÙˆØ¬ ÙˆØ§Ù„ÙˆÙØ¶ÙÙ€ÙˆØ­ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - [TechAgent-Pro] - %(levelname)s - %(message)s'
)

class TechAgentPro:
    def __init__(self):
        # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø¨Ù€ÙˆÙØ¶ÙÙ€ÙˆØ­
        self.config = self._load_config()
        
        # ØªÙ‡ÙŠØ¦Ø© Ø¹Ù…ÙŠÙ„ X (Twitter) API v2
        self.x_client = tweepy.Client(
            bearer_token=os.getenv("X_BEARER_TOKEN"),
            consumer_key=os.getenv("X_API_KEY"),
            consumer_secret=os.getenv("X_API_SECRET"),
            access_token=os.getenv("X_ACCESS_TOKEN"),
            access_token_secret=os.getenv("X_ACCESS_SECRET"),
            wait_on_rate_limit=True
        )
        
        # ØªÙ‡ÙŠØ¦Ø© OpenAI
        self.ai_client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        self.model = self.config['api']['openai']['model']
        
        # Ø¬Ù„Ø¨ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø­Ø³Ø§Ø¨ Ø§Ù„Ø´Ø®ØµÙŠ
        self.me = self.x_client.get_me(user_fields=["public_metrics"]).data
        logging.info(f"ğŸš€ ØªÙ… ØªÙØ¹ÙŠÙ„ Ø§Ù„ÙˆÙƒÙŠÙ„ @{self.me.username} Ø¨Ù€ÙˆÙØ¶ÙÙ€ÙˆØ­.")

    def _load_config(self):
        """Ù‚Ø±Ø§Ø¡Ø© Ù…Ù„Ù YAML Ø¨Ù€ÙˆÙØ¶ÙÙ€ÙˆØ­"""
        try:
            with open("config.yaml", 'r', encoding='utf-8') as f:
                return yaml.safe_load(f)
        except Exception as e:
            logging.error(f"âŒ ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ config.yaml: {e}")
            raise

    def _generate_ai_reply(self, tweet_text, author_name):
        """ØµÙŠØ§ØºØ© Ø±Ø¯ Ø§Ø­ØªØ±Ø§ÙÙŠ ÙˆÙ…ÙˆØ«ÙˆÙ‚ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù‚ÙˆØ§Ø¹Ø¯"""
        system_prompt = (
            f"Ø£Ù†Øª {self.config['agent']['name']}. Ø®Ø¨ÙŠØ± ØªÙ‚Ù†ÙŠ Ø¹Ø§Ù„Ù…ÙŠ Ù…Ø­Ø§ÙŠØ¯.\n"
            f"Ø§Ù„Ù„ØºØ© Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©: {self.config['agent']['primary_language']}. Ø±Ø¯ Ø¨Ù†ÙØ³ Ù„ØºØ© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù….\n"
            "Ù‚ÙˆØ§Ø¹Ø¯Ùƒ:\n"
            "1. ÙƒÙ† Ù…Ù‡Ù†ÙŠØ§Ù‹ØŒ Ù„Ø§ ØµØ¯Ø§Ù… ÙˆÙ„Ø§ Ø³Ø®Ø±ÙŠØ©.\n"
            "2. Ø§Ø¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ù…ØµØ§Ø¯Ø± Ù…ÙˆØ«ÙˆÙ‚Ø©: " + ", ".join(self.config['sources']['trusted_domains']) + ".\n"
            "3. Ø§ÙØªØ­ Ø¨Ø§Ø¨ Ø§Ù„Ù†Ù‚Ø§Ø´ Ø¨Ø³Ø¤Ø§Ù„ Ø°ÙƒÙŠ.\n"
            "4. Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰ 270 Ø­Ø±ÙØ§Ù‹.\n"
            "ØªØ°ÙƒØ±: Ø¹Ù†Ø¯ Ù†Ø·Ù‚ (ÙˆÙØ¶ÙÙ€ÙˆØ­) Ø¶Ù… Ø§Ù„Ø´ÙØªÙŠÙ† Ø¬ÙŠØ¯Ø§Ù‹."
        )
        
        try:
            response = self.ai_client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": f"Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… @{author_name} Ù‚Ø§Ù„: {tweet_text}"}
                ],
                temperature=self.config['api']['openai']['temperature_reply'],
                max_tokens=self.config['api']['openai']['max_tokens_reply']
            )
            return response.choices[0].message.content.strip()
        except Exception as e:
            logging.error(f"âŒ Ø®Ø·Ø£ AI: {e}")
            return None

    def process_mentions(self):
        """ÙØ­Øµ Ø§Ù„Ø±Ø¯ÙˆØ¯ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ù†Ø´Ù†Ø§Øª Ø¨Ù€ÙˆÙØ¶ÙÙ€ÙˆØ­"""
        logging.info("ğŸ” Ø¬Ø§Ø±ÙŠ ÙØ­Øµ Ø§Ù„Ù…Ù†Ø´Ù†Ø§Øª Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©...")
        
        # Ø¬Ù„Ø¨ Ø§Ù„Ù…Ù†Ø´Ù†Ø§Øª (Ø¢Ø®Ø± 10 ØªØºØ±ÙŠØ¯Ø§Øª
