# -*- coding: utf-8 -*-
"""
Tech Expert Master Bot (X) â€” Basic Plan â€” FULL Integrated Version
Features:
- Basic plan guards (monthly + 15-min soft)
- SOURCE_MODE: RSS-only content + Credibility Gate
- Threads with numbering, hashtags only in last tweet (<=2)
- Friendly tone + readable formatting + CTA magnet
- Blurb + practical example in first tweet + "Ø­Ø³Ø¨ ØªØµÙˆÙŠØªÙƒÙ… ðŸ‘‡" injection before "Ù†Ø¨Ø°Ø©:"
- Poll Mode:
  - per pillar polls
  - per audience level polls (beginner/intermediate/advanced)
  - attractive options (pain/outcome-based)
  - infer level from replies (proxy)
  - measure engagement via public_metrics
  - store performance & bias toward best level (bandit-like)
- Dashboard CLI + Smart Recommendation
- Email recommendation via SMTP (optional)
"""

import os
import re
import json
import time
import random
import logging
from datetime import datetime, timezone
from urllib.request import Request, urlopen
from urllib.error import URLError, HTTPError
import xml.etree.ElementTree as ET

import tweepy
from openai import OpenAI


# =========================
# Logging
# =========================
logging.basicConfig(level=logging.INFO, format="%(message)s")


# =========================
# Constants
# =========================
TWEET_LIMIT = 280
THREAD_DELIM = "\n---\n"
STATE_FILE = "state.json"
AUDIT_LOG = "audit_log.jsonl"

HASHTAG_RE = re.compile(r"(?<!\w)#([\w_]+)", re.UNICODE)
URL_RE = re.compile(r"https?://\S+", re.IGNORECASE)
DIGIT_RE = re.compile(r"\d+")


# =========================
# X Developer Platform â€” Basic guards (from plan table)
# =========================
POST_CAP_MONTHLY = int(os.getenv("POST_CAP_MONTHLY", "3000"))     # user-level posts/month
READ_CAP_MONTHLY = int(os.getenv("READ_CAP_MONTHLY", "15000"))    # app-level reads/month
POSTS_PER_15MIN_SOFT = int(os.getenv("POSTS_PER_15MIN_SOFT", "95"))  # POST /2/tweets per-user is limited; keep soft < 100


# =========================
# Automation Compliance switches
# =========================
AUTO_REPLY_MENTIONS_ONLY = True
MAX_REPLIES_PER_RUN = int(os.getenv("MAX_REPLIES_PER_RUN", "3"))
BLOCK_TREND_JACKING = True  # don't auto-post about trending topics


# =========================
# Modes
# =========================
SOURCE_MODE = os.getenv("SOURCE_MODE", "1") == "1"
POLL_MODE = os.getenv("POLL_MODE", "1") == "1"
TIP_MODE = os.getenv("TIP_MODE", "1") == "1"

POLL_EVERY_DAYS = int(os.getenv("POLL_EVERY_DAYS", "7"))
POLL_DURATION_MINUTES = int(os.getenv("POLL_DURATION_MINUTES", "1440"))  # 24h

SHOW_DASHBOARD = os.getenv("SHOW_DASHBOARD", "0") == "1"
SEND_RECOMMENDATION = os.getenv("SEND_RECOMMENDATION", "0") == "1"
EMAIL_RECO_WEEKDAY_UTC = int(os.getenv("EMAIL_RECO_WEEKDAY_UTC", "6"))  # 6=Sunday


# =========================
# Audience levels
# =========================
LEVELS = ["beginner", "intermediate", "advanced"]


# =========================
# Poll Config (Per Pillar + Per Level + Attractive options)
# =========================
POLL_CONFIG = {
    "Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ": {
        "question": "ÙˆÙŠÙ† ØªØ­Ø¨ Ù†Ø±ÙƒÙ‘Ø² ÙÙŠ Ø«Ø±ÙŠØ¯ AI Ø§Ù„Ù‚Ø§Ø¯Ù…ØŸ ðŸ¤–",
        "levels": {
            "beginner": {
                "options": ["ÙˆØ´ Ù‡Ùˆ AI Ø£ØµÙ„Ù‹Ø§ØŸ", "ÙƒÙŠÙ Ø£Ø¨Ø¯Ø£ØŸ", "Ø£ÙØ¶Ù„ Ø£Ø¯ÙˆØ§Øª", "Ø£Ù…Ø«Ù„Ø© Ø¨Ø³ÙŠØ·Ø©"],
                "keywords": {
                    "ÙˆØ´ Ù‡Ùˆ AI Ø£ØµÙ„Ù‹Ø§ØŸ": ["what is ai", "basics", "introduction"],
                    "ÙƒÙŠÙ Ø£Ø¨Ø¯Ø£ØŸ": ["getting started", "first steps"],
                    "Ø£ÙØ¶Ù„ Ø£Ø¯ÙˆØ§Øª": ["tools", "beginner", "no code"],
                    "Ø£Ù…Ø«Ù„Ø© Ø¨Ø³ÙŠØ·Ø©": ["example", "use case", "demo"],
                },
            },
            "intermediate": {
                "options": ["Ø§Ù„Ù…Ø®Ø±Ø¬Ø§Øª ØºÙŠØ± Ø¯Ù‚ÙŠÙ‚Ø©", "Ø§Ù„Ø´Ø±Ø­ Ù…Ùˆ ÙˆØ§Ø¶Ø­", "Ø§Ù„ØªÙƒÙ„ÙØ© Ù…Ø±ØªÙØ¹Ø©", "ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…"],
                "keywords": {
                    "Ø§Ù„Ù…Ø®Ø±Ø¬Ø§Øª ØºÙŠØ± Ø¯Ù‚ÙŠÙ‚Ø©": ["evaluation", "hallucination", "quality"],
                    "Ø§Ù„Ø´Ø±Ø­ Ù…Ùˆ ÙˆØ§Ø¶Ø­": ["prompt", "explainability"],
                    "Ø§Ù„ØªÙƒÙ„ÙØ© Ù…Ø±ØªÙØ¹Ø©": ["cost", "pricing", "tokens", "billing"],
                    "ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…": ["best practices", "optimization"],
                },
            },
            "advanced": {
                "options": ["RAG Ø¨Ø´ÙƒÙ„ ØµØ­ÙŠØ­", "Agents Ø¹Ù…Ù„ÙŠÙ‹Ø§", "ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù…Ø®Ø±Ø¬Ø§Øª", "Ø£Ù…Ø§Ù† Ø§Ù„Ù†Ù…Ø§Ø°Ø¬"],
                "keywords": {
                    "RAG Ø¨Ø´ÙƒÙ„ ØµØ­ÙŠØ­": ["rag", "vector", "retrieval", "embedding"],
                    "Agents Ø¹Ù…Ù„ÙŠÙ‹Ø§": ["agentic", "workflow", "orchestration"],
                    "ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù…Ø®Ø±Ø¬Ø§Øª": ["eval", "benchmark"],
                    "Ø£Ù…Ø§Ù† Ø§Ù„Ù†Ù…Ø§Ø°Ø¬": ["safety", "guardrails", "security"],
                },
            },
        },
    },

    "Ø§Ù„Ø­ÙˆØ³Ø¨Ø© Ø§Ù„Ø³Ø­Ø§Ø¨ÙŠØ©": {
        "question": "Ø¥ÙŠØ´ Ø£ÙƒØ«Ø± Ø´ÙŠØ¡ ÙŠØ±Ù‡Ù‚Ùƒ ÙÙŠ Ø§Ù„Ø³Ø­Ø§Ø¨Ø©ØŸ â˜ï¸",
        "levels": {
            "beginner": {
                "options": ["ÙˆØ´ Ù‡ÙŠ Ø§Ù„Ø³Ø­Ø§Ø¨Ø©ØŸ", "Ø£ÙˆÙ„ Ø®Ø¯Ù…Ø© Ø£ØªØ¹Ù„Ù…Ù‡Ø§", "ÙØ±Ù‚ AWS ÙˆAzure", "Ø£Ù…Ø«Ù„Ø© Ø§Ø³ØªØ®Ø¯Ø§Ù…"],
                "keywords": {
                    "ÙˆØ´ Ù‡ÙŠ Ø§Ù„Ø³Ø­Ø§Ø¨Ø©ØŸ": ["cloud basics", "introduction"],
                    "Ø£ÙˆÙ„ Ø®Ø¯Ù…Ø© Ø£ØªØ¹Ù„Ù…Ù‡Ø§": ["getting started", "compute"],
                    "ÙØ±Ù‚ AWS ÙˆAzure": ["aws vs azure"],
                    "Ø£Ù…Ø«Ù„Ø© Ø§Ø³ØªØ®Ø¯Ø§Ù…": ["use case", "example"],
                },
            },
            "intermediate": {
                "options": ["Ø§Ø±ØªÙØ§Ø¹ Ø§Ù„ØªÙƒØ§Ù„ÙŠÙ", "Ø§Ù„ØªØ¹Ù‚ÙŠØ¯", "Ø§Ù„Ø£Ù…Ø§Ù†", "Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯ÙŠØ©"],
                "keywords": {
                    "Ø§Ø±ØªÙØ§Ø¹ Ø§Ù„ØªÙƒØ§Ù„ÙŠÙ": ["finops", "cost", "billing", "spend"],
                    "Ø§Ù„ØªØ¹Ù‚ÙŠØ¯": ["architecture", "design", "complexity"],
                    "Ø§Ù„Ø£Ù…Ø§Ù†": ["security", "iam", "zero trust", "compliance"],
                    "Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯ÙŠØ©": ["reliability", "resilience", "availability"],
                },
            },
            "advanced": {
                "options": ["FinOps Ù…ØªÙ‚Ø¯Ù…", "Zero Trust", "Multiâ€‘Cloud", "SRE Ø¹Ù…Ù„ÙŠ"],
                "keywords": {
                    "FinOps Ù…ØªÙ‚Ø¯Ù…": ["finops", "governance"],
                    "Zero Trust": ["zero trust", "identity", "entra"],
                    "Multiâ€‘Cloud": ["multi cloud", "hybrid"],
                    "SRE Ø¹Ù…Ù„ÙŠ": ["sre", "slo", "error budget", "observability"],
                },
            },
        },
    },

    "Ø§Ù„Ø¨Ø±Ù…Ø¬Ø©": {
        "question": "Ø¥ÙŠØ´ Ø£ÙƒØ«Ø± Ø´ÙŠØ¡ ÙŠØ¶ÙŠÙ‘Ø¹ ÙˆÙ‚ØªÙƒ ÙÙŠ Ø§Ù„Ø¨Ø±Ù…Ø¬Ø©ØŸ ðŸ‘¨â€ðŸ’»",
        "levels": {
            "beginner": {
                "options": ["Ù…Ù† ÙˆÙŠÙ† Ø£Ø¨Ø¯Ø£ØŸ", "Ù„ØºØ© Ø£ØªØ¹Ù„Ù…Ù‡Ø§", "Ø£Ù…Ø«Ù„Ø© Ø¨Ø³ÙŠØ·Ø©", "Ø£Ø®Ø·Ø§Ø¡ Ø´Ø§Ø¦Ø¹Ø©"],
                "keywords": {
                    "Ù…Ù† ÙˆÙŠÙ† Ø£Ø¨Ø¯Ø£ØŸ": ["getting started", "roadmap"],
                    "Ù„ØºØ© Ø£ØªØ¹Ù„Ù…Ù‡Ø§": ["language choice", "beginner"],
                    "Ø£Ù…Ø«Ù„Ø© Ø¨Ø³ÙŠØ·Ø©": ["tutorial", "example"],
                    "Ø£Ø®Ø·Ø§Ø¡ Ø´Ø§Ø¦Ø¹Ø©": ["common mistakes"],
                },
            },
            "intermediate": {
                "options": ["Debugging", "Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª", "ØªÙ†Ø¸ÙŠÙ… Ø§Ù„ÙƒÙˆØ¯", "Ø£Ø¯Ø§Ø¡ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚"],
                "keywords": {
                    "Debugging": ["debug", "bug", "error"],
                    "Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª": ["testing", "unit test", "integration"],
                    "ØªÙ†Ø¸ÙŠÙ… Ø§Ù„ÙƒÙˆØ¯": ["refactor", "clean code", "maintain"],
                    "Ø£Ø¯Ø§Ø¡ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚": ["performance", "profiling", "latency"],
                },
            },
            "advanced": {
                "options": ["Refactoring ÙƒØ¨ÙŠØ±", "Ø£Ø¯Ø§Ø¡ Ø¹Ø§Ù„ÙŠ", "Ø£Ù†Ù…Ø§Ø· Ù…Ø¹Ù…Ø§Ø±ÙŠØ©", "Scalability"],
                "keywords": {
                    "Refactoring ÙƒØ¨ÙŠØ±": ["legacy", "refactor"],
                    "Ø£Ø¯Ø§Ø¡ Ø¹Ø§Ù„ÙŠ": ["low latency", "high performance", "profil"],
                    "Ø£Ù†Ù…Ø§Ø· Ù…Ø¹Ù…Ø§Ø±ÙŠØ©": ["architecture", "patterns"],
                    "Scalability": ["scaling", "distributed", "throughput"],
                },
            },
        },
    },
}


# =========================
# SMTP Email config (optional)
# =========================
SMTP_HOST = os.getenv("SMTP_HOST", "")
SMTP_PORT = int(os.getenv("SMTP_PORT", "587"))
SMTP_USER = os.getenv("SMTP_USER", "")
SMTP_PASSWORD = os.getenv("SMTP_PASSWORD", "")
SMTP_FROM = os.getenv("SMTP_FROM", "")
RECOMMENDATION_EMAIL_TO = os.getenv("RECOMMENDATION_EMAIL_TO", "")


class TechExpertMasterBasicFull:
    def __init__(self):
        logging.info("--- Tech Expert Master | Basic FULL ---")

        # DRY_RUN
        self.DRY_RUN = os.getenv("DRY_RUN", "0") == "1"

        # hashtags policy
        self.MAX_HASHTAGS = int(os.getenv("MAX_HASHTAGS", "2"))
        self.DEFAULT_HASHTAGS = ["#ØªÙ‚Ù†ÙŠØ©", "#Ø¨Ø±Ù…Ø¬Ø©"]

        # signature optional
        self.SIGNATURE = os.getenv("SIGNATURE", "").strip()

        # Required keys
        required = [
            "OPENROUTER_API_KEY",
            "X_API_KEY", "X_API_SECRET",
            "X_ACCESS_TOKEN", "X_ACCESS_SECRET",
        ]
        missing = [k for k in required if not os.getenv(k)]
        if missing:
            raise EnvironmentError(f"Missing env vars: {', '.join(missing)}")

        # OpenRouter client
        self.ai_client = OpenAI(
            base_url="https://openrouter.ai/api/v1",
            api_key=os.getenv("OPENROUTER_API_KEY")
        )

        # Tweepy client
        self.client_v2 = tweepy.Client(
            consumer_key=os.getenv("X_API_KEY"),
            consumer_secret=os.getenv("X_API_SECRET"),
            access_token=os.getenv("X_ACCESS_TOKEN"),
            access_token_secret=os.getenv("X_ACCESS_SECRET"),
            wait_on_rate_limit=True
        )

        # Content pillars
        self.content_pillars = {
            "Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ": "Ù…Ù„Ø®ØµØ§Øª Ù…ÙˆØ«ÙˆÙ‚Ø© + Ø£Ù…Ø«Ù„Ø© Ø¹Ù…Ù„ÙŠØ©",
            "Ø§Ù„Ø­ÙˆØ³Ø¨Ø© Ø§Ù„Ø³Ø­Ø§Ø¨ÙŠØ©": "Ù…Ø³ØªØ¬Ø¯Ø§Øª Ø±Ø³Ù…ÙŠØ© + ØªØ·Ø¨ÙŠÙ‚ Ø¹Ù…Ù„ÙŠ",
            "Ø§Ù„Ø¨Ø±Ù…Ø¬Ø©": "Ø£ÙØ¶Ù„ Ø§Ù„Ù…Ù…Ø§Ø±Ø³Ø§Øª + Ø­Ù„ÙˆÙ„ Ø¹Ù…Ù„ÙŠØ©",
        }

        # RSS feeds
        self.FEEDS = {
            "Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ": [
                "https://cloud.google.com/blog/rss",
                "https://blogs.microsoft.com/feed",
            ],
            "Ø§Ù„Ø­ÙˆØ³Ø¨Ø© Ø§Ù„Ø³Ø­Ø§Ø¨ÙŠØ©": [
                "https://aws.amazon.com/about-aws/whats-new/recent/feed/",
                "https://cloud.google.com/blog/rss",
            ],
            "Ø§Ù„Ø¨Ø±Ù…Ø¬Ø©": [
                "https://devblogs.microsoft.com/dotnet/feed/",
                "https://devblogs.microsoft.com/visualstudio/feed/",
            ],
        }

        # System instruction: friendly + credibility
        self.system_instr = (
            "Ø§ÙƒØªØ¨ ÙƒÙ…Ø®ØªØµ ØªÙ‚Ù†ÙŠ Ø¹Ø±Ø¨ÙŠ Ø¨Ø£Ø³Ù„ÙˆØ¨ ÙˆØ¯ÙˆØ¯ ÙˆÙˆØ§Ø¶Ø­.\n"
            "Ù…Ù…Ù†ÙˆØ¹ Ø§Ø®ØªÙ„Ø§Ù‚ Ù…ØµØ§Ø¯Ø±/Ø±ÙˆØ§Ø¨Ø·/Ø¥Ø­ØµØ§Ø¡Ø§Øª/Ø£Ø±Ù‚Ø§Ù….\n"
            "Ø§Ù„ØªØ²Ù… Ø¨Ø§Ù„Ù…ØµØ¯Ø± Ø§Ù„Ù…ÙØ¹Ø·Ù‰ ÙÙ‚Ø·.\n"
            "Ø§Ù„ØªÙ†Ø³ÙŠÙ‚: Ø£Ø³Ø·Ø± Ù‚ØµÙŠØ±Ø©ØŒ ÙÙƒØ±Ø© ÙˆØ§Ø­Ø¯Ø© Ù„ÙƒÙ„ ØªØºØ±ÙŠØ¯Ø©.\n"
            "ÙƒÙ„ ØªØºØ±ÙŠØ¯Ø©: Hook Ø«Ù… Value Ø«Ù… CTA (Ø³Ø¤Ø§Ù„ Ù„Ø·ÙŠÙ).\n"
            "Ù„Ø§ ØªØ¶Ø¹ Ù‡Ø§Ø´ØªØ§Ù‚Ø§Øª Ø¯Ø§Ø®Ù„ Ø§Ù„Ù†Øµ.\n"
            "Ù„Ø§ ØªØ¶Ø¹ Ø±ÙˆØ§Ø¨Ø· Ø¥Ù„Ø§ Ø±Ø§Ø¨Ø· Ø§Ù„Ù…ØµØ¯Ø± Ù…Ø±Ø© ÙˆØ§Ø­Ø¯Ø© ÙÙ‚Ø· ÙÙŠ Ø¢Ø®Ø± ØªØºØ±ÙŠØ¯Ø© ÙƒØ³Ø·Ø± ÙŠØ¨Ø¯Ø£ Ø¨Ù€ 'Ø§Ù„Ù…ØµØ¯Ø±:'.\n"
        )

        # Keywords to decide replies (mentions only)
        self.TECH_TRIGGERS = [
            "ÙƒÙŠÙ", "Ù„Ù…Ø§Ø°Ø§", "Ù…Ø§", "ÙˆØ´", "Ø£ÙØ¶Ù„", "Ø´Ø±Ø­", "Ø­Ù„", "Ù…Ø´ÙƒÙ„Ø©", "Ø®Ø·Ø£",
            "error", "bug", "issue", "api", "python", "javascript", "rust",
            "ai", "security", "cloud", "aws", "azure", "gcp"
        ]

        # Profile checklist reminder (human action)
        logging.info("ðŸ“Œ Profile Checklist (Manual):")
        logging.info("â€¢ Bio: Threads ØªÙ‚Ù†ÙŠØ© Ù…Ø¨Ù†ÙŠØ© Ø¹Ù„Ù‰ Ù…ØµØ§Ø¯Ø± Ø±Ø³Ù…ÙŠØ© + Ø£Ù…Ø«Ù„Ø© Ø¹Ù…Ù„ÙŠØ©")
        logging.info("â€¢ Pin Tweet: Ø£ÙØ¶Ù„ Ø«Ø±ÙŠØ¯/ØªØ¹Ø±ÙŠÙ Ø¨Ø§Ù„Ù‚ÙŠÙ…Ø© + Ù‚Ø§Ø¦Ù…Ø© Ù…ØµØ§Ø¯Ø±")
        logging.info("â€¢ Banner: ÙˆØ¹Ø¯ Ù‚ÙŠÙ…Ø© ÙˆØ§Ø¶Ø­ (Ù…Ù„Ø®ØµØ§Øª Ù…ÙˆØ«ÙˆÙ‚Ø© + Ø£Ù…Ø«Ù„Ø© Ø¹Ù…Ù„ÙŠØ©)")

        self.state = self._load_state()

    # =========================================================
    # State + Audit
    # =========================================================
    def _load_state(self):
        if os.path.exists(STATE_FILE):
            try:
                with open(STATE_FILE, "r", encoding="utf-8") as f:
                    s = json.load(f)
            except Exception:
                s = {}
        else:
            s = {}

        s.setdefault("used_links", [])

        # month guards
        s.setdefault("month_key", None)
        s.setdefault("posts_this_month", 0)
        s.setdefault("reads_this_month", 0)
        s.setdefault("post_times_15m", [])

        # mentions
        s.setdefault("last_mention_id", None)

        # polls
        s.setdefault("last_poll_at", None)
        s.setdefault("last_poll_id", None)
        s.setdefault("last_poll_pillar", None)
        s.setdefault("last_poll_level", None)
        s.setdefault("last_poll_processed", False)
        s.setdefault("poll_pillar_index", 0)
        s.setdefault("poll_perf", {})  # pillar -> level -> stats

        return s

    def _save_state(self):
        with open(STATE_FILE, "w", encoding="utf-8") as f:
            json.dump(self.state, f, ensure_ascii=False, indent=2)

    def _audit(self, event_type: str, payload: dict, content_type: str = None):
        rec = {
            "ts": datetime.now(timezone.utc).isoformat(),
            "type": event_type,
            "content_type": content_type or payload.get("content_type"),
            "payload": payload
        }
        with open(AUDIT_LOG, "a", encoding="utf-8") as f:
            f.write(json.dumps(rec, ensure_ascii=False) + "\n")

    # =========================================================
    # Guards
    # =========================================================
    def _month_key(self):
        now = datetime.now(timezone.utc)
        return f"{now.year}-{now.month:02d}"

    def _ensure_month_bucket(self):
        mk = self._month_key()
        if self.state.get("month_key") != mk:
            self.state["month_key"] = mk
            self.state["posts_this_month"] = 0
            self.state["reads_this_month"] = 0
            self.state["post_times_15m"] = []
            self._save_state()

    def _can_post_monthly(self, n=1):
        self._ensure_month_bucket()
        return self.state["posts_this_month"] + n <= POST_CAP_MONTHLY

    def _mark_posted_monthly(self, n=1):
        self._ensure_month_bucket()
        self.state["posts_this_month"] += n
        self._save_state()

    def _can_read_monthly(self, n=1):
        self._ensure_month_bucket()
        return self.state["reads_this_month"] + n <= READ_CAP_MONTHLY

    def _mark_read_monthly(self, n=1):
        self._ensure_month_bucket()
        self.state["reads_this_month"] += n
        self._save_state()

    def _can_post_15m(self, n=1):
        self._ensure_month_bucket()
        now = time.time()
        window_start = now - 15 * 60
        self.state["post_times_15m"] = [t for t in self.state["post_times_15m"] if t >= window_start]
        self._save_state()
        return len(self.state["post_times_15m"]) + n <= POSTS_PER_15MIN_SOFT

    def _mark_post_15m(self, n=1):
        now = time.time()
        self.state["post_times_15m"].extend([now] * n)
        self.state["post_times_15m"] = self.state["post_times_15m"][-400:]
        self._save_state()

    def _sleep_jitter(self, base=1.2, spread=2.0):
        time.sleep(base + random.random() * spread)

    # =========================================================
    # Automation compliance guard (simple)
    # =========================================================
    def _automation_compliance_guard(self, context: str) -> bool:
        if BLOCK_TREND_JACKING and ("trend" in context.lower() or "ØªØ±Ù†Ø¯" in context):
            logging.info("ðŸ›‘ Ù…Ù†Ø¹: ØªØ±Ù†Ø¯ (Automation Rules safety).")
            return False
        return True

    # =========================================================
    # Formatting (Shareability): short lines, 1 idea, breaks
    # =========================================================
    def _wrap_lines(self, text: str, max_len: int = 60) -> str:
        """
        Simple word wrap to max_len chars per line (best effort for Arabic/English).
        """
        words = (text or "").split()
        if not words:
            return ""
        lines, cur = [], []
        cur_len = 0
        for w in words:
            add = len(w) + (1 if cur else 0)
            if cur_len + add > max_len:
                lines.append(" ".join(cur))
                cur = [w]
                cur_len = len(w)
            else:
                cur.append(w)
                cur_len += add
        if cur:
            lines.append(" ".join(cur))
        return "\n".join(lines)

    def _enforce_readability(self, text: str) -> str:
        """
        - breaks paragraphs
        - wraps lines
        - avoids huge blocks
        """
        parts = [p.strip() for p in (text or "").split("\n") if p.strip()]
        if not parts:
            return text.strip()
        wrapped = [self._wrap_lines(p, max_len=60) for p in parts]
        # keep it visually scannable
        out = "\n".join(wrapped)
        out = re.sub(r"\n{3,}", "\n\n", out).strip()
        return out

    # =========================================================
    # Hashtag policy: <=2, last tweet only
    # =========================================================
    def _enforce_hashtag_policy(self, tags):
        return tags[:2]

    # =========================================================
    # CTA (friendly magnets)
    # =========================================================
    def _smart_cta(self, pillar=None):
        pool = [
            "ØªØ­Ø¨Ù‡Ø§ ÙƒØ®Ø·ÙˆØ§Øª ÙˆÙ„Ø§ ÙƒÙ‚Ø§Ø¦Ù…Ø© Ø£Ø¯ÙˆØ§ØªØŸ",
            "Ù‚Ø¯ ÙˆØ§Ø¬Ù‡Øª Ø§Ù„Ø´ÙŠ Ù‡Ø°Ø§ØŸ Ø¥ÙŠØ´ ÙƒØ§Ù† Ø£ØµØ¹Ø¨ Ø¬Ø²Ø¡ØŸ",
            "ØªØ­Ø¨ Ù…Ø«Ø§Ù„ Ø¹Ù…Ù„ÙŠ Ø¹Ù„Ù‰ Ø¨ÙŠØ¦ØªÙƒØŸ",
            "Ø£ÙŠ Ø®ÙŠØ§Ø± ÙŠÙ†Ø§Ø³Ø¨ Ø´ØºÙ„Ùƒ Ø£ÙƒØ«Ø±ØŸ",
            "ØªØ¨ØºØ§Ù†ÙŠ Ø£Ø¨Ø³Ø·Ù‡Ø§ Ø£ÙƒØ«Ø± ÙˆÙ„Ø§ ÙƒØ°Ø§ ÙˆØ§Ø¶Ø­Ø©ØŸ",
        ]
        # light personalization
        if pillar == "Ø§Ù„Ø­ÙˆØ³Ø¨Ø© Ø§Ù„Ø³Ø­Ø§Ø¨ÙŠØ©":
            pool.append("ØªØ¨ØºÙ‰ Ù…Ø«Ø§Ù„ AWS ÙˆÙ„Ø§ Azure ÙˆÙ„Ø§ GCPØŸ")
        if pillar == "Ø§Ù„Ø¨Ø±Ù…Ø¬Ø©":
            pool.append("ØªÙØ¶Ù„ Ù…Ø«Ø§Ù„ Python ÙˆÙ„Ø§ .NETØŸ")
        if pillar == "Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ":
            pool.append("ØªØ¨ØºÙ‰ Ù…Ø«Ø§Ù„ Prompt ÙˆÙ„Ø§ RAGØŸ")
        return random.choice(pool)

    def _ensure_cta(self, text: str, pillar=None) -> str:
        if "ØŸ" not in text and "?" not in text:
            return text.rstrip() + "\n" + self._smart_cta(pillar)
        return text

    # =========================================================
    # Blurb with practical example
    # =========================================================
    def _make_blurb(self, title: str, summary: str) -> str:
        prompt = (
            "Ø§ÙƒØªØ¨ Ù†Ø¨Ø°Ø© Ù‚ØµÙŠØ±Ø© Ø¬Ø¯Ù‹Ø§ (Ø³Ø·Ø± ÙˆØ§Ø­Ø¯ Ø£Ùˆ Ø³Ø·Ø±ÙŠÙ†) ØªØ¨Ø¯Ø£ Ø¨Ù€ 'Ù†Ø¨Ø°Ø©:'\n"
            "ÙˆØªØ­ØªÙˆÙŠ 'Ù…Ø«Ø§Ù„ Ø³Ø±ÙŠØ¹:' ÙŠÙˆØ¶Ø­ Ø§Ù„ÙÙƒØ±Ø© Ø¨Ù…Ø«Ø§Ù„ Ø¹Ù…Ù„ÙŠ ØµØºÙŠØ± Ø¬Ø¯Ù‹Ø§.\n"
            "Ø¨Ø¯ÙˆÙ† Ø±ÙˆØ§Ø¨Ø·ØŒ Ø¨Ø¯ÙˆÙ† Ù‡Ø§Ø´ØªØ§Ù‚Ø§ØªØŒ Ø¨Ø¯ÙˆÙ† Ø£Ø±Ù‚Ø§Ù….\n"
            "Ù„ØºØ© ÙˆØ¯Ù‘ÙŠØ© ÙˆÙˆØ§Ø¶Ø­Ø©.\n\n"
            f"Ø§Ù„Ø¹Ù†ÙˆØ§Ù†: {title}\n"
            f"Ø§Ù„Ù…Ù„Ø®Øµ: {summary}\n"
        )

        resp = self.ai_client.chat.completions.create(
            model="qwen/qwen-2.5-72b-instruct",
            messages=[
                {"role": "system", "content": "Ø³Ø·Ø±/Ø³Ø·Ø±ÙŠÙ† ÙÙ‚Ø·. Ø¨Ø¯ÙˆÙ† Ø£Ø±Ù‚Ø§Ù…/Ø±ÙˆØ§Ø¨Ø·/Ù‡Ø§Ø´ØªØ§Ù‚Ø§Øª."},
                {"role": "user", "content": prompt}
            ]
        )
        blurb = resp.choices[0].message.content.strip()
        blurb = re.sub(URL_RE, "", blurb)
        blurb = re.sub(HASHTAG_RE, "", blurb)
        blurb = re.sub(DIGIT_RE, "", blurb).strip()

        if not blurb.startswith("Ù†Ø¨Ø°Ø©:"):
            blurb = "Ù†Ø¨Ø°Ø©: " + blurb
        if "Ù…Ø«Ø§Ù„" not in blurb:
            blurb = blurb.rstrip(" .") + " â€” Ù…Ø«Ø§Ù„ Ø³Ø±ÙŠØ¹: Ø·Ø¨Ù‘Ù‚ Ø§Ù„ÙÙƒØ±Ø© Ø¹Ù„Ù‰ Ø¬Ø²Ø¡ ØµØºÙŠØ± Ø£ÙˆÙ„Ù‹Ø§."

        if len(blurb) > 170:
            blurb = blurb[:169].rstrip() + "â€¦"
        return blurb

    def _prepend_blurb_to_first_tweet(self, tweets, blurb: str, soft_limit=220):
        if not tweets:
            return tweets
        first = tweets[0].strip()
        if "Ù†Ø¨Ø°Ø©:" in first:
            return tweets

        lines = [l.strip() for l in first.splitlines() if l.strip()]
        hook = lines[0] if lines else first
        rest = "\n".join(lines[1:]).strip()

        new_first = f"{hook}\n{blurb}"
        if rest:
            new_first += f"\n{rest}"

        if len(new_first) > soft_limit:
            new_first = new_first[:soft_limit - 1].rstrip() + "â€¦"

        tweets[0] = new_first
        return tweets

    def _inject_poll_prefix_before_blurb(self, tweets):
        """
        Inject 'Ø­Ø³Ø¨ ØªØµÙˆÙŠØªÙƒÙ… ðŸ‘‡' before 'Ù†Ø¨Ø°Ø©:' in first tweet (if present).
        """
        if not tweets:
            return tweets
        if "Ù†Ø¨Ø°Ø©:" in tweets[0] and "Ø­Ø³Ø¨ ØªØµÙˆÙŠØªÙƒÙ…" not in tweets[0]:
            tweets[0] = re.sub(r"\nÙ†Ø¨Ø°Ø©:", "\nØ­Ø³Ø¨ ØªØµÙˆÙŠØªÙƒÙ… ðŸ‘‡\nÙ†Ø¨Ø°Ø©:", tweets[0], count=1)
        return tweets

    # =========================================================
    # RSS fetch + parse
    # =========================================================
    def _fetch_url(self, url, timeout=12):
        headers = {"User-Agent": "Mozilla/5.0 (compatible; TechExpertBot/1.0)"}
        req = Request(url, headers=headers)
        with urlopen(req, timeout=timeout) as resp:
            return resp.read()

    def _strip_html(self, s: str) -> str:
        if not s:
            return ""
        s = re.sub(r"<[^>]+>", " ", s)
        s = re.sub(r"\s{2,}", " ", s).strip()
        return s

    def _parse_feed(self, xml_bytes: bytes):
        items = []
        try:
            root = ET.fromstring(xml_bytes)
        except Exception:
            return items

        tag = root.tag.lower()

        if "rss" in tag:
            channel = root.find("channel")
            if channel is None:
                return items
            for it in channel.findall("item"):
                title = (it.findtext("title") or "").strip()
                link = (it.findtext("link") or "").strip()
                desc = (it.findtext("description") or "").strip()
                items.append({
                    "title": self._strip_html(title),
                    "link": link,
                    "summary": self._strip_html(desc)
                })
            return items

        if "feed" in tag:
            ns = ""
            if root.tag.startswith("{"):
                ns = root.tag.split("}")[0] + "}"
            for entry in root.findall(f"{ns}entry"):
                title = (entry.findtext(f"{ns}title") or "").strip()
                summary = (entry.findtext(f"{ns}summary") or entry.findtext(f"{ns}content") or "").strip()
                link = ""
                for l in entry.findall(f"{ns}link"):
                    rel = l.attrib.get("rel", "alternate")
                    if rel == "alternate" and l.attrib.get("href"):
                        link = l.attrib["href"]
                        break
                items.append({
                    "title": self._strip_html(title),
                    "link": link.strip(),
                    "summary": self._strip_html(summary)
                })
            return items

        return items

    def _get_source_item(self, pillar: str, keyword: str = None):
        feeds = self.FEEDS.get(pillar, [])
        if not feeds:
            return None

        keyword_l = (keyword or "").lower().strip()

        random.shuffle(feeds)
        for feed_url in feeds:
            try:
                xml_bytes = self._fetch_url(feed_url)
                items = self._parse_feed(xml_bytes)

                # Prefer keyword match if provided
                if keyword_l:
                    for it in items[:50]:
                        blob = (it.get("title", "") + " " + it.get("summary", "")).lower()
                        if keyword_l in blob and it.get("link") and it["link"] not in self.state["used_links"]:
                            return it

                # fallback to first unused
                for it in items[:25]:
                    if it.get("link") and it["link"] not in self.state["used_links"]:
                        return it

            except (HTTPError, URLError, TimeoutError):
                continue
            except Exception:
                continue

        return None

    # =========================================================
    # Credibility gate
    # =========================================================
    def _credibility_gate(self, tweets, source_link: str, source_text: str):
        joined = "\n".join(tweets)

        # Only allow source link
        urls = URL_RE.findall(joined)
        allowed = {source_link}
        for u in urls:
            uu = u.rstrip(").,]")
            if uu not in allowed:
                return False, f"Ø±Ø§Ø¨Ø· ØºÙŠØ± Ù…Ø³Ù…ÙˆØ­: {u}"

        # Must include source link at least once
        if source_link not in joined:
            return False, "Ø±Ø§Ø¨Ø· Ø§Ù„Ù…ØµØ¯Ø± ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯"

        # numbers must exist in source text
        out_nums = set(DIGIT_RE.findall(joined))
        src_nums = set(DIGIT_RE.findall(source_text or ""))
        extra = out_nums - src_nums
        if extra:
            return False, f"Ø£Ø±Ù‚Ø§Ù… ØºÙŠØ± Ù…Ø¯Ø¹ÙˆÙ…Ø© Ø¨Ø§Ù„Ù…ØµØ¯Ø±: {sorted(list(extra))[:10]}"

        return True, "ok"

    # =========================================================
    # Thread helpers: split/number/hashtags-last-only
    # =========================================================
    def _normalize_thread_parts(self, raw: str):
        parts = [p.strip() for p in raw.split(THREAD_DELIM) if p.strip()]
        if not parts:
            parts = [raw.strip()]
        return parts

    def _add_numbering_prefix(self, tweets):
        n = len(tweets)
        if n <= 1:
            return [tweets[0][:TWEET_LIMIT]]
        out = []
        for i, t in enumerate(tweets, start=1):
            prefix = f"{i}/{n} "
            max_len = TWEET_LIMIT - len(prefix)
            t = t.strip()
            if len(t) > max_len:
                t = t[:max_len - 1].rstrip() + "â€¦"
            out.append(prefix + t)
        return out

    def _apply_hashtags_to_last_tweet(self, tweets):
        tags = self._enforce_hashtag_policy(self.DEFAULT_HASHTAGS)[: self.MAX_HASHTAGS]
        tag_line = " ".join(tags).strip()

        last = tweets[-1].rstrip()
        # keep hashtags as last line
        last = f"{last}\n\n{tag_line}".strip()
        if self.SIGNATURE:
            last = (last + f" {self.SIGNATURE}").strip()

        # trim while keeping tags
        if len(last) > TWEET_LIMIT:
            reserve = len("\n\n" + tag_line) + (len(" " + self.SIGNATURE) if self.SIGNATURE else 0)
            body_max = max(0, TWEET_LIMIT - reserve)
            body = tweets[-1]
            body = body[:max(0, body_max - 1)].rstrip() + "â€¦" if body_max > 0 else ""
            last = f"{body}\n\n{tag_line}".strip()
            if self.SIGNATURE:
                last = (last + f" {self.SIGNATURE}").strip()

        tweets[-1] = last
        return tweets

    # =========================================================
    # Posting
    # =========================================================
    def _publish_tweet(self, text: str, in_reply_to_tweet_id=None):
        if not self._can_post_monthly(1):
            logging.info("ðŸ›‘ Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„Ù†Ø´Ø±: ØªØ¬Ø§ÙˆØ² Ø­Ø¯ Ø§Ù„Ù†Ø´Ø± Ø§Ù„Ø´Ù‡Ø±ÙŠ.")
            self._audit("blocked_post_cap", {"cap": POST_CAP_MONTHLY}, content_type="guard")
            return None

        if not self._can_post_15m(1):
            logging.info("ðŸ›‘ Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„Ù†Ø´Ø±: Ù‚Ø±Ø¨Øª ØªØªØ¬Ø§ÙˆØ² Ø­Ø¯ 15 Ø¯Ù‚ÙŠÙ‚Ø© (soft).")
            self._audit("blocked_post_15m", {"soft": POSTS_PER_15MIN_SOFT}, content_type="guard")
            return None

        if self.DRY_RUN:
            logging.info(f"[DRY_RUN] Tweet:\n{text}\n")
            self._mark_posted_monthly(1)
            self._mark_post_15m(1)
            return f"dry_{random.randint(1000,9999)}"

        if in_reply_to_tweet_id:
            resp = self.client_v2.create_tweet(text=text, in_reply_to_tweet_id=in_reply_to_tweet_id, user_auth=True)
        else:
            resp = self.client_v2.create_tweet(text=text, user_auth=True)

        tid = resp.data["id"]
        self._mark_posted_monthly(1)
        self._mark_post_15m(1)
        return tid

    def _publish_thread(self, tweets, pillar=None):
        needed = len(tweets)
        if not self._can_post_monthly(needed):
            logging.info("ðŸ›‘ Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„Ø«Ø±ÙŠØ¯: Ø³ÙŠØ¤Ø¯ÙŠ Ù„ØªØ¬Ø§ÙˆØ² Ø­Ø¯ Ø§Ù„Ù†Ø´Ø± Ø§Ù„Ø´Ù‡Ø±ÙŠ.")
            self._audit("blocked_thread_post_cap", {"needed": needed, "cap": POST_CAP_MONTHLY}, content_type="guard")
            return []

        if not self._can_post_15m(needed):
            logging.info("ðŸ›‘ Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„Ø«Ø±ÙŠØ¯: Ø¹Ø¯Ø¯ Ø§Ù„ØªØºØ±ÙŠØ¯Ø§Øª Ù‚Ø¯ ÙŠØ¶ØºØ· Ø­Ø¯ 15 Ø¯Ù‚ÙŠÙ‚Ø©.")
            self._audit("blocked_thread_15m", {"needed": needed, "soft": POSTS_PER_15MIN_SOFT}, content_type="guard")
            return []

        prev_id = None
        ids = []

        for idx, t in enumerate(tweets):
            if idx > 0:
                self._sleep_jitter(1.1, 1.5)

            tid = self._publish_tweet(t, in_reply_to_tweet_id=prev_id)
            if not tid:
                break
            prev_id = tid
            ids.append(tid)

        if ids:
            self._audit("thread_posted", {"pillar": pillar, "tweet_ids": ids}, content_type="thread")

        return ids

    # =========================================================
    # Poll adaptive learning
    # =========================================================
    def _init_perf_bucket(self, pillar):
        self.state.setdefault("poll_perf", {})
        self.state["poll_perf"].setdefault(pillar, {})
        for lvl in LEVELS:
            self.state["poll_perf"][pillar].setdefault(lvl, {"polls": 0, "eng_sum": 0, "reply_sum": 0})
        self._save_state()

    def _classify_level_from_text(self, text: str) -> str:
        t = (text or "").lower()

        beginner_kw = [
            "Ù…Ø¨ØªØ¯Ø¦", "Ø£Ù†Ø§ Ø¬Ø¯ÙŠØ¯", "Ø¬Ø¯ÙŠØ¯", "ÙˆØ´ ÙŠØ¹Ù†ÙŠ", "Ù…Ø§ Ù…Ø¹Ù†Ù‰", "Ø´Ø±Ø­ Ø¨Ø³ÙŠØ·", "Ø¨Ø¨Ø³Ø§Ø·Ø©", "Ù…Ù† ÙˆÙŠÙ† Ø£Ø¨Ø¯Ø£", "Ø£Ø³Ø§Ø³ÙŠØ§Øª",
            "what is", "beginner", "basics", "eli5", "how to start"
        ]
        advanced_kw = [
            "rag", "vector", "embedding", "orchestration", "agentic", "sre", "slo", "error budget",
            "latency", "profil", "kubernetes", "terraform", "observability", "distributed", "scalability"
        ]
        intermediate_kw = [
            "best practice", "Ù…Ø´ÙƒÙ„Ø©", "Ø­Ù„", "ØªÙƒÙ„ÙØ©", "debug", "testing", "unit test", "refactor", "clean code",
            "cost", "billing", "security", "iam", "performance"
        ]

        score = {"beginner": 0, "intermediate": 0, "advanced": 0}

        for k in beginner_kw:
            if k in t:
                score["beginner"] += 2
        for k in advanced_kw:
            if k in t:
                score["advanced"] += 2
        for k in intermediate_kw:
            if k in t:
                score["intermediate"] += 1

        best = max(score, key=lambda x: score[x])
        return best if score[best] > 0 else "intermediate"

    def _poll_has_ended(self) -> bool:
        last = self.state.get("last_poll_at")
        if not last:
            return False
        try:
            last_dt = datetime.fromisoformat(last)
        except Exception:
            return False
        delta = datetime.now(timezone.utc) - last_dt
        return delta.total_seconds() >= (POLL_DURATION_MINUTES * 60)

    def _infer_level_from_poll_replies(self, poll_id: str) -> str:
        # Guard read
        if not self._can_read_monthly(1):
            return "intermediate"

        try:
            query = f"conversation_id:{poll_id} -is:retweet"
            res = self.client_v2.search_recent_tweets(query=query, max_results=50, user_auth=True)
            self._mark_read_monthly(1)

            if not res or not res.data:
                return "intermediate"

            votes = {"beginner": 0, "intermediate": 0, "advanced": 0}
            for tw in res.data:
                lvl = self._classify_level_from_text(getattr(tw, "text", ""))
                votes[lvl] += 1

            best = max(votes, key=lambda k: votes[k])
            return best if votes[best] > 0 else "intermediate"

        except Exception:
            return "intermediate"

    def _get_poll_engagement_score(self, poll_id: str) -> int:
        # Guard read
        if not self._can_read_monthly(1):
            return 0
        try:
            tw = self.client_v2.get_tweet(
                id=poll_id,
                tweet_fields=["public_metrics"],
                user_auth=True
            )
            self._mark_read_monthly(1)

            if not tw or not tw.data or not getattr(tw.data, "public_metrics", None):
                return 0

            m = tw.data.public_metrics
            likes = int(m.get("like_count", 0))
            replies = int(m.get("reply_count", 0))
            rts = int(m.get("retweet_count", 0))
            quotes = int(m.get("quote_count", 0))

            # weighted score
            score = replies * 3 + quotes * 3 + rts * 2 + likes
            return score

        except Exception:
            return 0

    def _update_poll_learning(self):
        if not self._poll_has_ended():
            return
        if self.state.get("last_poll_processed") is True:
            return

        poll_id = self.state.get("last_poll_id")
        pillar = self.state.get("last_poll_pillar")
        used_level = self.state.get("last_poll_level")

        if not poll_id or not pillar or not used_level:
            return

        self._init_perf_bucket(pillar)

        inferred_level = self._infer_level_from_poll_replies(poll_id)
        score = self._get_poll_engagement_score(poll_id)

        self.state["poll_perf"][pillar][used_level]["polls"] += 1
        self.state["poll_perf"][pillar][used_level]["eng_sum"] += score
        self.state["poll_perf"][pillar][inferred_level]["reply_sum"] += 1

        self.state["last_poll_processed"] = True
        self._save_state()

        self._audit("poll_learned", {
            "pillar": pillar,
            "level": used_level,
            "inferred_level": inferred_level,
            "score": score
        }, content_type="poll")

    def _choose_level_for_pillar(self, pillar: str) -> str:
        """
        70% choose best avg engagement; 20% choose reply-pref; 10% random exploration
        """
        self._init_perf_bucket(pillar)
        perf = self.state["poll_perf"][pillar]

        avgs = {}
        for lvl in LEVELS:
            polls = max(1, int(perf[lvl].get("polls", 0)))
            avgs[lvl] = perf[lvl].get("eng_sum", 0) / polls

        best_level = max(avgs, key=lambda k: avgs[k])
        reply_pref = max(perf, key=lambda k: perf[k].get("reply_sum", 0))

        r = random.random()
        if r < 0.70:
            return best_level
        elif r < 0.90:
            return reply_pref
        else:
            return random.choice(LEVELS)

    # =========================================================
    # Poll mode
    # =========================================================
    def _should_run_poll(self):
        if not POLL_MODE:
            return False
        last = self.state.get("last_poll_at")
        if not last:
            return True
        try:
            last_dt = datetime.fromisoformat(last)
        except Exception:
            return True
        delta = datetime.now(timezone.utc) - last_dt
        return delta.days >= POLL_EVERY_DAYS

    def _pick_poll_pillar(self):
        pillars = [p for p in POLL_CONFIG.keys() if p in self.content_pillars]
        if not pillars:
            return None
        idx = int(self.state.get("poll_pillar_index", 0)) % len(pillars)
        pillar = pillars[idx]
        self.state["poll_pillar_index"] = (idx + 1) % len(pillars)
        self._save_state()
        return pillar

    def _post_poll(self):
        if not self._automation_compliance_guard("poll"):
            return

        pillar = self._pick_poll_pillar()
        if not pillar:
            return

        # adaptive level selection
        level = self._choose_level_for_pillar(pillar)
        cfg = POLL_CONFIG[pillar]["levels"].get(level, POLL_CONFIG[pillar]["levels"]["intermediate"])

        question = POLL_CONFIG[pillar]["question"]
        options = cfg["options"][:4]

        if not self._can_post_monthly(1) or not self._can_post_15m(1):
            logging.info("ðŸ›‘ Ù…Ù†Ø¹ Poll Ø¨Ø³Ø¨Ø¨ Ø­Ø¯ÙˆØ¯ Ø§Ù„Ù†Ø´Ø±.")
            return

        if self.DRY_RUN:
            logging.info(f"[DRY_RUN] Poll ({pillar}/{level}): {question} | {options}")
            poll_id = f"dry_poll_{random.randint(1000,9999)}"
        else:
            resp = self.client_v2.create_tweet(
                text=question,
                poll_options=options,
                poll_duration_minutes=POLL_DURATION_MINUTES,
                user_auth=True
            )
            poll_id = resp.data["id"]
            self._mark_posted_monthly(1)
            self._mark_post_15m(1)

        self.state["last_poll_at"] = datetime.now(timezone.utc).isoformat()
        self.state["last_poll_id"] = poll_id
        self.state["last_poll_pillar"] = pillar
        self.state["last_poll_level"] = level
        self.state["last_poll_processed"] = False
        self._save_state()

        self._audit("poll_posted", {"pillar": pillar, "level": level, "poll_id": poll_id}, content_type="poll")
        logging.info(f"ðŸ“Š Poll posted ({pillar}/{level}): {poll_id}")

    # =========================================================
    # Determine keyword focus from last poll option (simple heuristic)
    # (We don't fetch actual poll result; we use inferred level + use pillar as direction)
    # =========================================================
    def _get_keyword_for_pillar_from_level(self, pillar: str, level: str):
        """
        Choose a keyword set seed for filtering RSS.
        We use the first option's keyword list as a loose filter seed (safe).
        """
        try:
            cfg = POLL_CONFIG[pillar]["levels"][level]
            # pick the first option and its keywords, use first keyword as filter seed
            opt = cfg["options"][0]
            kws = cfg.get("keywords", {}).get(opt, [])
